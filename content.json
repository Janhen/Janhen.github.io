[{"title":"","date":"2021-04-14T02:09:09.131Z","path":"2021/04/14/Spark-管理/","text":"命令操作命令参数： –-class： 指定运行的入口类 --master ：Standalone 模式时指定集群地址，yarn 模式时指定为 yarn， –-deploy-mode：运行模式，client 代表 yarn-client，cluster 代表 yarn-cluster 资源分配参数： --num-executors 2: 指定多少个 executor 进程进行执行 --total-executor-cores: 总的核心数量 --executor-memory=1G: 每个 executor 使用的内存, PROD 中可以设置为2~4G即可 --executor-cores 2： 每个 executor 分配的 CPU --driver-memory 1G: 设置 Driver 进程的内存，使用类似于 collect 之类的 action 算子向 Driver 端拉取数据时，需要调大 --driver-cores 1: driver 进程分配多少 core，默认为 1 覆盖运行的配置参数： –conf：指定运行时候的配置 spark.default.parallelism=10: 设置每个 stage 的默认 task 数量，也可以认为是分区数，官网建议设置该参数为 num-executors * executor-cores 的 2~3倍 较为合适 spark.storage.memoryFraction: 设置 RDD 持久化数据在 Executor 内存中能占的比例，默认是 0.6 spark.shuffle.memoryFraction: 设置 shuffle 过程中一个 task 拉取到上个 stage 的 task 的输出后，进行聚合操作时能够使用的 Executor 内存的比例，默认是0.2。 其他参数： --jars fastjson.jar,abc.jar： 设置 job 依赖的第三方 jar 包 123456789101112131415161718192021222324252627282930313233343536373839# 本地模式启动计算 PIspark-submit --master local-cluster[4,2,1024] \\\\ --class org.apache.spark.examples.SparkPi \\\\ $SPARK_HOME/examples/jars/spark-examples_2.11-2.4.5.jar \\\\ 10# 分配执行内存和 cpu 进行计算$SPARK_HOME/bin/spark-submit \\\\ --class org.apache.spark.examples.SparkPi \\\\ --executor-memory 1G \\\\ --total-executor-cores 2 \\\\ ./examples/jars/spark-examples_2.11-2.1.1.jar \\\\ 100 # Standalone 模式下的运行$SPARK_HOME/bin/spark-submit \\\\ --class org.apache.spark.examples.SparkPi \\\\ --master spark://linux122:7077 \\\\ --executor-memory 1G \\\\ --total-executor-cores 2 \\\\ ./examples/jars/spark-examples_2.11-2.1.1.jar \\\\ 100# Yarn-client 模式下运行$SPARK_HOME/bin/spark-submit \\\\ --master yarn \\\\ --deploy-mode client \\\\ --class org.apache.spark.examples.SparkPi \\\\ --executor-memory 1G \\\\ --num-executors 5 \\\\ --executor-cores 2 ./examples/jars/spark-examples_2.11-2.1.1.jar \\\\ 100# 配置额外参数运行spark-submit \\\\ --name &quot;My app&quot; \\\\ --master local[4] \\\\ --conf spark.eventLog.enabled=false \\\\ --conf &quot;spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps&quot; \\\\ --conf spark.hadoop.abc.def=xyz \\\\ myApp.jar Spark 配置 spark.master： 部署模式, 运行时的必选参数 spark.app.name： 应用程序名称 运行相关的资源参数： spark.driver.memory： 默认 1G? spark.default.parallelism：默认的 RDD 分区数，分布式模式 max(应用程序持有executor的core总数, 2)，官网建议 task 个数为 CPU 的核数 * executor的个数的 2~3倍 spark.executor.memory： 指定 Executor 占用的内存的大小，也可通过配置系统变量 SPARK_EXECUTOR_MEMORY 或者 SPARK_MEM 设置其大小 spark.executor.extraJavaOptions：Executor 执行额外的 JVM 参数 Shuffle 重要参数： spark.shuffle.file.buffer： 默认值为32K。shuffle write阶段buffer缓冲大小 spark.reducer.maxSizeInFlight： 默认值为48M。shuffle read阶段buffer缓冲区大小 spark.shuffle.io.maxRetries： 默认值3。Shuffle read阶段拉取数据失败时的最大重试次数 spark.shuffle.io.retryWait： 默认值5s。Shuffle read阶段拉取数据失败重试时的等待时间 spark.shuffle.sort.bypassMergeThreshold： 默认值为200。使用bypass机制，RDD分区数的限制 spark.memory.fraction &amp; spark.memory.storageFraction： 调整Shuffle相关内存 spark.memory.fraction： 缺省值0.6。存储内存和执行内存占（heap内存 - 300M）的百分比 spark.memory.storageFraction： 缺省值0.5。存储内存 与 （存储内存与执行内存之和）的百分比 SQL 相关 spark.sql.parquet.writeLegacyFormat: 连接 Hive 时候使用，使用与 Hive 相同的约定写 parquet 数据 spark.sql.autoBroadcastJoinThreshold： SQL.Broadcase Hash Join 小表控制的大小，默认 10MB spark.sql.join.preferSortMergeJoin： 默认为 true, 默认选择 Sort Merge Join spark.sql.shuffle.partitions： 分区个数，默认 200 spark.sql.inMemoryColumnarStorage.compressed： 压缩 spark.sql.inMemoryColumnarStorage.batchSize ： 批量操作的大小 fs.local.block.size: 文件 block 大小配置，设置本地文件切分大小 历史服务器： spark.history.retainedApplications： 设置缓存 Cache 中保存的应用程序历史记录的个数（默认50），如果超过这个值，旧的将被删除； 广播变量相关： spark.broadcast.blockSize： 缺省值为 4M spark.broadcast.checksum: 校验和，默认为 true spark.broadcast.compress： 压缩，默认为 true SparkUI 相关 spark.ui.enabled： SparkUI 是否开启 调试相关： spark.logLineage： 调用 Action 时打印 rdd 的 lineage 信息。 部署相关参数: spark.deploy.recoveryMode： 在集群模式时的恢复，在 Master 的 onStart 方法中选择恢复 spark.deploy.zookeeper.url： 当 spark.deploy.recoveryMode 设置为 ZOOKEEPER, 此配置生效 spark.deploy.zookeeper.dir： 配置用于将 Zookeeper 目录设置为存储恢复状态 Spark 的堆外内存配置，可用于缓存级别 StorageLevel.OFF_HEAP spark.memory.offHeap.enabled ：是否开启堆外内存，默认值为 false，需要设置为 true spark.memory.offHeap.size : 堆外内存空间的大小，默认值为 0，需要设置为正值。 12345678spark.master spark://linux121:7077spark.eventLog.enabled truespark.eventLog.dir hdfs://linux121:9000/spark-eventlogspark.serializer org.apache.spark.serializer.KryoSerializerspark.driver.memory 512m# 与 hadoop historyserver集成spark.yarn.historyServer.address linux121:18080spark.history.ui.port 18080 Spark-env.sh SPARK_WORKER_CORES：工作核心数 SPARK_WORKER_MEMORY： 工作内存 123456789101112export JAVA_HOME=/usr/local/java/jdk1.8.0_251export HADOOP_HOME=/opt/janhen/servers/hadoop-2.9.2export HADOOP_CONF_DIR=/opt/janhen/servers/hadoop-2.9.2/etc/hadoopexport SPARK_DIST_CLASSPATH=$(/opt/janhen/servers/hadoop-2.9.2/bin/hadoop classpath)export SPARK_MASTER_HOST=linux121export SPARK_MASTER_PORT=7077# History export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=50 -Dspark.history.fs.logDirectory=hdfs://linux121:9000/spark-eventlog&quot;# HAexport SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=linux121,linux122,linux123 -Dspark.deploy.zookeeper.dir=/spark&quot; 配置日志 Spark 使用 log4j 记录日志, 配置文件位置 config/log4j.properties 配置文件覆盖 默认的配置文件夹为 SPARK_HOME/conf, 可以指定 SPARK_CONF_DIR 变量, Spark 将会使用该目录下的 spark-defaults.conf, spark-env.sh, log4j.properties 等配置文件。 Spark 安装// TODO 集群环境安装的补充 历史服务器12345spark.eventLog.enabled=truespark.eventLog.compress=truespark.eventLog.dir=hdfs://linux121:9000/tmp/logs/root/logsspark.history.fs.logDirectory=hdfs://linux121:9000/tmp/logs/root/logsspark.yarn.historyServer.address=http://linux121:18080 spark-env.sh 1export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=18080 -Dspark.history.fs.logDirectory=hdfs://linux121:9000/tmp/logs/root/logs&quot; Docker 安装 /spark：为容器内 spark home 在资源不怎么充足的时候，使用单机的 Spark 进行测试。 1234567docker rm -f spark-masterdocker run -d --name spark-master \\\\ -h spark-master \\\\ -e ENABLE_INIT_DAEMON=false \\\\ -p 4040:4040 \\\\ -p 7077:7077 \\\\ bde2020/spark-master:2.4.5-hadoop2.7 Refwangzhiwubigdata/God-Of-BigData Spark 资源调优 http://spark.apache.org/docs/2.4.5/sql-performance-tuning.html 2.4.5 Spark 性能调优官方文档 Index of /dist/spark Spark 官网下载地址 big-data-europe/docker-spark Spark2.4.5 版本的 Docker Spark Configuration Spark 配置官方文档","tags":[]},{"title":"Flume-数据采集","date":"2021-04-09T08:56:07.000Z","path":"2021/04/09/Flume-数据采集/","text":"概述Flume 是 Cloudera 提供的一个分布式、高可靠、高可用的海量日志采集、聚合和传输的系统，基于流式架构，灵活简单。 实时采集日志的数据采集引擎。 作用： 实时读取服务器本地磁盘的数据，将数据写入到 HDFS。 特点 可与任意存储进程集成，Redis、 Kafka； 输入的速率大于写入目的存储的速率，会被缓存，减少 HDFS 压力； 事务基于 channel，两个事务模型（sender + receiver），保证消息被可靠发送 Flume 是一个专用工具被设计为旨在往 HDFS，HBase 发送数据。它对 HDFS 有特殊的优化，集成了 Hadoop 的安全特性。官方建议数据被多个系统消费，使用 kafka；数据被设计给 Hadoop 使用，使用 Flume。 体系结构 Agent： 为一个 JVM 进程，运行在日志手机节点服务器上。 Source：以处理各种类型、各种格式的日志数据 Channel：位于Source和Sink之间的缓冲 Sink：不断地轮询 Channel 中的事件且批量地移除，将其写入到其他系统或另一个 Agent Event： ****Flume 数据传输的基本单元，以事件的形式将数据从源发送到目的。 Header: 附加的信息，可通过配置文件组的方式对匹配到的目录文件增加 header，可通过拦截器为 Event 增加特定的 header Body：事件的实际内容 拓扑结构 串行模式： 复制模式： 单 Souce 多 Channel、Sink 模式 负载均衡模式(单 Source、Channel 多 Sink )： 解决负载均衡和故障转移问题。 聚合模式： 最常用 传送到一个集中收集日志的 flume，再由此 flume 上传到 hdfs、hive、hbase、消息队列中。 执行流程 Source 接收事件，交给 Channel 处理器处理事件 处理器通过拦截器 Interceptor，对事件一些处理,比如压缩解码，正则拦截，时间戳拦截，分类等 经过拦截器处理过的事件再传给 Channel 选择器，将事件写入相应的 Channel 最后由 Sink 处理器处理各个 Channel 的事件 Source 负责接收数据到 Flume Agent 的组件。可以处理各种类型、各种格式的日志数据 常见的 Source directory、http kafka source: avro source: 监听 Avro 端口来接收外部 avro 客户端的事件流。 exec source: 可以将命令产生的输出作为 source。如 ping、tail。 可监控实时追加的文件，无法保证数据不丢。 tail -f： 文件改名/被删除，追踪停止 tail -F: 等同于 --follow=name --retry ，根据文件名进行追踪，并保持重试，文件被删除/改名后，若再次创建相同的文件名，会继续追踪 Netcat source: 用来监听一个指定端口，并接收监听到的数据，常用于测试。 spooling directory source: 将指定的文件加入到“自动搜集”目录中。 taildir source： 监控指定的多个文件，支持续传。 拦截器 在运行时，可实时对 event 进行修改或丢弃。 拦截器的分类 时间添加戳拦截器： 生成时间戳到 Eventheader 中 Host添加拦截器： 生成主机 host/ip 到 Event header 中 正则表达式过滤拦截器： 使用正则过滤需要的数据 自定义拦截器： 可解析 Event Body 中的数据，提取到 Event Header 中做特殊处理，如提取日志的事件事件 时间添加戳拦截器 向每个 event 的 header 中添加一个时间戳属性进去，key → value = “timestamp” → 当前的毫秒 123456a1.sources.r1.interceptors = i1 a1.sources.r1.interceptors.i1.type = timestamp# 是否保留Event header中已经存在的同名时间戳，缺省值falsea1.sources.r1.interceptors.i1.preserveExisting= false# 向event header中添加时间戳键值对的keya1.sources.r1.interceptors.i1.header= time Host添加拦截器 把当前Agent的 hostname 或者 IP 地址写入到 Event 的 header 中 type: host preserveExisting: header 中存在同名属性是否保留，默认 false userIp: ip 地址，默认 true, 为 false 时使用 hostname hostHeader: 默认为 host, Event header 中添加 host 键值对的 key 正则表达式过滤拦截器 把 Event 的 body 当做字符串来处理，并用配置的正则表达式来匹配。 自定义拦截器 生产环境中常需要自定义拦截器, org.apache.flume.interceptor.Interceptor。 123456789public interface Interceptor &#123; void initialize(); Event intercept(Event var1); List&lt;Event&gt; intercept(List&lt;Event&gt; var1); void close(); public interface Builder extends Configurable &#123; Interceptor build(); &#125;&#125; Spooldir source 监听一个指定的目录，向指定目录添加新的文件，sourcce 可获取到该信息 注意事项 以追加的方式向已被处理的文件中添加内容，source并不能识别 复制的 spooldir 目录下的文件不可再打开编辑 无法监控子目录的文件夹的变动 被监控文件夹 500ms 扫描一次文件 适合同步新文件，不适合对实时追加日志文件进行监听并同步 Taildir source 1.7.0 引入，支持断点续传、可监控多目录。相当于 spooldir source + exec source。生产环境中使用较多。 特点： 支持正则表达式匹配目录中的文件名 监控日志文件，有文件写入进行收集 高可靠，agent 重启后不会数据丢失 不会对跟踪文件有任何处理，不会重命名、删除 不能读取二进制文件，支持按照行读取文本文件。 Taildir source 配置 filegroups：文件组，使用空格分隔 filegroups.&lt;filegroupName&gt;： 文件组中指定的绝对路径，可根据正则匹配路径 positionFile： 记录每个拖尾文件的索引节点，绝对路径和最后位置，解决断点续传问题 batchSize： 一次读取并发送到通道的最大行数，默认 100，一般需要更改下 headers.&lt;filegroupName&gt;.&lt;headerKey&gt;： 指定指定文件组的 header key，用于区分不同的文件，放入不同的 header 后，方便在拦截器中根据传入的 headerKey 进行特定的处理 fileHeader： 是否增加一个 header fileHeaderKey： hdfs.useLocalTimeStamp： 使用本地时间，而非事件时间 1234a1.sources.r1.type &#x3D; TAILDIR a1.sources.r1.positionFile &#x3D; &#x2F;data&#x2F;janhen&#x2F;conf&#x2F;startlog_position.json a1.sources.r1.filegroups &#x3D; f1 a1.sources.r1.filegroups.f1 &#x3D; &#x2F;data&#x2F;janhen&#x2F;logs&#x2F;start&#x2F;.*log HDFS sink 配置 HDFS 文件的滚动方式： 基于时间: 基于文件大小: 默认 1024b，生产中调大 基于 EVENT 数量: 基于文件空闲时间: 1234567891011121314151617181920a1.sinks.k1.type = hdfs a1.sinks.k1.hdfs.path = /user/data/logs/start/%Y-%m-%d/ a1.sinks.k1.hdfs.filePrefix = startlog.# 配置文件滚动方式（文件大小32M）, 默认 1024b, 生产中 &gt;&gt;a1.sinks.k1.hdfs.rollSize = 33554432 # 基于时间的数量，默认10a1.sinks.k1.hdfs.rollCount = 0 # 基于时间的滚动，默认30sa1.sinks.k1.hdfs.rollInterval = 0 # 基于文件空闲时间a1.sinks.k1.hdfs.idleTimeout = 0 # 默认值与 hdfs 副本数一致, 设为1是为了让 Flume 感知不到hdfs的块复制a1.sinks.k1.hdfs.minBlockReplicas = 1# 向hdfs上刷新的event的个数, 默认100 a1.sinks.k1.hdfs.batchSize = 100# 使用本地时间 a1.sinks.k1.hdfs.useLocalTimeStamp = true agent 配置 123456789101112131415161718192021222324252627282930313233a1.sources=r1a1.sinks = k1a1.channels = c1# taildir sourcea1.sources.r1.type = TAILDIRa1.sources.r1.positionFile = /data/janhen/conf/startlog_position.jsona1.sources.r1.filegroups = f1a1.sources.r1.filegroups.f1 = /data/janhen/logs/start/.*log# memorychannela1.channels.c1.type = memorya1.channels.c1.capacity = 100000a1.channels.c1.transactionCapacity = 2000# hdfs sinka1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = /user/data/logs/start/%Y-%m-%d/a1.sinks.k1.hdfs.filePrefix = startlog.# 配置文件滚动方式（文件大小32M）a1.sinks.k1.hdfs.rollSize = 33554432a1.sinks.k1.hdfs.rollCount = 0a1.sinks.k1.hdfs.rollInterval = 0a1.sinks.k1.hdfs.idleTimeout = 0a1.sinks.k1.hdfs.minBlockReplicas = 1# 向hdfs上刷新的event的个数a1.sinks.k1.hdfs.batchSize = 1000# 使用本地时间a1.sinks.k1.hdfs.useLocalTimeStamp = true# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 使用自定义拦截器 1flume-ng agent --conf-file /root/janhen/conf/flumelog2hdfs1.conf -name a1 -Dflum e.roog.logger=INFO,console Channel 位于 Source 和 Sink 之间的缓冲区，是线程安全的，可同时处理多个 Source 的写入和多个 Sink 的读取，让 Source 与 Sink 可以运行在不同的速率上. 常用的 Channel 类型： memory channel: 缓存到内存中，宕机数据丢失 file channel: 缓存到文件中，宕机数据不丢失，访问速度较慢 kafka channel: 缓存到 Kafka 中。兼具 memory, file 的优点，支持扩展。速度快，容量大。 jdbc channel: 缓存到关系型数据库中 配置参数： type: capacity: 存储在 channel 中的最大事件数量，默认为 100，生产环境需要加大 transactionCapacity：每次事务从 source 到 sink 的最大事件数量，默认为 100 keep-alive：.. 123456a1.channels = c1a1.channels.c1.type = memory# channel 最大缓存 50W 个事件a1.channels.c1.capacity = 500000# 每批 channle 事务处理最大 2W 个事件a1.channels.c1.transactionCapacity = 20000 Channel 选择器确定 source 以何种方式向多个 channel 写， 默认为复制的。 复制选择器（Replicating Channel Selector) 可通过 selector.optional 指定哪些 channel 可选，空格分开，失败忽略不会进行实物回滚。 1a1.sources.r1.selector.type &#x3D; replicatin 多路复用选择器(Multiplexing Channel Selector) 按配置进行分发 自定义选择器 接口 org.apache.flume.ChannelSelector 的实现类, 实现类以及依赖的jar包在启动时候都必须放入Flume的classpath。 123a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sources.r1.selector.type &#x3D; com.janhen.bigdata.flume.MyChannelSelector Sink Sink 组件用来保存数据，不断轮询 Channel 中的事件并批量的移除。将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent。 完全事务性的 HDFS sink： 存储到 HDFS 中，使用较多，Flume 对其有特殊的优化 Hive sink: Hive 事务写 events Kafka sink: 写入到 Kafka 中 官方提供的 flume Kafka 插件，自定义了 flume 的 sink，将数据从 channel 中取出，通过 Kafka 的 producer 写入到 Kafka 中，可以自定义分区等。 HBase sink: 写入到 HBase 中 Logger sink: 信息显示在标准输出上，主要用于测试 Avro sink: 转换为 Avro events HDFS Sink基本配置： hdfs.path： 配置 sink 的基础目录 hdfs.filePrefix： 在目录下创建的文件前缀 hdfs.fileSuffix： 在目录下创建的文件后缀 hdfs.useLocalTimeStamp： 是否使用本地时间戳，默认 false hdfs.round: 时间戳是否四舍五入, 默认 false hdfs.roundValue： 四舍五入的最高倍数 hdfs.roundUnit: 可选 second, minute, hour，默认为 second hdfs.threadsPoolSize： 滚动生成文件配置 hdfs.rollInterval： 基于时间回滚，缺省 30s，0 表示不根据时间进行回滚 hdfs.rollSize： 基于文件大小回滚，缺省 1024byte，0 表示… hdfs.rollCount： 基于 event 的数量回滚，缺省 10 hdfs.idleTimeout： 基于文件空闲时间回滚，关闭非活动文件超时时间，缺省禁用 hdfs.minBlockReplicas： HDFS 副本数，默认与 HDFS 副本数一致，将参数设置为 1，避免小文件和复制的网络开销 其他： hdfs.batchSize： 向 hdfs 上刷新的 event 个数 123456789101112131415161718192021222324a2.sinks.k2.hdfs.path = hdfs://linux121:9000/flume/%Y%m%d/%H%M# 上传文件的前缀a2.sinks.k2.hdfs.filePrefix = log# 使用本地时间戳a2.sinks.k2.hdfs.useLocalTimeStamp = true# 500 个 Event 才 flush 到 HDFS 一次a2.sinks.k2.hdfs.batchSize = 500# 设置文件类型，支持压缩。DataStream 没启用压缩a2.sinks.k2.hdfs.fileType = DataStream# 1 分钟滚动一次a2.sinks.k2.hdfs.rollInterval = 60# 128M 滚动一次a2.sinks.k2.hdfs.rollSize = 134217700# 文件的滚动与 Event 数量无关a2.sinks.k2.hdfs.rollCount = 0# HDFS 最小冗余数a2.sinks.k2.hdfs.minBlockReplicas = 1 规避小文件、提高写入 HDFS 速度参数 123456789101112a1.sinks.k1.type=hdfsa1.sinks.k1.hdfs.useLocalTimeStamp=truea1.sinks.k1.hdfs.path=hdfs://linux121:9000/flume/events/%Y/%m/%d/ %H/%Ma1.sinks.k1.hdfs.minBlockReplicas=1a1.sinks.k1.hdfs.rollInterval=3600# 文件大小 1G 滚动a1.sinks.k1.hdfs.rollSize=1073741824a1.sinks.k1.hdfs.rollCount=0a1.sinks.k1.hdfs.idleTimeout=0# 向hdfs上刷新的event个数a1.sinks.k1.hdfs.batchSize = 10000 Sink 组逻辑处理器 把多个 sink 分成一个组， Sink 组逻辑处理器可以对这同一个组里的几个 sink 进行 负载均衡 或者 其中一个sink发生故障后将输出 Event 的任务转移到其他的sink上。默认无 sink 组。 负载均衡 轮训，随机。。。 退避机制,backoff 设置，失败的 sink 放入黑名单 … 故障转移 N 中只有一个在工作，其他备用 故障 sink 降级到一个池 设置 sink 组的选择器为 failove 为每一个 sink 设置一个唯一的优先级数值 其他事务机制与可用性 保证数据在 Flume 流动过程中数据不丢失。 1、Put 事务: 在 Source 到 Channel 之间保证事务 把一批 Event 放到一个事务中，这批 event 一次性的放入 Channel 中。 (1) 执行流程： 事务开始的时候会调用一个 doPut 方法， doPut 方法将一批数据放在 putList 中； putList在向 Channel 发送数据之前先检查 Channel 的容量能否放得下，如 果放不下一个都不放，只能 doRollback； 数据批的大小取决于配置参数 batch size 的值； putList 的大小取决于配置 Channel 的参数 transaction capacity 的大 小，该参数大小就体现在putList上； 数据顺利的放到 putList 之后，接下来可以调用 doCommit 方法，把 putList 中所有的 Event 放到 Channel 中，成功放完之后就清空 putList； (2) 事务的回滚(doRollback)： doRollback 方法会进行两项操作：将 putList 清空和抛出 ChannelException 异常。source 会捕捉到 doRollback 抛出的异常，然后 source 就将刚才的一批数据重新采集，然后重新开始一个新的事务。 2、Take 事务: Channel 到 Sink 之间保证事务 把一批 event 组成的事务统一拿出来到 sink 放到 HDFS 上。 source、sink 的可靠不可控，随着具体使用 Source、Channel、Sink 的类型。 (1) 执行过程 doTake 方法会将 channel 中的 event 剪切到 takeList 中。后面接的是HDFS Sink 的话，在把 Channel 中的 event 剪切到 takeList 中的同时也往写入 HDFS 的 IO 缓冲流中放一份 event(数据写入 HDFS 是先写入 IO 缓冲流然后 flush 到 HDFS）； 当 takeList 中存放了 batch size 数量的 event 之后，就会调用 doCommit 方法， doCommit 方法会做两个操作： 针对 HDFS Sink，手动调用 IO 流的 flush 方法，将 IO 流缓冲区的数据写入到 HDFS磁盘中； 清空 takeList 中的数据 （2） 事务的回滚 回滚并没有“一半”之说，它只会把整个 takeList 中的数据返回给 channel，然后继续进行数据的读写。这样开启下一个事务的时候容易造成数据重复的问题。 Flume 执行Agent JVM heap 一般为 4G ~ 8G，部署在单独的服务器上。 全局参数 --conf / -c： 指定配置文件 -classpath,-C ： 指定 … --dryrun,-d: 打印命令，实际不执行 -Dproperty=value： 设置 JDK 系统属性 flume.root.logger=INFO,console： 调试时候使用，将日志打印到控制台 org.apache.flume.log.printconfig=true org.apache.flume.log.rawdata=true Agent 参数 —-name: 指定 agent 的名称 123456$FLUME_HOME/bin/flume-ng agent --name a1 \\\\ --conf-file $FLUME_HOME/conf/flume-netcat-logger.conf \\\\ -Dflume.root.logger=INFO,console$FLUME_HOME/bin/flume-ng agent --conf /opt/apps/flume-1.9/conf \\\\ --conf-file conf/flume-log2hdfs1.conf \\\\ -name a1 -Dflume.root.logger=INFO,console 指定 JVM 参数运行 $FLUME_HOME/conf/flume-env.sh 中指定运行时的 JVM 参数 123456# 在 $FLUME_HOME/conf/flume-env.sh 中指定运行时的 JVM 参数export JAVA_OPTS=&quot;-Xms4000m -Xmx4000m -Dcom.sun.management.jmxremote&quot;flume-ng agent --conf /opt/apps/flume-1.9/conf \\\\ --conf-file /data/janhen/conf/flume-log2hdfs1.conf \\\\ -name a1 -Dflume.roog.logger=INFO,console 生产环境时候的运行 nohup，该命令允许用户退出帐户/关闭终端之后继续运行相应的进程 /dev/null，代表linux的空设备文件，所有往这个文件里面写入的内容都会丢 失，俗称黑洞 标准输入0，从键盘获得输入 /proc/self/fd/0 标准输出1，输出到屏幕（控制台） /proc/self/fd/1 错误输出2，输出到屏幕（控制台） /proc/self/fd/2 /dev/null 标准输出1重定向到 /dev/null 中，此时标准输出不存在，没有任何地方能够找到输出的内容 2&gt;&amp;1 错误输出将会和标准输出输出到同一个地方 /dev/null 2&gt;&amp;1 不会输出任何信息到控制台，也不会有任何信息输出到文件 中 123nohup flume-ng agent --conf /opt/apps/flume-1.9/conf \\\\ --conf-file /data/janhen/conf/flume-log2hdfs3.conf \\\\ -name a1 -Dflume.root.logger=INFO,LOGFILE &gt; /dev/null 2&gt;&amp;1 &amp; Flume 日志数据采集事项 使用 taildir source 监控指定的多个目录，可以给不同目录的日志加上不同 header 在每个目录中可以使用正则匹配多个文件 使用自定义拦截器，主要功能是从 json 串中获取时间戳，加到 event 的 header 中 hdfs sink 使用 event header 中的信息写数据（控制写文件的位置） hdfs 文件的滚动方式（基于文件大小、基于event数量、基于时间） 调节 flume jvm 内存的分配 数据采集优化 channel 选取 Kafka, 兼具 File Channel 的可靠性与 Memory Channel 的速度 配置负载均衡 根据数据采集的业务需求情况，替换成其他的日志采集组件，Logstash, FileBeat 在 Flume 中过滤数据，剔除掉无用的数据，只收集核心的数据(json) Flume 丢包问题单机 upd 的 flume source 的配置，100+M/s数据量，10w qps flume就开始大量丢包。 一般公司在使用 Flume 工作过程中，对业务日志进行监控，如 Flume agent 中有多少条日志，Flume到 Kafka 后有多少条日志等等，如果数据丢失保持在 1% 左右是没有问题的，当数据丢失达到 5% 左右时就必须采取相应措施。 采集日志采集 Nginx 日志，日志格式固定，但是缺少 sessionId Log4j 日志，带有 sessionId，与项目联系紧密 Refs 中文flume帮助文档 Flume 1.9.0 User Guide - Apache Flume Flume 官方用户指南文档 Flume 1.9.0 User Guide - Apache Flume Flume tailDIR source 官方文档说明 Flume 1.9.0 User Guide - Apache Flume Flume 拦截器官方文档 Flume 1.9.0 User Guide - Apache Flume Flume HDFS sink 文档","tags":[{"name":"工具","slug":"工具","permalink":"http://example.com/tags/%E5%B7%A5%E5%85%B7/"},{"name":"数据采集","slug":"数据采集","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"}]},{"title":"MySQL-管理","date":"2021-04-08T17:10:05.000Z","path":"2021/04/09/MySQL-管理/","text":"基本管理查看数据库中行数大于 0 的表 1234567891011121314151617181920-- 查找表行大于0的SELECT CONCAT(table_schema, &#x27;.&#x27;, table_name) AS TABLE_NAME ,engine AS TABLE_ENGINE ,table_type AS TABLE_TYPE ,table_rows AS TABLE_ROWS ,CONCAT(ROUND(data_length / ( 1024 * 1024), 2), &#x27;M&#x27;) AS TB_DATA_SIZE ,CONCAT(ROUND(index_length / ( 1024 * 1024), 2), &#x27;M&#x27;) AS TB_IDX_SIZE ,CONCAT(ROUND((data_length + index_length ) / ( 1024 * 1024 ), 2), &#x27;M&#x27;) AS TOTAL_SIZE ,CASE WHEN data_length =0 THEN 0 ELSE ROUND(index_length / data_length, 2) END AS TB_INDX_RATE ,CONCAT(ROUND( data_free / 1024 / 1024,2), &#x27;MB&#x27;) AS TB_DATA_FREE ,CASE WHEN (data_length + index_length) = 0 THEN 0 ELSE ROUND(data_free/(data_length + index_length),2) END AS TB_FRAG_RATEFROM information_schema.TABLES WHERE TABLE_ROWS &gt; 0 AND table_name in (&#x27;entitylog&#x27;,&#x27;iterfacelog&#x27;,&#x27;stockshiftflow&#x27;,&#x27;stockoperlog&#x27;)ORDER BY data_length limit 0, 20 命令执行 -e: 执行 SQL 语句 -D: 连接的数据库 -s, --silent: 使用 tab 作为分隔符，row ⇒ line，用于制作报表 -C, --compress: 使用压缩在 client/server 之间 输出： 配合 Excel 制表 -B: 使用 Tab 替换分隔符 -N: 不输出列信息 -E: 垂直输出，展示格式 -H: 以 HTML 输出 -X: 以 XML 输出 1234# 连接 DB 执行 SQL，让结果按照特定格式显示，方便 awk, sed 处理mysql -uroot -p$MYSQL_ROOT_PASSWORD -h&lt;ip&gt; -D school -e &quot;SELECT * FROM student;&quot;mysql -udbuser -p123456 -h&lt;ip&gt; -D school -N -B -e &quot;SELECT * FROM student;&quot;mysql -udbuser -p123456 -h&lt;ip&gt; -D school -N -H -B -e &quot;SELECT * FROM student;&quot; &gt; result.html 服务信息查看查看最近的 InnoDB 信息 1SHOW ENGINE INNODB STATUS \\\\G 查看存储过程 1SHOW PROCEDURE STATUE LIKE &#x27;.*&#x27; \\\\G 查看函数 1SHOW FUNCTION STATUE LIKE &#x27;.*&#x27; \\\\G 查找出所有 function,routines 1SELECT SPECIFIC_NAME FROM information_schema.Routines \\\\G 查看服务器状态 1show status like &#x27;%lock%&#x27;\\\\G 查询是否锁表 12# 记录当前锁表状态 show OPEN TABLES where In_use &gt; 0 查询 MySQL 进程 123# Top 100show processlistshow full processlist; 查看正在锁的事务 1SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS; 查看等待锁的事务 1SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS; 慢查询 123456-- 查看慢查询时间show variables like &quot;long_query_time&quot;;-- 查看慢查询配置情况show status like &quot;%slow_queries%&quot;;-- 查看慢查询日志路径show variables like &quot;%slow%&quot;; 查看当前有那些表是打开的 12show open tables;show open tables from database; 查看服务器超时参数 12345678910111213141516show variables like ‘%timeout%’;-- 隔离级别SHOW VARIBALES LIKE &#x27;ios%&#x27;-- innodbSHOW VARIABLES LIKE &#x27;innodb_file_per_table&#x27;-- 日志缓冲区SHOW VARIABLES lIKE &#x27;innodb_log_buffer_size&#x27;-- 表的索引信息查看SHOW INDEX FROM &lt;mytab&gt;-- 表信息查看SHOW CREATE TABLE &lt;mytab&gt; \\\\G-- warning 出现后的查看show warnings;-- 时区设置set time_zone=&#x27;+10:00&#x27;;-- 隔离级别设置SET SESSSION TRANSACTION LEVEL 用户与权限对于连接程序的账户，不给予 ALERT 权限，但是给定 DML 语句的 EXEC，让用户通过 EXEC 执行函数或是存储过程来变更表结构，防止权限造成的表结构更改混乱问题 权限 创建检查账号: 123grant select,process,super,replication slave on . to &#x27;mysql_check&#x27;@&#x27;x.x.x.x&#x27; identified by &#x27;mysql_check&#x27;; flush privileges; 如何在给定场景下为某用户授权？ 对于对接的数据库授权，主要为 DML 权限 如何为用户授权 遵循最小权限原则 使用 Grant 命令对用户授权(非改 db) 用户管理流程规范 数据库用户管理流程规范 (1) 最小权限原则 (2) 密码强度策略 (3) 密码过期原则。5.7 中引入 (4) 限制历史密码重用原则 定义账号 如何定义MySQL数据库账号？ (1) 用户名@可访问控制列表 1.%：代表可以从所有外部主机访问 2.192.168.1.%：表示可以从192.168.1网段访问 3.localhost: DB服务器本地访问 (2) 使用CREATE USER命令建立用户 8.0 信息更多 如何从一个实例迁移数据库账号到另一个实例？ 根据 mysql 的数据库版本是否一致判断 一致，备份 mysql 数据库，目的实例进行恢复 - 不一致，到处授权 sql 1234567891011121314151617181920pt-show-grants u=root，p=123456，h=localhostshow priviledges; CREATE USER userinfo IDENTIFIED BY &#x27;3UuQuskw2k%k&#x27;;RENAME USER userinfo TO janhen;DROP USER janhen;SHOW GRANTS FOR janhen;-- 授权给新增的用户GRANT SELECT,UPDATE,INSERT,DELETE ON openapi.* TO openapi;-- 取消授权REVOKE DELETE ON openapi.* FROM openapi;、-- 更改密码SET PASSWORD FOR openapi = PASSWORD(&#x27;!BrJ%4MROWvN&#x27;);SET PASSWORD FOR &#x27;dbadmin&#x27;@&#x27;localhost&#x27; = bigshark;-- 设置登陆用户的密码SET PASSWORD = PASSWORD(&#x27;xxxxx&#x27;); 权限查看 1show grants for &#x27;test&#x27;@&#x27;%&#x27; 权限收回 1revoke insert on test.* from &#x27;test&#x27;@&#x27;%&#x27; 常见权限 (1) 管理权限 不提供 MySQL 的 root 权限，给管理员权限，防止误操作 12345GRANT ALL PRIVILEGESON *.* TO &#x27;janhenadmin&#x27;@&#x27;sjfksjfaksdjfad&#x27; WITH GRANT OPTIONgrant super,process,file on *.* to &#x27;janhen&#x27;@&#x27;%&#x27; (2) 数据库的读写权限 主要用于系统对接使用， SELECT, INSERT, UPDATE, DELETE, EXECUTE 1234GRANT SELECT,INSERT,UPDATE,DELETE,EXECUTE ON openapi.* TO &#x27;openapi&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;123&#x27; (3) 监控权限 用于命令行监控，或是 Prometheus 进行监控信息的获取 基本的主从命令 对于统计数据库的权限 基本的要求： SHOW MASTER LOGS SHOW ENGINE INNODB STATUS SHOW VIEW, PROCESS, REPLICATION CLIENT, SELECT and SHOW DATABASES 对 mysql 表的查看 (4) 备份权限 通过 mysqldump 或是 xtracback 执行备份，需要对 VIEW、Function、event 等进行备份 SELECT, RELOAD, LOCK TABLES, REPLICATION CLIENT, SHOW VIEW, EVENT, PROCESS 123GRANT select,reload,lock tables,replication client,show view,event,process ON *.* TO &#x27;backup&#x27;@&#x27;%&#x27;; (5) 登陆权限 1grant usage on *.* to &#x27;test&#x27;@&#x27;%&#x27; MySQL 监控概述监控什么 1、可用性: 端口检测，执行 select 1 或 show status 作为测试 SQL 2、性能监控: 并发线程监控 3、对主从复制进行监控 4、服务器资源的监控 可用性监控 本地 | 网络, 自带命令 ping 进行检测 1mysqladmin -umonitor -p -h pingtelnet ip db_port 2、Telent 连接 3、程序通过网络建立数据库 @Q: 确定 DB 是否读写 read_only 参数，主从中注意，定期检查 简单查询 SELECT @@version 连接数量监控： 123456-- 运行最大show variables like &#x27;max_connections&#x27;;-- 当前连接show global status like &#x27;Threads_connected&#x27;;-- 比例监控Threads_connected/max_connections&gt;0.8; 性能监控 @Q: QPS 和 TPS TPS 每秒的事务数量 123CRU QPS=(Queries2-Queries1)/(Uptime_since_flush_status2- Uptime_since_flush_status1)TPS=((Com_insert2+Com_update2+Com_delete2)- (Com_insert1+Com_update1+Com_delete1))/ (Uptime_ since_flush_status2-Uptime_since_flush_ status1) 主从复制监控 @Q: 监控主从复制链路的状态 123456Master_Log_File:mysql-bin.001083Read_Master_Log_Pos:228613650Relay_Log_File:mysqld-relay-bin.003606Relay_Log_Pos:228613813Relay_Master_Log_File:mysql-bin.001083Slave_Io_Running:Yes Slave_SQL_Running:Yes Replicate_Do__DB: Replicate_Iqnore_DB: @Q: 主从延迟 #{。。。} @Q: 验证主从复制的数据一致性 MySQL 信息查看 不使用其他的工具，借助 MySQL 自带的命令或表进行查看 MySQL 状态信息查看 收集这些信息可以用于检测数据库的运行情况 分为全局状态、会话状态 123show processlist; show status; show status like ‘%下面变量%’; Aborted_clients 由于客户没有正确关闭连接已经死掉,已经放弃的连接数量. Flush_commands 执行FLUSH命令的次数. Aborted_connects 尝试已经失败的MySQL服务器的连接的次数. Max_used_connections 同时使用的连接的最大数目. Slow_queries：要花超过long_query_time时间的查询数量. Open_tables 打开表的数量. Open_files 打开文件的数量. Open_streams 打开流的数量(主要用于日志记载） Opened_tables 已经打开的表的数量. Threads_connected 当前打开的连接的数量. INFORMATION_SCHEMA 辅助表信息 transaction 1SELECT * FROM information_schema.INNODB_TRX; PERFORMANCE_SCHEMA 1SHOW STATUS LIKE &#x27;perf%&#x27;; 可不与 mysql 在同一个机器上，作为额外的监控组件进行操作 支持服务器分组，支持多个 mysql 连接，较少的配置易于执行 InnotopInnotop 为 MySQL和 InnoDB 事务/状态的监视器，类似 MySQL 的 top 命令，显示查询、InnoDB 事务、锁等待、死锁、打开的表、复制的状态、缓冲信息等。 运行的参数 d：多久时间更新一次 h：连接的主机名 p：连接的端口 S：socket的位置 u：连接的用户 c: 指定配置文件运行 # 进行服务器分组@ 进行选择连接 12innotop -h 127.0.0.1 \\\\ -u root -p$MYSQL_ROOT_PASSWORD 服务器分组 保存配置 1234567891011[server_groups]inner&#x3D;master233 slave158[&#x2F;server_groups] [connections]master233&#x3D;user&#x3D;slave have_user&#x3D;1 pass&#x3D;slave have_pass&#x3D;1 dsn&#x3D;DBI:mysql:;host&#x3D;172.17.10.233;port&#x3D;3306 savepass&#x3D;1slave158&#x3D;user&#x3D;slave have_user&#x3D;1 pass&#x3D;slave have_pass&#x3D;1 dsn&#x3D;DBI:mysql:;host&#x3D;172.17.10.158;port&#x3D;3306 savepass&#x3D;1[&#x2F;connections] 面板 M: 主从情况 T: 事务情况 O: 打开的表 是否在使用，是否被锁住 数据存储碎片化概述碎片分类 三种类型的数据碎片化 行碎片（Row fragmentation） 行间碎片（Intra-row fragmentaion） 剩余空间碎片（Free space fragmentation） 碎片的影响 磁盘上索引页的物理排序不接近页面上记录的索引排序，或者64页块中有许多未使用的页面被分配给索引 占用的空间比“应该”占用的空间多，所有 InnoDB 数据和索引都存储在 B-trees 中，它们的 fill factor 可能在50％到100％之间变化。 表扫描需要比“应该”花费更多的时间。 碎片查看 (1)碎片大小=数据总大小-实际表空间文件大小 (2)数据总大小=data_length+index_length=15220736 (3)实际表空间文件大小=rowsavg_rog_length=29933550=14966750 (4)碎片大小=（15220736-14966750)/1024/1024=0.2M 123456789101112131415161718192021show table status from mall like &#x27;stockshiftflow&#x27; \\\\G;show table status from mall like &#x27;stockoperlog&#x27; \\\\G;-- 查询空闲空间超过 50M 大小的表SELECT CONCAT(table_schema, &#x27;.&#x27;, table_name) AS TABLE_NAME ,engine AS TABLE_ENGINE ,table_type AS TABLE_TYPE ,table_rows AS TABLE_ROWS ,CONCAT(ROUND(data_length / ( 1024 * 1024), 2), &#x27;M&#x27;) AS TB_DATA_SIZE ,CONCAT(ROUND(index_length / ( 1024 * 1024), 2), &#x27;M&#x27;) AS TB_IDX_SIZE ,CONCAT(ROUND((data_length + index_length ) / ( 1024 * 1024 ), 2), &#x27;M&#x27;) AS TOTAL_SIZE ,CASE WHEN data_length =0 THEN 0 ELSE ROUND(index_length / data_length, 2) END AS TB_INDX_RATE ,CONCAT(ROUND( data_free / 1024 / 1024,2), &#x27;MB&#x27;) AS TB_DATA_FREE ,CASE WHEN (data_length + index_length) = 0 THEN 0 ELSE ROUND(data_free/(data_length + index_length),2) END AS TB_FRAG_RATEFROM information_schema.TABLES WHERE ROUND(DATA_FREE/1024/1024,2) &gt;=50ORDER BY data_free DESClimit 0, 20; 碎片整理optimize table table_name OPTIMIZE 操作会暂时锁住表,而且数据量越大,耗费的时间也越长。 OPTIMIZE TABLE 后，表的变化跟存储引擎有关。 对于 INNODB 表,OPTIMIZE TABLE 映射到 ALTER TABLE … FORCE（或者这样翻译：在 InnoDB 表中等价 ALTER TABLE … FORCE），它重建表以更新索引统计信息并释放聚簇索引中未使用的空间。当您在InnoDB表上运行时，它会显示在 OPTIMIZE TABLE 的输出中 对于 innodb_file_per_table=1 的 InnoDB 表，OPTIMIZE TABLE 会重组表和索引的物理存储，将空闲空间释放给操作系统。也就是说 OPTIMIZE TABLE [tablename] 这种方式只适用于独立表空间 ALTER TABLE table_name ENGINE = Innodb 实际上重新整理碎片了.当执行优化操作时,实际执行的是一个空的 ALTER 命令,但是这个命令也会起到优化的作用,它会重建整个表,删掉未使用的空白空间。 ALTER TABLE ENGINE= INNODB,会重新整理在聚簇索引上的数据和索引。 在有些情况下，ALTER TABLE xxxx ENGINE= INNODB 更好。例如old_alter_table 系统变量没有启用等等。另外对于 MyISAM 类型表，使用 ALTER TABLE xxxx ENGINE= INNODB 是明显要优于 OPTIMIZE TABLE 这种方法的。 1ALTER TABLE stockshiftflow ENGINE = Innodb; pt-online-schema-change 12345678910pt-online-schema-change \\\\ --user=$&#123;user&#125; \\\\ --password=$&#123;passwd&#125; \\\\ --host=$&#123;host&#125; \\\\ P=3306,D=$&#123;database&#125;,t=$table \\\\ --charset=utf8 \\\\ --alter=&quot;ENGINE=InnoDB&quot; \\\\ --nocheck-replication-filters \\\\ --alter-foreign-keys-method=auto \\\\ --execute 执行脚本文件名 clean_pieces.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#!/bin/bash##********************************************************************#$1 : 清理碎片的数据库名称#$2 : 清理碎片的表名称#Description: 清理 MySQL 表的碎片#args=($*)database=$&#123;args[0]&#125;tables=( $&#123;args[@]:1&#125; )echo &quot;database: $database&quot;for table in &quot;$&#123;tables[@]&#125;&quot;;do echo &quot;table: $table&quot;donehost=&#x27;127.0.0.1&#x27;user=&#x27;root&#x27;passwd=$MYSQL_ROOT_PASSWORDfor table in &quot;$&#123;tables[@]&#125;&quot;dosql=&quot;select CONCAT(table_schema, &#x27;.&#x27;, table_name) AS TABLE_NAME, concat(round(sum((DATA_LENGTH+Index_length)/1024/1024),2),&#x27;MB&#x27;) AS TABLE_SIZE, concat(round(sum(DATA_LENGTH/1024/1024),2),&#x27;MB&#x27;) AS DATA_SIZE, concat(round(sum(Index_length/1024/1024),2),&#x27;MB&#x27;) AS INDEX_SIZE, table_rows AS TABLE_ROWS, CASE WHEN data_length =0 THEN 0 ELSE ROUND(index_length / data_length, 2) END AS TB_INDX_RATEfrom TABLES where table_schema=&#x27;$database&#x27; and table_name=&#x27;$table&#x27;&quot;echo &quot;--------------------&quot;echo &quot;--------------- $(date +%s) Database: $database, Table: $table, Begin clean ... --------------- &quot;mysql -u$user -p$passwd -P3306 -h$host information_schema -e &quot;$sql&quot;time pt-online-schema-change \\\\ --user=$&#123;user&#125; \\\\ --password=$&#123;passwd&#125; \\\\ --host=$&#123;host&#125; \\\\ P=3306,D=$&#123;database&#125;,t=$table \\\\ --charset=utf8 \\\\ --alter=&quot;ENGINE=InnoDB&quot; \\\\ --nocheck-replication-filters \\\\ --alter-foreign-keys-method=auto \\\\ --executemysql -u$user -p$passwd -P3306 -h$host information_schema -e &quot;$sql&quot;done 执行 CASE 12# 清理 mall 数据库中表 interfacelog stockoperatelog stockshiftflow 的碎片bash clean_pieces.sh mall interfacelog stockoperatelog stockshiftflow MySQL 配置服务器参数基本参数 123auto-increment-increment &#x3D; 2 auto-increment-offset &#x3D; 2 slave-skip-errors &#x3D; all InnoDb基本参数 状态变量 基本配置 innodb_buffer_pool_size 缓冲池并不仅仅缓存素引：它还会缓存行的数据、自适应哈希索引、插入缓冲（Insert Buffer）、锁，以及其他内部数据结构。 来帮助延迟写入，可合并多个写入操作 innodb_log_file_size max_connections 最大连接数，阿里云的 MySQL5.8, 8C16G 默认为 4000 Innodb 参数 innodb_file_per_table 每张表都是一个独立表空间，对应每张表都是一个 idb 文件。 配置成独立表空间后可以对表进行数据、索引碎片整理。 Innodb_flush_log_at_trx_commit 事务刷写磁盘日志的策略 0: 数据不安全，适合配置 slave 机器 1: 默认值，最安全的设置，每次事务后日志都会刷新到磁盘。 2: 每秒刷新一次事务，有丢失的风险，对于 master 有时可以接受; MySQL 进挂掉，不会丢任何事务，整个服务器挂了或断点，可能会丢失一些事务 缓冲区大小 Innodb read io threads innodb write io threads 以上两个参数決定了 Innodb 读写的 I0 进程数，默认为 4 Innodb stats on metadata 决定了 MYSQL 在什么情況下会刷新 innodb 表的统计信息。 配置的内容： 个数 - 字节 - 开启与关闭 - 百分比 table_cache: 表可被缓存的数量 key_buffer_size: 以字节为单位 max_heap_table_size: 指定隐式内存临时表最大允许的大小 table_cache_size: 结果值比缓存中的表数小，MYSQL 将从缓存中删除不常使用的表 thread_cache_size: query_cache_size: sort_buffer_size: innodb_buffer_pool_pages_dirty: 状态变量，缓冲池中的脏页数量 innodb_buffer_pool_instances: 5.5+ 新增 open_files_limit: 设置的较少，可能出现 too many open files 参数动态配置 12345set sort_buffer_size = &lt;value&gt;set GLOBAL sort_buffer_size = &lt;value&gt;SET @@sort_buffer_size := &lt;value&gt;SET @@session.sort_buffer_size := &lt;value&gt;@@global.sort_buffer_size := &lt;value&gt; 控制 Innodb 并发 innodb_thread_concurrency: 并发设置，默认为 0，表示不限制 建议并发值 = CPU 数量 * 磁盘数量 * 2 innodb_thread_sleep_delay: 微秒为单位, 在进入内核线程超过运行的数量后，第一次休眠的时间，之后重试，若不能进入，则放入到等待队列，由 OS 处理 innodb_concurrency_tickets: 较少小改 innodb commit concurrency: 変量控制有多少个线程可以在同时间提交 排序 max_length_for_sort_data： 影响使用哪种排序算法 max_connect_errors: 网络、配置、权限等问题导致大连个链接重试，记录黑名单设置的大，有效地禁用主机黑名单 read_only： 建议备库设置成只读模式 slave_net_timeout： 默 1h，可缩短到 1min I/O 配置 innodb_log_file_in_group 主从配置 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断 1062: 一些主键重复 1032: 主从数据库数据不一致 1slave_skip_errors&#x3D;1062 忽略指定的数据库同步 1binlog-ignore-db&#x3D;mysql sync_binlog 控制何时 binlog 刷新到磁盘上 0: 由 OS 进行控制 1: 每次事务提交的时候写入，最为安全，但影响性能 连接参数Connection createDatabaseIfNotExist： 默认为 false rollbackOnPooledClose: 默认为 true, 在 Driver 因为问题回滚.. useAffectedRows: 默认为 false Session characterEncoding=utf8: 设置 session 对应的编码格式，告知 server 返回的结果编码格式，默认为 “autodetect”。 useUnicode=true： 使用 Unicode serverTimezone=Asia/Shanghai： 设置时区 Networking connectTimeout： 默认为 0，连接超时, 单位 ms socketTimeout: 默认为 0，单位 ms. 网络 socket 操作的超时时间 maxAllowedPacket： 默认为 65535，最大允许发送的网络包 Packet 大小 useCompression： 默认为 false，使用 zlib 压缩，当与服务器通信的时候 Statements cacheDefaultTimezone： 默认为 true，缓存客户端默认时区，MySQL8.0.20 后添加的 queryTimeoutKillsConnection: 默认 false Performance Extensions If ‘cacheCallableStmts’ is enabled, how many callable statements should be cached? Default: 100 metadataCacheSize: 结果元数据缓存的大小，在 cacheResultSetMetaData 设置为 true 时有效，默认 50 prepStmtCacheSize： 多少 prepared statements 被缓存，默认 25 rewriteBatchedStatements=true：默认 false，重写批量的语句，提高批量的操作效率 allowMultiQueries=true： 允许多查询 归档数据库配置可定期对 MySQL 进行数据归档，可考虑使用 Archive 存储引擎，对于归档的数据库可适当减少配置，关闭一些耗时的配置。 配置如下： 使用 4G 的 innodb 缓冲区 更改事务隔离级别为 READ-UNCOMMITTED，一般只是导入导出无事务操作 适当调整 mysqldump 的 max_allowed_packet，方便进行必要数据的恢复 关闭慢查询、binlog 减小相应的 buffer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697[mysqld]port &#x3D; 3306socket &#x3D; &#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.sockpid-file &#x3D; &#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.piddatadir &#x3D; &#x2F;var&#x2F;lib&#x2F;mysqllog-error &#x3D; &#x2F;var&#x2F;log&#x2F;mysql&#x2F;mysql-error.logtmpdir &#x3D; &#x2F;tmp# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links&#x3D;0# binlog setting#log-bin &#x3D; mysql-bin#binlog_format &#x3D; MIXED # can be mixed, decrease ..#binlog_format &#x3D; ROW # can be mixed, decrease ..#binlog_row_image &#x3D; minimalsync_binlog &#x3D; 1 # default 0 affect performance， # safe to guarantee replicationslave_net_timeout &#x3D; 60 # defacult 3600#expire_logs_days &#x3D; 7#relay_log &#x3D; mysql-relay#server_id &#x3D; &#123;&#123;server_id | default(&#39;555&#39;)&#125;&#125;#log_bin_trust_function_creators &#x3D; 1wait_timeout &#x3D; 57600interactive_timeout &#x3D; 57600max_allowed_packet &#x3D; 512M # or 100Mevent_scheduler &#x3D; 1max_connections &#x3D; 2000# log setting &#123;slow query, not using indexs&#125;#slow_query_log &#x3D; &#123;&#123;slow_query_log | default(&#39;0&#39;)&#125;&#125;#slow_query_log_file &#x3D; mysql-slow.log#long_query_time &#x3D; &#123;&#123;long_query_time | default(&#39;0.20&#39;)&#125;&#125;#log_queries_not_using_indexes &#x3D; &#123;&#123;log_queries_not_using_indexes | default(&#39;0&#39;)&#125;&#125;# charactercharacter-set-server &#x3D; utf8mb4# txtransaction-isolation &#x3D; READ-UNCOMMITTED #REPEATABLE-READ req for ACID, SERIALIZABLE req XA# innodb#innodb_buffer_pool_size &#x3D; 768Minnodb_buffer_pool_size &#x3D; 4G # old 128M, 70%-80% mache meminnodb_log_file_size &#x3D; 32M # old 500M, total 1G, can bigger to 1024M # Note: modify need to move old file to other position,may start fail # 64G_RAM+ &#x3D; 768, 24G_RAM+ &#x3D; 512, 8G_RAM+ &#x3D; 256, 2G_RAM+ &#x3D; 128#innodb_log_files_in_group &#x3D; 2 # defacult 2innodb_log_buffer_size &#x3D; 64M # defalut 8Minnodb_buffer_pool_instances &#x3D; 8 # defaultinnodb_flush_log_at_trx_commit &#x3D; 2 # defalut 1, per second trx flush 2&#x2F;0 &#x3D; perf, 1 &#x3D; ACIDinnodb_file_per_table &#x3D; 1innodb_lock_wait_timeout &#x3D; 60 # timeout 500innodb_status_output &#x3D; ONinnodb_status_output_locks &#x3D; ONinnodb_print_all_deadlocks &#x3D; ONinnodb_read_io_threads &#x3D; 6 # default 4innodb_write_io_threads &#x3D; 6 # default 4#innodb_thread_concurrency &#x3D; 16 # default 0 recommend 2x core quantity#innodb_additional_mem_pool_size &#x3D; 8M #default 8M#innodb_open_files &#x3D; 2000 # default 2000, can open *.idb file count# buffer settingsort_buffer_size &#x3D; 256K # default 0.25Mjoin_buffer_size &#x3D; 256K # default 0.25M, can bigger to 128Mread_buffer_size &#x3D; 512K # default 0.125Mread_rnd_buffer_size &#x3D; 512K # default 0.25M#max_length_for_sort_data &#x3D; 1024 # default 1024#max_connect_errors &#x3D; 100 # defacult 100#innodb_doublewrite &#x3D; 1 # default on#thread_concurrency &#x3D; 12 # default 10 recommend 2x CPU cores#thread_cache_size &#x3D; 28 # defacult 28 recommend 5% of max_connections#open_files_limit &#x3D; 1048576 # default 1048576# MyISAM#key_buffer_size &#x3D; 32M # default 8M#query_cache_size &#x3D; 1M # default 1M# table sizetmp_table_size &#x3D; 128M # default 16Mmax_heap_table_size &#x3D; 128M # default 16M recommend same size as tmp_table_size# concatgroup_concat_max_len &#x3D; 100000000[mysql]default-character-set &#x3D; utf8mb4[client]default-character-set &#x3D; utf8mb4[mysqldump]quickquote-namesmax_allowed_packet &#x3D; 256M 配置案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596[mysqld]port = 3306socket = /var/run/mysqld/mysqld.sockpid-file = /var/run/mysqld/mysqld.piddatadir = /var/lib/mysqllog-error = /var/log/mysql/mysql-error.logtmpdir = /tmp# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0# binlog settinglog-bin = mysql-bin#binlog_format = MIXED # can be mixed, decrease ..binlog_format = ROW # can be mixed, decrease ..binlog_row_image = minimal#sync_binlog = 1 # default 0 affect performance， # safe to guarantee replicationslave_net_timeout = 60 # defacult 3600expire_logs_days = 7relay_log = mysql-relayserver_id = 199192log_bin_trust_function_creators = 1wait_timeout = 57600interactive_timeout = 57600max_allowed_packet = 256M # or 100Mevent_scheduler = 1max_connections = 2000# log setting &#123;slow query, not using indexs&#125;slow_query_log = 1slow_query_log_file = mysql-slow.loglong_query_time = 0.20log_queries_not_using_indexes = 1# charactercharacter-set-server = utf8mb4# txtransaction-isolation = REPEATABLE-READ #REPEATABLE-READ req for ACID, SERIALIZABLE req XA# innodbinnodb_buffer_pool_size = 18G # old 128M, 70%-80% mache meminnodb_log_file_size = 500M # old 500M, total 1G, can bigger to 1024M # Note: modify need to move old file to other position,may start fail # 64G_RAM+ = 768, 24G_RAM+ = 512, 8G_RAM+ = 256, 2G_RAM+ = 128#innodb_log_files_in_group = 2 # defacult 2innodb_log_buffer_size = 8M # defalut 8Minnodb_buffer_pool_instances = 8 # defaultinnodb_flush_log_at_trx_commit = 2 # defalut 1, per second trx flush 2/0 = perf, 1 = ACIDinnodb_file_per_table = 1innodb_lock_wait_timeout = 60 # timeout 500innodb_status_output = ONinnodb_status_output_locks = ONinnodb_print_all_deadlocks = ONinnodb_read_io_threads = 6 # default 4innodb_write_io_threads = 6 # default 4#innodb_thread_concurrency = 16 # default 0 recommend 2x core quantity#innodb_additional_mem_pool_size = 8M #default 8M#innodb_open_files = 2000 # default 2000, can open *.idb file count# buffer settingsort_buffer_size = 1M # default 0.25Mjoin_buffer_size = 1M # default 0.25M, can bigger to 128Mread_buffer_size = 1M # default 0.125Mread_rnd_buffer_size = 1M # default 0.25M#max_length_for_sort_data = 1024 # default 1024#max_connect_errors = 100 # defacult 100#innodb_doublewrite = 1 # default on#thread_concurrency = 12 # default 10 recommend 2x CPU cores#thread_cache_size = 28 # defacult 28 recommend 5% of max_connections#open_files_limit = 1048576 # default 1048576# MyISAM#key_buffer_size = 32M # default 8M#query_cache_size = 1M # default 1M# table sizetmp_table_size = 128M # default 16Mmax_heap_table_size = 128M # default 16M recommend same size as tmp_table_size# concatgroup_concat_max_len = 100000000[mysql]default-character-set = utf8mb4[client]default-character-set = utf8mb4[mysqldump]quickquote-namesmax_allowed_packet = 128Mr 其他MySql 大表数据删除Mysql 官网对于大表数据的删除说明 If you are deleting many rows from a large table, you may exceed the lock table size for an InnoDB table. To avoid this problem, or simply to minimize the time that the table remains locked, the following strategy (which does not use DELETE at all) might be helpful: Select the rows not to be deleted into an empty table that has the same structure as the original table: INSERT INTO t_copy SELECT * FROM t WHERE … ; Use RENAME TABLE to atomically move the original table out of the way and rename the copy to the original name: RENAME TABLE t TO t_old, t_copy TO t; Drop the original table: 123456INSERT INTO copy_interfacelog SELECT * FROM mall.servicelog WHERE logtime between &#x27;2020-04-07 02:00:00&#x27; and &#x27;2020-04-14 10:23:24&#x27;;-- 使用 mysqldump, file...备份恢复RENAME TABLE mall.servicelog TO mall.old_servicelog copy_interfacelog TO interfacelog; 通过 INSERT INTO &lt;table-name&gt; SELECT .. 筛选出不需要删除的数据 1234567DELETE FROM `table` WHERE (whatever criteria) ORDER BY `id` LIMIT 1000MYSQL=&quot;mysql -uroot -p$MYSQL_ROOT_PASSWORD &quot;sql=&quot;select uuid from mail.interfacelog where (condition) order by uuid desc limit 1000 &quot;for i in `seq 1 1000`; do $MYSQL -e &quot;$sql&quot; | sed &#x27;s;/|;;g&#x27; | awk &#x27;&#123;if(NR&gt;1)print &quot;delete from table_name where uuid = &quot;,$1,&quot;;&quot; &#125;&#x27; | $MYSQL; done RefDeleting millions of rows in MySQL MySQL :: MySQL 8.0 Reference Manual :: 13.7.3.4 OPTIMIZE TABLE Statement 官网对于 OPTIMIZE TABLE 说明 潇湘隐者 MySQL 数据存储碎片化 Optimized my.cnf configuration for MySQL/MariaSQL (on Ubuntu, CentOS etc. servers) 优化的 mysql 配置 Ten MySQL performance tuning settings after installation percona 网站对于 MySQL 性能调整配置说明 https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-reference-configuration-properties.html 官网对于属性的配置 What’s the difference between cachePrepStmts and useServerPrepStmts in MySQL JDBC Driver https://stackoverflow.com/questions/2993251/jdbc-batch-insert-performance 批量操作参数的更改","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}]},{"title":"MySQL-备份与恢复","date":"2021-04-08T17:06:57.000Z","path":"2021/04/09/MySQL-备份与恢复/","text":"备份概述 备份决定了数据库的安全，在主从不一致的情况下删除从库的所有数据，进行数据重新整理。 对于 MyISAM 备份 1FLUSH TABLES WITH READ LOCK; 备份策略逻辑备份： 结果为 sql 文件，方法适用于所有 Engine 物理备份： 数据库目录的拷贝，对于内存表只备份结构 备份的内容： 事务的 innodb 表，不带有事务的(无法保证一致性) 函数(routines)、触发器、视图 包含的表，有时无需对整个数据库所有的表进行备份，只需要备份重要的业务表，用于以后的恢复 排除的表，在做全量备份的时候，排除掉日志表、记录表、备份表等与业务非强相关的表，提高备份的效率 备份的类型： 热备份： 在线备份，不需要任何的服务停机时间，ibbackup 商业工具可实现 冷备份： 备份 .frm, .idb 文件，需要离线备份 温备份： 非离线备份，在线执行，会影响线上的数据库运行 备份的方案: 全量备份 增量备份 压缩备份 加密备份 备份的数据存放到另一块物理磁盘上或是传输到另一台机器上 12345678910SELECT data INTO OUTFILE xxx.txtLOAD data INFILE xxx.txt INTO TABLEselect * from dc_mp_fans into outfile &#x27;/data/fans.txt&#x27;;zip fans.zip /data/fans.txtscp fans.zip root@ip:/data/ unzip /data/fans.zipload data infile &#x27;/data/fans.txt&#x27; into table wxa_fans( id,appid,openid,unionid,created_at); 备份场景 备份业务中重要的业务表 日志表的归档，归档到另一个表中 备份/导出/ETL 到大数据平台 强制恢复处理恢复过程中启动不起来的情况 MySQL 配置参数 innodb_force_recovery 控制 1: (SRV_FORCE_IGNORE_CORRUPT): 忽略检查到的 corrupt 页。 2: (SRV_FORCE_NO_BACKGROUND): 阻止主线程的运行，如主线程需要执行 full purge 操作，会导致 crash。 3: (SRV_FORCE_NO_TRX_UNDO): 不执行事务回滚操作。 4: (SRV_FORCE_NO_IBUF_MERGE): 不执行插入缓冲的合并操作。 5: (SRV_FORCE_NO_UNDO_LOG_SCAN): 不查看重做日志，InnoDB 存储引擎会将未提交的事务视为已提交。 6: (SRV_FORCE_NO_LOG_REDO): 不执行前滚的操作。 当 innodb_purge_threads 和 innodb_force_recovery 一起设置会出现一种loop 现象 12innodb_force_recovery&#x3D;6innodb_purge_thread&#x3D;0 当设置参数值大于0后，可以对表进行 select,create,drop 操作,但 insert,update 或者 delete 这类操作是不允许的 MySQL crash 或者 MySQL 数据库服务器 crash 会导致各种各样的问题 ，比如主备之间的 error 1594 (5.6 版本开启crash-safe ，会最大程度上避免 error 1594的问题)，error 1236， 日志损坏，数据文件损坏等。 物理文件离线备份离线方式的备份，需要停止整个服务 存在复制过程中的文件损坏，对应表空间没法使用，数据库服务一直尝试重启 注意数据库的 .frm, .idb 的权限 660；目录为 700；目录的属主为 mysql:mysql 基于日志点的恢复增量恢复 备份二进制日志，使用 FLUSH LOGS 开始一个新的二进制日志 https://dba.stackexchange.com/questions/60722/how-to-recover-truncate-table-in-mysql mysqlbinlog： --start-position: 开始位置 --stop-position: 停止位置 1234567891011mysqlbinlog -v mysqlbin.000002 | grep -B5 TRUNCATE --colormysqlbinlog \\\\ --start-position=1603 \\\\ --stop-position=919664 \\\\ mysqlbin.000002 &gt; /mysqlbackup/binlog_`date +%y%m%d%H`.sqlshow binary logs;show master logs;SHOW BINLOG EVENTS;show binlog events in &quot;mysql-bin.000005&quot;;mysqlbinlog binlog.[0-9]* | mysql -u root -p 导入导出可直接使用 DBver, Navicate, DataGrip 等图形化连接客户端进行各种格式的导出，包含 INSERT 语句、UPDATE 语句、HTML 表格、CSV(TSV)、JSON、格式化的text文本、Markdown 的表格等。 SELECT … INTO OUTFILE 需要登录的 mysql 账号具有 FILE 权限 123select * from entitylog into outfile &#x27;/var/lib/mysql/backup/entitylog.txt&#x27;;-- 指定分割符合换行符， 导出为 CSV 格式select * from entitylog into outfile &#x27;/var/lib/mysql/backup/entitylog.txt&#x27; fields terminated by &#x27;,&#x27; enclosed by &#x27;&quot;&#x27; lines terminated by &#x27;\\\\r\\\\n&#x27;; mysql 命令导出 12-- 导出为 CSV 格式mysql -uroot -p -e &quot;select * from entitylog&quot; --skip-column-names test|sed -e &quot;s/[\\\\t]/,/&quot; -e &quot;s/$/\\\\r/&quot; &gt; entity.txt mysqldump MySQL 自带的备份工具，可以对数据库进行全备份和部分备份,不支持增量备份。 使用场景： 10G 以下的数据库操作简单 缺点： 数据量范围：30G –&gt; TB级别 的时候备份、恢复操作很慢，效率低 基本配置： -h / -P / -u / -p: 基本连接参数 --single-transaction: 保证导出的数据一致性，保证备份 InnoDB 的一致性逻辑备份，和 –lock-tables 选项是互斥的 -l / --lock-tables:一般用于 MyIsam 备份，与上面的互斥 x, --lock-all-tables: 数据库 -master-data: [1/2] 备份内容配置: --all-databases: 备份全部的 DB --databases &lt;db1&gt; &lt;db2&gt;: 备份指定的多个数据库 --tables a1 a2: 备份指定的多个 Table -w, --where=&quot;&quot;: 备份表中筛选后的数据 -R, --routines: 备份 procedures, functions --triggers: 备份触发器 -e, --events: --no-data: 只导出表结构，不导出数据 控制输出内容配置： --skip-add-drop-table: 取消每个数据表创建之前添加 drop 数据表语句(默认每个表之前存在drop语句)，对于部分恢复的情况下需要注意。 --skip-add-locks: 跳过锁表语句 --no-create-info： 导出的 SQL 中不包含 create table 语句 --set-gtid-purged： 跳过导 GTID --add-drop-database: 增加删除数据库 sql（默认不会） -opt： 等同于 --add-drop-table, --add-locks, --create-options, --quick, --extended-insert, --lock-tables, --set-charset, --disable-keys，默认开启 -c, --complete-insert: 生成的 Insert 语句中带有字段名称, insert into T(col1,col2..) values(…) 其他配置 –-lock-all-tables: 锁住所有的表，表变为只读的 –master-data=: 将当前服务器的binlog的位置和文件名追加到输出文件，不会停止当前服务器的主从服务 -F: 生成新的 binlog 文件 -C： 启用压缩传递 --dump-slave 全量备份1234mysqldump --all-databases \\\\ --master-data \\\\ --single-transaction \\\\ &gt; backup_$(date +%y%m%d).sql 多服务器数据传输1234mysqldump --host=h1 -uroot -proot --databases db1 | mysql --host=h2 -uroot -proot db2# 压缩传输mysqldump --host=192.168.80.137 -uroot -proot -C --databases test | mysql --host=192.168.80.133 -uroot -proot test 压缩备份线上环境导入导出使用 123456789# 压缩备份mysqldump -P3306 \\\\ -uroot -p \\\\ -q -Q --set-gtid-purged=OFF \\\\ --default-character-set=utf8 \\\\ --hex-blob --skip-lock-tables \\\\ --databases abc 2&gt;/abc.err |gzip &gt;/abc.sql.gz# 还原gunzip -c abc.sql.gz | mysql -uroot -p -vvv -P3306 --default-character-set=utf8 abc 1&gt; abc.log 2&gt;abc.err 按条件备份备份指定数据库 123456789mysqldump \\\\ -hlocalhost -P3306 \\\\ -uroot -p$MYSQL_ROOT_PASSWORD \\\\ --add-drop-table \\\\ --master-data=2 \\\\ --single-transaction \\\\ --routines --triggers --events \\\\ --databases account basic report \\\\ &gt; backup_$(date +%y%m%d).sql 备份满足特定条件的表 12345678910111213# 备份指定时间的日志数据mysqldump -uroot -p$MYSQL_ROOT_PASSWORD \\\\ --databases basic \\\\ --tables interfacelog \\\\ --where=&quot;logtime &lt; &#x27;2020-05-01 00:00:00&#x27; and logtime &gt; &#x27;2020-04-08 00:00:00&#x27;&quot; \\\\ &gt; backup-interfacelog-`date +%y%m%d` # 从重命名的表中备份数据mysqldump -uroot -proot \\\\ --databases basic \\\\ --tables old_interfacelog \\\\ --where=&quot;logtime between &#x27;2020-04-13 00:00:00&#x27; and &#x27;2020-04-14 00:00:00&#x27;&quot; \\\\ &gt; backup-interfacelog-`date +%y%m%d` 备份表并在 insert 语句中插入字段 使用场景： 备份数据，并恢复到变更表结构的该表 归档表重命名原始表名，对应 INSERT INTO 与生产库的表名一致 对于表结构变更的，需要 Insert 增加字段名 用于从归档到生产库的，需要去除逻辑备份前面的 Drop 语句 1234567891011121314151617181920212223# 测试输出的结果db=accounttable=entitylogtime mysqldump -uroot -p$MYSQL_ROOT_PASSWORD \\\\ --databases $db \\\\ --tables $table \\\\ --complete-insert \\\\ --skip-add-locks \\\\ --no-create-info \\\\ --skip-add-drop-table \\\\ --where=&quot;1=1 limit 0,1&quot; \\\\ &gt; test.sqldb=facilitytable=&quot;stockoperlog &quot;time mysqldump -uroot -p$MYSQL_ROOT_PASSWORD \\\\ --databases $db \\\\ --complete-insert \\\\ --tables $table \\\\ --no-create-info \\\\ --skip-add-locks \\\\ --skip-add-drop-table \\\\ --where=&quot;logtime &gt; &#x27;2021-01-01 00:00:00&#x27;&quot; \\\\ &gt; stockoperlog_210101_`date +%y%m%d`.sql 备份表结构1234mysqldump -uroot -proot \\\\ --no-data \\\\ --databases account \\\\ &gt; /tmp/account_schema.sql 备份恢复专门创建一个备份数据库，查询出必要的数据，之后插入到实际使用的表 使用 table 进行恢复，只对需要的进行恢复 进行时间点的恢复 前提： 一个时间点的全备 对应的 binlog 123456# 将 SQL 恢复到指定的数据库mysql -uroot -proot \\\\ basic &lt; backup_200604_basic.sqlmysql -uroot -proot \\\\ account &lt; backup_200604_account.sql xtrabackup 可对数据库进行全备和增量备份，使用 binlog 对数据库进行时间点的… 支持在线的物理备份 配置 在 my.cnf 中指定 12[xtrabackup]target_dir &#x3D; &#x2F;data&#x2F;backups&#x2F;mysql&#x2F; 基础参数 --print-defaults: 打印程序参数和列表并退出 --target-dir=&lt;path&gt;: 备份到的目标文件夹 --backup: take backup to target-dir --prepare: 准备备份以在备份上启动 mysql 服务器。 --export: --print-param: --rate-limit: 限制备份脚本的吞吐量 备份过滤的参数： --tables=name / : 根据正则表达式过滤表名，过滤列表根据 --tables-file=name: 根据文件中的 database.table 进行过滤 --databases=name: 根据数据库列表过滤 --tables-exclude=name: 排除指定的表, 与 --tables 相反，优先级比 --tables 高 --databases-exclude=name: 排除指定的数据库 压缩备份参数 --compress[=name]: 是否进行压缩备份 --compress： 进行压缩备份 --compress-threads=N: 指定压缩的线程数 --decompress: 解压缩恢复备份数据 增量备份参数 --incremental-basedir=&lt;path&gt;： 增量备份基于的全量备份文件夹 数据库恢复参数 --copy-back: 复制之前所有的文件 其他参数 --slave-info: 主从复制备份有效 --safe-slave-backup: –-binlog-info[=name] 全量备份123456789MYSQL_BACKUP_DIR=/var/log/mysql/backups/$dir_name/Full_$(date +%Y.%m.%d_%H.%M.%S)mkdir -p $MYSQL_BACKUP_DIRxtrabackup --backup \\\\ --target-dir=$MYSQL_BACKUP_DIR \\\\ -uroot -proot xtrabackup --prepare \\\\ --target-dir=$MYSQL_BACKUP_DIR \\\\ -uroot -proot 有条件的全量备份xtrabackup 当前只能指定到表级别，没法对表的数据进行筛选 若需要对表的内容进行筛选，可考虑使用 mysqldump 进行逻辑备份，配合 xtrbackup 的物理备份实现 排除指定表进行备份 可根据实际需要列出需要排除的大表，拼接成正则或是到文件中进行过滤； 可通过 (.*log.*|.*record.*|.*bak.*|.*\\\\d+.*) 正则表达式进行排除不相关的表； 1234567891011121314151617181920212223242526272829# 1. define back infoMYSQL_BACKUP_DIR=/var/lib/mysql/backupback_up_dir=$MYSQL_BACKUP_DIR/Full_$(date +%Y.%m.%d_%H.%M.%S)# 1.2 compare back info# Filter pattern: (.*log.*|.*record.*|.*bak.*|.*\\\\d+.*)mkdir -p $back_up_dirtable_exclude_names=( &quot;account.entitylog&quot; &quot;account.mqlog&quot; &quot;basic.interfacelog&quot; &quot;basic.article_bak&quot;)function join() &#123; local IFS=&quot;$1&quot; shift echo &quot;$*&quot;&#125;join_str=$(join \\\\| &quot;$&#123;table_exclude_names[@]&#125;&quot;)exclude_table=&quot;($join_str)&quot;echo &quot;exclude_table: $exclude_table&quot;# 2. 非压缩备份time xtrabackup --backup \\\\ --host=127.0.0.1 \\\\ -uroot -proot \\\\ --tables-exclude=&quot;$exclude_table&quot; \\\\ --datadir=/var/lib/mysql/ \\\\ --target-dir=$back_up_dir 备份指定的表 单独备份表的话需要表在独立的表空间，对应 innodb_file_per_table=1 压缩备份需要使用 qpress 工具 1234567891011121314151617# 安装 qpresspercona-release enable toolsapt-get updateapt-get install qpressMYSQL_BACKUP_DIR=/var/log/mysql/backups/$dir_name/Full_$(date +%Y.%m.%d_%H.%M.%S)mkdir -p $MYSQL_BACKUP_DIR xtrabackup --backup \\\\ --compress \\\\ --compress-threads=4 \\\\ --target-dir=$MYSQL_BACKUP_DIR # 解压缩xtrabackup --decompress \\\\ --target-dir=/data/compressed/xtrabackup --copy-back \\\\ --target-dir=/data/backups/ 数据恢复 数据目录在恢复前必须为空 在执行恢复前，MySQL 服务需要关闭 12xtrabackup --copy-back \\\\ --target-dir=$MYSQL_BACKUP_DIR 增量备份增量并压缩备份 12345678datadir=/var/lib/mysql/backupxtrabackup --user=root \\\\ --password=root \\\\ --backup \\\\ --compress \\\\ --compress-threads=4 \\\\ --target-dir=$&#123;datadir&#125;/inc$&#123;today&#125; \\\\ --incremental-basedir=/var/lib/mysql/backup/Full_2020.08.23 比较偏移的点 123456789101112131415cat xtrabackup_checkpointsbackup_type = full-backupedfrom_lsn = 0to_lsn = 10820498082last_lsn = 10820498082compact = 0recover_binlog_info = 0flushed_lsn = 10820475795backup_type = incrementalfrom_lsn = 10820498082to_lsn = 10820575291last_lsn = 10820575291compact = 0recover_binlog_info = 0flushed_lsn = 10820572909 数据库恢复准备 可以在任何机器上运行 prepare 命令，无需在原始服务器上, 可复制备份到指定机器上进行恢复 准备步骤使用此嵌入式 InnoDB 对复制的数据文件执行崩溃恢复 1xtrabackup --prepare --target-dir=/data/backups/ 复制与授权复制备份文件 方式一: 使用 xtraback 提供的功能 1xtrabackup --copy-back --target-dir=/data/backups/ 方式二: 使用 rsync 或 cp 复制备份文件到指定目录 12rsync -avrP /data/backup/ /var/lib/mysql/# 2 cp 授权 1chown -R mysql:mysql /var/lib/mysql 其他对于数据量大的情况，可以考虑将大表的数据通过 ETL 工具导出到大数据平台进行存储，业务系统中只保留最近 1年的数据。 RefMySQL mysqldump数据导出详解 MySql数据库备份与恢复–使用mysqldump 导入与导出方法总结_helloxiaozhe的博客-CSDN博客_mysqldump备份数据库 MySql数据库备份与恢复——使用mysqldump 导入与导出方法总结 Percona XtraBackup - Documentation Percona XtraBackup 2.4 官方文档 Partial Backups Percona 官网部分备份说明文档 使用innobackupex对数据库进行部分备份(指定表或数据库) 使用innobackupex对数据库进行部分备份(指定表或数据库) MySQL 导出数据 MySQL 导出数据","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}]},{"title":"Mysql-日志","date":"2021-04-08T17:06:49.000Z","path":"2021/04/09/Mysql-日志/","text":"慢查询日志 MySQL 的动态参数，可以随关随停 慢查询优化优化策略： 调优 TOP10, 之后迭代… 业务扩展，用户流量增加，进一步调优 慢查询日志的性能剖析工具：汇总一些信息，自动排序 查看慢日志： 正常的格式 explain EXPLAIN 关键字模拟优化器执行 SQL 查询语句 12345explain select star, evaluator_no from indicator_evaluate order by evaluator_no desc; id: 代表执行的顺序 type: 找到数据行的方式, 数据的访问类型，取值为如下 all: 全表扫描 index: ALL 和 INDEX 都读全表，INDEX 从索引读， ALL 从硬盘读 range： between, in, &gt;, &lt; 等的查询，无需扫全表 ref：非唯一性索引扫描，返回匹配.. eq_ref: 唯一性索引扫描，对于每一个索引键，表中只有一条记录与之匹配。常见于主键或者唯一索引扫描。 const: 表示通过索引一次就找到了，const 用于比较为 primary key(主键索引)或者是 unique 索引，因为只匹配一行，所以很快，若将主键作为 where 条件，MySQL 就会把该查询作为一个常量 system ：表中只有一条记录(等同于系统表) 这是 const 特例，平时不会出现。 1234# 最好达到 ref OR range 级别system&gt;const&gt;eq_ref&gt;ref&gt;range&gt;index&gt;all extra： Using where, Using index， Using filesort 最重要的 extra 的取值是前三个 filesort、 using temporary(性能差的sql语句) 、use index(性能好的 sql语句)； 是查询优化器进行选择索引的一个参数，及排序的规则；extra 中出现下面两项意味着 MySQL 不能使用索引，效率会有很大的影响，需尽早优化 Using filesort: 外部索引排序，不是从表中按索引次序读取相关内容，可能在内存或磁盘上排序，MySQL 无法利用索引完成的排序。 Using temporary: 使用临时表，在 order by 和 group by 中常见 select_type: 查询的类型 有以下六种取值 simplye: 表示简单查询 ，不包含子查询以及 union primary: 若查询中包含了任何的子查询，最外层的主查询标识为 primary subquery: 标识为子查询 derived: 派生的，在 from 子查询的结果会被放入为衍生虚表(临时表) union: 若第二个 select 出现在 union 之后，则会标记为 union(若 union 包含在 from 子句的子查询中，外层的的 select 标识为 derived) union result：从 union 表中获取数据的 select 标识 key:实际上使用的索引,为 null 表示为索引失效 possiable_keys: 理论上可能使用到的索引，若在查询的时候使用了覆盖索引，那么该索引就不会出现在 possible_keys 中，而只会出现在 key 列中。 key_len：表示索引在使用的字节数，可以通过该列计算查询中使用的索引的长度，长度越小越好 key_len 显示的值为索引字段的最大可能长度，而非使用长度，及key_len 是根据表定义计算而得，不是通过表内检索出来的。 ref: 表示索引的哪一列被使用，如果可能的话是一个常数，哪些列或者常量被用于索引列上的查找。 rows: 根据表信息统计，估算出大约要扫描的行数。 优化： &amp;手动优化 ① 修改SQL: 在业务允许的情况下使得语句走对应的索引； 1EXPLAIN SELECT evaluator_no, course_id from evaluator; ② 添加索引：对于无法通过修改 SQL 满足业务的情况下，而此 SQL 又进行多次的查询，对其进行添加索引处理 1alter table &lt;table&gt; add index idx_name(&lt;col&gt;); &amp;查询优化器优化 不使用密集索引，稀疏索引为二级索引不存放对应的全行信息； 查询的不需要整体信息； 记住抽样统计，同时查询是否排序、是否使用临时表进行索引的选择； 123EXPLAIN SELECT COUNT( id ) FROM person_info_large;-- 测试使用什么索引所以更好&lt;sql&gt; FORCE INDEX(&lt;index&gt;); mysqldumpslow mysql 自带的工具，较为简单 按序显示执行的 SQL 情况 1mysqldumpslow --verbose slow-query.log -c: 总次数 -t: 时间 l: 锁的时间 r: 总数据行 at,al,ar: 平均数 按照时间排序的前10个查询 1234mysqldumpslow -s t -t 10 slow-query.logmysqldumpslow -s c -t 10 slow-query.logmysqldumpslow -s l -t 10 slow-query.logmysqldumpslow -s r -t 10 slow-query.log pt-query-digest percona toolkit 中提供 功能增强的慢查询分析工具，具体使用见 这里 123pt-query-digest \\\\ --type slowlog \\\\ slow-query.log 根据 STDIN 进行分析 1pt-query-digest --query &quot;select * from mysql.user&quot; 配置配置项： slow_query_log=1： 开启慢查询； long_query_time=0.2： 设置慢查询的时间阈值，设置为 200 毫秒； slow_query_log_file：设置慢查询日志文件目录； log_queries_not_using_indexes=1： 显示没有使用索引的 SQL 语句； 1234SHOW VARIABLES LIKE &#x27;%quer%&#x27;; --慢查询日志开启、位置、时长，查询缓冲、SHOW STATUS LIKE &#x27;%slow_queries%&#x27;; -- 当前慢查询数-- Set slow querySET GLOBAL slow_query_log=on;SET GLOBAL long_query_time=1; -- 当前会话有效, 修改 my.ini 永久 Binlog概述 逻辑日志。为归档日志；记录了完整的逻辑记录；属于 Server 层的日志，可作用于任何存储引擎； 包含了一些事件，这些事件描述了数据库的改动，如建表、数据改动等，主要用于备份恢复、回滚操作等。 binlog 有三种格式：Statement, Row 和 Mixed. Statement: 基于 SQL 语句的复制（statement-based replication, SBR） ： 对一些函数如 now() 在不同主机上执行结果不同会出现不一致的问题 Row: 基于行的复制（row-based replication, RBR): 不会出现某些特定情况下的存储过程，或 function，以及 trigger 的调用和触发无法被正确复制的问题。 基于 Row 的，数据恢复更快 Mixed: 混合模式复制（mixed-based replication, MBR），混合 statement 和物理文件，减少 binlog 的大小 生成新的 binlog 1flush logs 生成新的 binlog 的时机 MySQL 服务器停止或重启时执行； 使用 flush logs 命令； 当 binlog 文件大小超过 max_binlog_size 系统变量配置的上限时； 查看相关 查看 binblog 12show binary logs;show master logs;; 查看 binlog 文件的内容 1show binlog events in &quot;mysql-bin.000005&quot;; 两阶段提交为了保证 binlog 和 redo log 的一致性使用，如果不使⽤“两阶段提交”，那么数据库的状态就有可能和⽤它的⽇志恢复出来的库的状态不⼀致； 分为 prepare 和 commit 两个阶段； 对于 prepare 状态的事务，参考 binlog，若该事务在 binlog 中存在，则将其提交，不存在，则将其回滚，这样保证主从之间的一致性。 不只是误操作后需要⽤这个过程来恢复数据。当需要扩容的时候，也就是多搭建⼀些备库来增加系统的读能⼒的时候，常⻅的做法是⽤全量备份加上应⽤ binlog 来实现的，这个“不⼀致”就会导致你的线上出现主从数据库不⼀致的情况。 简单说，redo log 和 binlog 都可以⽤于表示事务的提交状态，⽽两阶段提交就是让这两个状态保持逻辑上的⼀致。 用途数据库恢复 借助 binlog 配合一次全量备份实现指定时间点数据的恢复 一些参数 --start-position: 开始位置 --stop-porition: 结束位置 --start-datetime: 开始时间 --stop-datetime: 结束时间 --database: 指定数据库 -no-defaults: 处理默认字符集问题 123456mysqlbinlog mysql-bin.000001 | mysql -uroot -prootmysqlbinlog mysql-bin.000002 | mysql -uroot -proot.....mysqlbinlog \\\\ --start-datetime=&quot;2005-12-25 11:25:56&quot;\\\\ binlog.000003 多个 binlog 的恢复 1mysqlbinlog binlog.[0-9]* | mysql -u root -p 更改一内容再恢复 123456mysqlbinlog binlog.000001 &gt; tmpfile... edit tmpfile ...mysql -u root -p &lt; tmpfilemysqlbinlog --no-defaults \\\\ -v --base64-output=DECODE-ROWS \\\\ mysql-bin.000009 指定位置进行重新执行 123mysqlbinlog --start-position=27284 \\\\ binlog.001002 binlog.001003 binlog.001004 | mysql --host=host_name -u root -p 与 redo log 比较binlog 与 redo log 日志的区别： redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使⽤。 redo log 是物理⽇志，记录的是“在某个数据⻚上做了什么修改”；binlog是逻辑⽇志，记录的是这个语句的原始逻辑，⽐如“给ID=2这⼀⾏的c字段加1”。 redo log 空间固定会⽤完；binlog是可以追加写⼊的。“追加写”是指binlog ⽂件写到⼀定⼤⼩后会切换到下⼀个，并不会覆盖以前的⽇志。 日志比较 实时监控 binlog适用场景： 通过实时对 binlog 进行监控分析，将 binlog 中的数据按照业务进行分开，控制拆分的逻辑 通过实时监控 binlog，根据事件的类型，删除或更改基于数据库的倒排索引，以此来保证索引的有效性 通过实时对 binlog 进行读取，将数据写入到 Kafka 中，交由流式处理工具(如Flink) 做实时的分析 实现方案： 通过第三方库操作(阿里的 canal)，因为 mysql 支持通过远程方式下载指定 mysql-server 上的 binlog，从而能够实现借助端口对 MySQL 进行运行情况监控和更改信息的获取。 binlog 清理12345678910111213-- 删除指定日期以前的日志索引中 binlog 日志文件purge master logs before &#x27;2016-09-01 17:20:00&#x27;; -- 删除指定日志文件的日志索引中binlog日志文件-- 将 bin.000022 之前的binlog清掉purge master logs to &#x27;mysql-bin.000022&#x27;; -- 清除master.info文件、relay-log.info文件，以及所有的relay log文件,并重新启用一个新的relaylog文件-- 使用reset slave之前必须使用stop slave 命令将复制进程停止reset slave-- 将删除日志索引文件中记录的所有binlog文件，创建一个新的日志文件，起始值从000001开始。不要轻易使用该命令，这个命令通常仅仅用于第一次用于搭建主从关系的时的主库reset master: 配置配置项 log-bin：指定 binlog 的文件名 innodb_flush_log_at_trx_commit： expire_logs_days：保留 binlog 的天数 max_binlog_size： binlog 单个文件的大小，默认 1G，一个事务所产生的所有事件必须记录在同一个 binlog 文件中，所以即使 binlog 文件的大小达到 max_binlog_size 参数指定的大小，也会写入到 binlog 后才能切换。 配置更改 1234567[mysqld] log-bin &#x3D; mysql-binbinlog_format &#x3D; ROW # can be mixed, decreasebinlog_row_image &#x3D; minimalexpire_logs_days &#x3D; 30sync_binlog &#x3D; 1 # default 0 affect performance， safe to guarantee replicationinnodb_flush_log_at_trx_commit &#x3D; 2 # defalut 1, per second trx flush 2&#x2F;0 &#x3D; perf, 1 &#x3D; ACID 会话级别更改 123# modifySET SQL_LOG_BIN&#x3D;0 SET GLOBAL expire_log_days&#x3D;3; redolog (物理) 为物理日志。重做日志。InnoDB 在处理更新语句的时候，只做了写⽇志这⼀个磁盘操作。这个⽇志叫作 redo log，在更新内存写完 redo log 后，就返回给客户端，本次更新成功。 作用： 确保事务的持久性 防止在发生故障的时间点，尚有脏页未写入磁盘，在重启 MySQL 服务的时候，根据 redo log 进行重做，从而达到事务的持久性这一特性 只有 InnoDB 有，其他引擎没有； redolog 确保事务的持久性。 由来： 如果每⼀次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很⾼。 包含两部分，一是内存中的日志缓冲(redo log buffer)，该部分日志是易失性的；二是磁盘上的重做日志文件(redo log file)，该部分日志是持久的。 WAL**WAL(Write-Ahead Logging)**： 预写日志。先写⽇志，再写磁盘。利⽤ WAL 技术，数据库将随机写转换成了顺序写，⼤⼤提升了数据库的性能。 但是，由此也带来了内存脏⻚的问题。脏⻚会被后台线程⾃动 flush，也会由于数据⻚淘汰⽽触发 flush，⽽刷脏⻚的过程由于会占⽤资源，可能会让更新和查询语句的响应时间⻓⼀些。 刷新脏页Q： MySQL “抖” 了一下的原因 A：当内存数据⻚跟磁盘数据⻚内容不⼀致的时候，我们称这个内存⻚为“脏⻚”。 内存数据写⼊到磁盘后，内存和磁盘上的数据⻚的内容就⼀致了，称为“⼲净⻚”。 平时执⾏很快的更新操作，其实就是在写内存和⽇志，⽽ MySQL 偶尔“抖”⼀下的那个瞬间，可能就是在刷脏⻚(flush)。 几种可能的原因： ① InnoDB 的 redo log 写满了。这时候系统会停⽌所有更新操作，把 checkpoint 往前推进，redo log留出 空间可以继续写。 ② 系统内存不⾜。当需要新的内存⻚，⽽内存不够⽤的时候，就要淘汰⼀些数据⻚，空出内存给别的数据⻚使⽤。如果淘汰的是“脏⻚”，就要先将脏⻚写到磁盘。 ③ MySQL 认为系统“空闲”的时候，即使是“⽣意好”的时候，也要⻅缝插针地找时间，只要有机会就刷⼀点“脏⻚” ④ MySQL正常关闭的情况，这时候，MySQL 会把内存的脏⻚都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。 对性能的影响： 对于 ① 出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。 对于 ② 这种情况其实是常态。InnoDB ⽤缓冲池(buffer pool)管理内存，缓冲池中的内存⻚有三种状态：第⼀种是，还没有使⽤的； 第⼆种是，使⽤了并且是⼲净⻚； 第三种是，使⽤了并且是脏⻚。 InnoDB 刷脏页的控制策略 要正确地告诉 InnoDB 所在主机的 IO 能⼒，这样 InnoDB 才能知道需要全⼒刷脏⻚的时候，可以刷多快。 可能的问题： MySQL 的写⼊速度很慢，TPS 很低，但是数据库主机的 IO 压⼒并不⼤ 1innodb_io_capacity # 会告诉 InnoDB 磁盘能⼒,建议设置成磁盘的 IOPS 如果你来设计策略控制刷脏⻚的速度，会参考哪些因素呢？ InnoDB 的刷盘速度就是要参考这两个因素：⼀个是脏⻚⽐例，⼀个是 redo log 写盘速度。 参数 innodb_max_dirty_pages_pct 是脏⻚⽐例上限，默认值是 75%。InnoDB 会根据当前的脏⻚⽐例(假设为M)，算出⼀个范围在 0 到 100 之间的数字，计算这个数字的伪代码类似这样： 根据上述算得的F1(M)和F2(N)两个值，取其中较⼤的值记为R，之后引擎就可以按照innodb_io_capacity定义的能⼒乘以R%来控制刷脏⻚的速度。 配置 innodb_flush_log_at_trx_commit 123456789101112131415161718192021222324252627282930313233343536 ：指定何时将事务日志刷到磁盘，默认为1。 - 0： 事务提交时不会将 log buffer 中日志写入到 os buffer，而是每秒写入 os buffer 并调用fsync() - 系统崩溃，会丢失 1 秒钟的数据。 - 1：每次 commit 都会把 redo log 从 redo log buffer 写入到 system，并 fsync 刷新到磁盘文件中。 - 即使系统崩溃也不会丢失任何数据，但是因为每次提交都写入磁盘，IO 的性能较差。 - 2： 每次事务提交时 MySQL 会把日志从 redo log buffer 写入到 system，但只写入到 file system buffer，由系统内部来 fsync 到磁盘文件。- &#96;innodb_log_buffer_size&#96;: redo 日志的缓冲区，默认为 8M，延迟事务日志写入磁盘- &#96;innodb_log_files_in_group&#96;： redo 日志的个数，默认为2，命名为 ib_logfile&lt;N&gt;- &#96;innodb_log_file_size&#96;： 事务日志的大小- &#96;innodb_log_group_home_dir&#96;： 事务日志组路径，当前目录表示数据目录## undo 日志&gt; 属于 InnoDB 存储引擎特有的日志，做事务的处理。**提供回滚和多个行版本控制(MVCC)**。为逻辑日志，默认存放在共享表空间中，如果配置了 innodb_file_per_table，将会存放在 &lt;table-name&gt;.ibd 中。相关的一些问题### 配置- innodb_max_undo_log_size:- innodb_undo_tablespaces:- innodb_undo_log_truncate:- innodb_purge_rseg_truncate_frequency:&#96;&#96;&#96;sqlshow global variables like &#39;%undo%&#39;; 其他relay log 中继日志，是复制过程中产生的日志。 relay log 是从库服务器 I/O 线程将主库服务器的二进制日志读取过来记录到从库服务器本地文件，然后从库的 SQL 线程会读取 relay-log 日志的内容并应用到从库服务器上。 配置 relay_log： max_relay_log_size： relay_log_recovery 错误日志查找 MySQL 错误日志，查看 MySQL 普通的日志 通过在 mysql.cnf 中配置错误日志的地址 log_error：on|文件路径 是否启用错误日志,on表示开启,文件路径表示指定自定义日志路径 log_warnings： 1|0 是否记录warnings信息到错误日志中 1show variables like &quot;log_error&quot;; 查询日志 general_log： on / off general_log_file：文件保存地址 1234-- 查看show global variables like &quot;%genera%&quot;;-- 开启set global general_log = on; RefsMySQL :: MySQL 8.0 Reference Manual :: 4.6.8 mysqlbinlog - Utility for Processing Binary Log Files 工具 mysqlbinlog 官方文档 [玩转MySQL之八]MySQL日志分类及简介 【MySQL （六） | 详细分析MySQL事务日志redo log】 【MySQL （六） | 详细分析MySQL事务日志redo log】 MySQL实战45讲_MySQL_数据库-极客时间 极客时间-MySQL实战45讲 MySQL中的重做日志（redo log），回滚日志（undo log），以及二进制日志（binlog）的简单总结 - MSSQL123 - 博客园","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}]},{"title":"MySQL-结构索引与SQL优化","date":"2021-04-08T17:06:14.000Z","path":"2021/04/09/MySQL-结构索引与SQL优化/","text":"MySQL 结构优化结构设计 过分的反范式化为表建立太多的列 过分的范式化造成太多的表关联 在 OLTP 环境中使用不恰当的分区表 使用外键保证数据的完整性 性能优化顺序 数据库结构设计和SQL语句 数据库存储引擎的选择和参数配置 系统选择及优化 硬件升级 知道常用的数据类型，数据库的范式和反范式，各种字段类型占据的大小，更改字段类型是否锁表，如何处理分布式系统下字段的更改。 1、数据库优化的目的 减少数据冗余，节约数据存储空间 提高查询效率 避免数据维护中的异常： 插入异常： 必须有什么才能有什么 更新异常： 如果更改表中的某个实体的单独属性时，需要对多行进行更新 删除异常： 如果删除表中的某一实体则会导致其他实体的消失 2、数据库结构设计步骤 (1) 需求分析： (2) 逻辑设计: 确定数据实体之间的逻辑关系，逻辑存储结构 (3) 物理设计： 跟据所使用的数据库特点进行表结构设计 (4) 维护优化： 维护好索引 范式与反范式1、数据库设计范式 (1) 1NF (2) 2NF： 要求一个表中只具有一个业务主键，符合第二范式的表中不存在非主键列，只对部分主键的依赖关系 复合主键 进行表的拆分实现 (3) 3NF： 指每一个非主属性既不部分依赖于也不传递依赖于业务主键，也就是在第二范式的基础上消除了非主属性对主键的传递依赖 范式化优点： 可以尽量的减少数据冗余； 范式化的更新操作比反范式化更快； 范式化的表通常比反范式化更小 范式化缺点： 对于查询需要对多个表进行关联；更难进行索引优化 2、反范式化设计 是为了性能和读取效率的考虑而适当的对数据库设计范式的要求进行违反，而允许存在少量的数据冗余，反范式化就是使用空间来换取时间 反范式化优点： 可以减少表的关联，可以更好的进行索引优化 反范式化缺点： 存在数据冗余及数据维护异常，对数据的修改需要更多的成本 字段选择当一个列可以选择多种数据类型时， 考虑顺序： 数字类型 ⇒ 日期 / 二进制类型 ⇒ 字符类型 对于相同级别的数据类型，应该优先选择占用空间小的数据类型 == 加载的页(16k) 整数类型 共五种类型 int(2) 不影响占用的字节，使用 tinyint 进行存储 @Q: int(x) x 是什么? @A: 11 代表的并不是长度，而是字符的显示宽度,navicat 进行格式化显示了 实数类型 共三种类型 FLOAT 4B DOUBLE 8B DECIMAL: 每 4 个字节存 9 个数字，小数点占一个字节 @Q: DECIMAL（18，9）占用几个字节 @A: 需要 9 个字节来存储, 表示 9 位整数，9 位小数, 小数点占一个字节， 2*4+1=9 字符串类型 常用的两种，还支持 blog, text 类型 1、varchar 类型 varchar 特点 varchar 用于存储变长字符串，只占用必要的存储空间列的 最大长度小于 255 则只占用一个额外字节用于记录字符串长度 列的最大长度大于 255 则要占用两个额外字节用于记录字符串长度 更改 varchar 小于 255 不会锁表，大于 255 会锁表 使用时需要确定 varchar 的长度 varchar 使用场景： 字符串列的最大长度比平均长度大很多 字符串列很少被更新 使用了多字节字符集存储字符串 2、char 类型 char 类型的特点： CHAR 类型是定长的 字符串存储在 CHAR 类型的列中会删除末尾的空格 CHAR 类型的 最大宽度为255 char 的适用场景： CHAR 类型适合存储所长度近似的值，如 MD5 加密的密码 CHAR 类型适合存储短字符串，限制 255 CHAR 类型适合存储经常更新的字符串列，不会锁表，内存占用固定 日期类型 Untitled 共支持四种类型,分别占8B,4B,3B,xB (1) DATATIME 类型 以格式存储日期时间 DATATIME 类型与时区无关，占用8个字节的存储空间时间范围 12345YYYY-MM-DD HH:MM:SS[.fraction]datetime=YYYY-MM-DD HH:MM:SSdatetime(6)=YYYY-MM-DD HH:MM:SS.fraction-- 支持的范围1000-1-1 0:0:0~9999-12-31 23:59:59 (2) TIMESTAMP 类型 由格林尼治时间 1970 年 1 月 1 日到当前时间的秒数以]的格式显示 占用 4个字节 默认使用 第一个 timestamp 进行自动更新 1YYYY-MM-DD HH:MM:SS.[L.fraction 比较 timestamp 类型显示依赖于所指定的时区 在行的数据修改时可以自动修改 timestamp 列的值 时区比较 存储微秒值 123set time_zone=&#x27;+10:00&#x27;;# timestamp 与时区相关ALERT TABLE t MODIFY d1 DATETIME(6), MODIFY d2 TIMESTAMP(6); (3) Date 类型 为实现 Date，原来使用 int, datetime 存储 占用的字节数比使用字符串、datetime、int 存储要少，使用 date 类型只需要3个字节 Date 类型还可以利用日期时间函数进行日期之间的计算 (4) time 类型 time 类型用于存储时间数据 日期类型使用注意事项 不要使用字符串类型来存储日期时间数据日期时间类型 通常比字符串占用的存储空间小日期时间类型在进行查找过滤时可以利用日期来进行对比 日期时间类型还有着丰富的处理函数，可以方便的对时期类型进行日期计算 使用 Int 存储日期时间不如使用 Timestamp 类型 使用 int 时间戳保存 12select UNIX_TIMESTAMP(&#x27;2020-01-11 09:53:32&#x27;); select FROM_UNIXTIME(1578707612); Recommand使用 TIMESTAMP(4 个字节) 或 DATETIME 类型 (8 个字节) 存储时间 TIMESTAMP 存储的时间范围 1970-01-01 00:00:01 ~ 2038-01-19-03:14:07 TIMESTAMP 占用 4 字节和 INT 相同，但比 INT 可读性高 超出 TIMESTAMP 取值范围的使用 DATETIME 类型存储 尽量做到冷热数据分离,减小表的宽度 MySQL 限制每个表最多存储 4096 列，并且每一行数据的大小不能超过 65535 字节。 减少磁盘 IO, 保证热数据的内存缓存命中率（表越宽，把表装载进内存缓冲池时所占用的内存也就越大,也会消耗更多的 IO）； 更有效的利用缓存，避免读入无用的冷数据； 经常一起使用的列放到一个表中（避免更多的关联操作）。 尽量控制单表数据量的大小, 建议控制在 500 万以内。 500 万并不是 MySQL 数据库的限制，过大会造成修改表结构，备份，恢复都会有很大的问题。 可以用历史数据归档（应用于日志数据），分库分表（应用于业务数据）等手段来控制数据量大小。 临时库表必须以 tmp_ 为前缀并以日期为后缀，备份表必须以 bak_ 为前缀并以日期 (时间戳) 为后缀。 日期相关 timestamp 与时区相关，占用 4个字节 datetime, 1000-01-01 00:00:00 ~ 9999-12-31 23:59:59 12345678910111213# 查看当前会话时区SELECT @@session.time_zone;# 设置当前会话时区SET time_zone = &#x27;Europe/Helsinki&#x27;;SET time_zone = &quot;+00:00&quot;;# 数据库全局时区设置SELECT @@global.time_zone;# 设置全局时区SET GLOBAL time_zone = &#x27;+8:00&#x27;;SET GLOBAL time_zone = &#x27;Europe/Helsinki&#x27;; 索引优化索引是在存储引擎层实现的, 部分存储引擎不支持索引 B+ 树索引B+树索引： 1、特点： B-tree 索引能够加快数据的查询速度 B-tree 索引更适合进行范围查找，叶子节点指向的是主键 2、适合使用 B+ 树的索引 全值匹配 最左前缀 匹配列前缀 范围匹配 精确匹配 只访问索引，覆盖索引 3、B+ 树使用限制 查询优化器判定。。。 非最左列开始查找，。。。 不能跳过左边的 Not in 和 &lt;&gt; 无法使用 如果查询中有某个列的范围查询，则其右边所有列都无法使用索引 Hash 索引Memory 支持，且为默认的 InnoDB 存储引擎支持自适应 Hash 索引 1、特点 精确匹配使用，只用在等值查询 所有列，每一行计算一个Hash码 2、限制 需要两次查找，先找到行，之后读取值 无法进行排序 只支持等值查找，不支持索引查找、范围查找 Hash 冲突可能导致性能问题，不适合选择性差的列 索引概述1、使用索引的好处： 减少存储引擎需要扫描的数量，16K 每页，内存中存放更多的索引 帮助排序，避免使用临时表，索引存储结构为 B 树，按序存储 将随机 I/O 转化成顺序的 I/O，扫描的行变少 2、索引带来的损耗： 写操作成本，需要对应的索引控制 多个索引会增加查询优化器的选择时间 3、建立索引的策略： 安装演示数据库： 123&lt;http://downloads.mysql.com/docs/sakila-db.tar.gztar-zxf&gt; sakila-db.tar.gzmysql -uroot -p &lt; sakila-schema.sqlmysql-uroot -p &lt; sakila-data.sql 索引优化策略1、索引列上不能使用函数或表达式 2、索引长度有限，针对字符串 3、通过索引的选择性确定前缀索引的长度(字符串) 4、选取索引列的顺序： 经常使用的放到列的左边(选择性差的例外) 选择性高的优先 宽度小的列优先，I/O 小 5、覆盖索引直接获取： 优点： 可以优化缓存，减少磁盘I/O 减少随机I/O，将磁盘I/O改为内存的顺序 I/O 可避免对 Innodb 主键索引的二次查询， 可以避免 MyISAM 表进行系统调用 缺点： memory 不支持覆盖索引 查询中太多的列。。。 使用双 % 号的 LIKE 查询 123456789101112-- using index，覆盖索引使用expalin select id from &lt;mytab&gt; where id=1\\\\G-- using where, 非覆盖所以不explain select * from &lt;mytab&gt; where id=1\\\\G-- idx_name name 建立索引查询， using where,using index, 自动增加上主键索引的信息-- 高版本优化 using indexexplain select id,name from &lt;mytab&gt; where name=&#x27;joe&#x27;-- 1. 表达式剔除: out_date 为索引列, 查找近一个月内添加的商品select ... from productwhere to_days(out_date)-to_days(current_date)&lt;=30-- 优化后的select ... from productwhere out_date &lt;= date_add(current_date，interval 30 day)-- 2. 前缀索引长度确定 索引优化优化排序优化排序： 通过排序操作 按照索引顺序扫描数据 优化的限制： 索引的列顺序和 Order By 子句的顺序完全一致 索引中所有列的方向（升序，降序）和 Order by 子句完全一致 Order by 中的字段全部在关联表中的第一张表中 仿 Hash 索引优化模拟 Hash 索引优化 (1) 使用流程 新增 title_hash 列 设置值 创建索引 使用限制 只能处理键值的全值匹配查找 所使用的Hash函数决定着索引键的大小 123456789alter table &lt;mytab&gt; add title_hash varchar(32)update &lt;mytab&gt; set title_name=md5(title);create index idx_title_hash on &lt;mhytab&gt;(title_hash);-- using condition index,...explain select *from &lt;mytab&gt;where title_hash=md5(&#x27;sss&#x27;) and title=&#x27;sdfsfds&#x27;\\\\G 优化锁索引优化锁 避免对表的全部锁定 123456-- session1begin;select * from actor where name=&#x27;ddd&#x27; for update;-- session 2, 未使用锁住，使用索引不会锁住begin;select * from actor where name=&#x27;eee&#x27;; 索引自身问题4、删除重复和冗余的索引 单列的索引类型不同的重复 联合索引的最左匹配原则，形成冗余 12-- todo 工具下载pt-duplicate-key-checker h=127.9.0.1 5、查找未使用的索引 定期清理 通过 performance_schema,information 数据库查询出信息 123456789-- 查看所有数据中对应的表，对应的索引名称，索引使用的次数SELECT CONCAT(object_schema, &quot;.&quot;, object_name) AS TABLE_NAME, index_name AS INDEX_NAME, b.`TABLE_ROWS` AS TABLE_ROWSFROM performance_schema.table_io_waits_summary_by_index_usage a JOIN information_schema.tables b ON a.`OBJECT_SCHEMA`=b.`TABLE_SCHEMA` AND a.`OBJECT_NAME`=b.`TABLE_NAME`WHERE index_name IS NOT NULL AND count_star = 0ORDER BY object_schema, object_name; 6、更新索引统计信息及减少索引碎片 123analyze table &lt;mytab&gt;-- 可能锁表optimize table &lt;mytab&gt; SQL 查询优化SQL 性能优化的目标：至少要达到 range 级别，要求是 ref 级别，如果可以是 consts 最好。 获取慢查询的方式： 最终用户、测试人员获取存在性能问题的 SQL, 被动，常用 慢查询日志获取，服务层的日志 实时获取存在性能问题的 SQL SQL 语句执行流程查询语句的执行流程 优化器的作用：选择索引是优化器的⼯作。⽽优化器选择索引的⽬的，是找到⼀个最优的执⾏⽅案，并⽤最⼩的代价去执⾏语句。在数据库⾥⾯，扫描⾏数是影响执⾏代价的因素之⼀。扫描的⾏数 越少，意味着 访问磁盘数据的次数 越少，消耗的CPU资源越少。 当然，扫描⾏数并不是唯⼀的判断标准，优化器还会结合 是否使⽤临时表、是否排序 等因素进⾏综合判断。 1权限校验---&gt;查询缓存---&gt;分析器---&gt;优化器---&gt;权限校验---&gt;执行器---&gt;引擎 Q: MySQL是怎样得到索引的基数的呢？ MySQL采样统计的⽅法，采样统计的时候，InnoDB 默认会选择 N 个数据⻚，统计这些⻚⾯上的不同值，得到⼀个平均值，然后乘以这个索引的⻚⾯数，就得到了这个索引的基数。 ⽽数据表是会持续更新的，索引统计信息也不会固定不变。所以，当变更的数据⾏数超过 1/M 的时候，会⾃动触发重新做⼀次索引统计。 执行更新语句的流程 MySQL可以恢复到半个⽉内任意⼀秒的状态； 涉及到两个日志文件 redo log 和 binlog； 两阶段提交，以及 Insert Buffer 插入缓冲 12UPDATE T SET c=c+1 WHERE ID=2;分析器----&gt;权限校验----&gt;执行器---&gt;引擎---redo log prepare---&gt;binlog---&gt;redo log commit SQL 解析及执行计划 1、SQL 执行过程 (1) 客户端发送 SQL 请求给服务器 (2) 服务器检查是否可以在查询缓存中命中该 SQL (3) 服务器端进行 SQL 解析，预处理，再由优化器生成对应的执行计划 (4) 跟据执行计划，调用存储引擎 API 来查询数据将结果返回给客户端 查询缓存 查询缓存： 缓存加锁 对于一个读写频繁的系统使用查询缓存很可能会降低查询处理的效率 所以在这种情况下建议大家不要使用查询缓存 查询缓存的配置参数 123456query_cache_type# 所使用的的大小query_# 缓存结果的最大值# 锁住，是否返回缓存中的数据# 查询缓存内存块的最小 查询优化器 1、可能导致失败的情况： (1) 统计信息不准确，如 Innodb 中的总条数为抽样的数据 (2) 执行计划中的成本估算不等同于实际的执行计划的成本，无法知道顺序读取与随机读取，是否在内存中 (3) MySQL 基于其成本模型进行。。。 (4) 从不考虑其他并发的查询 (5) 不会考虑不受控制的成本，如存储过程、用户自定义的函数 2、查询优化器的作用 (1) 优化 count(), min() 和 max(), B+树的。。。 (2) 将表达式转换成一个常数，MyISAM中的 Selelct Count(*) 子查询优化： 转化成关联查询 提前终止查询： 对 in() 条件进行优化，对 in 中的数据进行排序，之后通过二分查找方式确定是否满足条件 ### 查询耗时 1、通过 profile 高版本已经不建议使用，建议使用方式二 开启与执行 123456789-- 开启set profiling=1;select count(*) from &lt;mytab&gt;-- 查看整体信息show profiles;-- 查看执行的每个阶段信息show profile for query &lt;query_id&gt;;-- 查看 CPU 的信息show profile cpu for query &lt;query_id&gt;; 结果数据 2、通过 performance_schema(建议) 在 MySQL5.6 之后建议使用 对数据全局有效 (1) 开启与使用 12345678910111213141516171819-- 1. 处理的先决条件，开启 performance_schemaUPDATE `setup_instruments`SET enabled=&#x27;YES&#x27;,TIMED=&#x27;YES&#x27;WHERE NAME LIKE &#x27;stage%&#x27;;UPDATE setup_consumersSET enabled=&#x27;YES&#x27;WHERE NAME LIKE &#x27;events%&#x27;;-- 2. 查询使用SELECT a.THREAD_ID, SQL_TEXT,c.EVENT_NAME,(c.TIMER_END-c.TIMER_START)/1000000000 AS &#x27;DURATION(ms)&#x27;FROM events_statements_history_long a JOIN threads b ON a.THREAD_ID=b.THREAD_IDJOIN events_stages_history_long c ON c.THREAD_ID=b.THREAD_ID AND c.EVENT_ID BETWEEN a.EVENT_ID AND a.END_EVENT_ID WHERE b.PROCESSLIST_ID=CONNECTION_ID() AND a.EVENT_NAME=&#x27;statement/sql/select&#x27;ORDER BY a.THREAD_ID,c.EVENT_ID 信息返回 返回各个阶段的耗时 执行的线程ID 慢查询日志写日志为顺序存储 慢查询配置 动态参数,可配合脚本定时开关 slow_query_log 存放位置。将日志存储与数据存储分开 slow_query_log_file 查询阈值，秒为单位，建议 0.001 秒 long_query_time 记录未使用索引的 SQL log_queries_not_using_indexes 实际使用的SQL 1234SHOW VARIABLES LIKE &#x27;%quer%&#x27;SET GLOBAL slow_query_log=on;SET GLOBAL slow_query_log_file=&#x27;/var/log/mysql/mysql-slow.log&#x27;SET GLOBAL long_query_time=0.001;SET GLOBAL log_queries_not_using_indexes=ON; 2、慢查询记录的内容 用户 查询的时间，精确到毫秒 查询占用的锁时间 返回的数据行数 扫描的行数 执行SQL的时间， UNIX时间戳 对应的SQL语句 12345# Time: 2019-05-30T10:10:02.771744Z# User@Host: root[root] @ localhost [127.0.0.1] Id: 44# Query_time: 0.001503 Lock_time: 0.001002 Rows_sent: 12 Rows_examined: 12SET timestamp&#x3D;1559211002;SELECT * FROM exam; mysqldumpslow 获取 通过自带的工具进行筛选输出 (1) -s order（c，t，l，r，at，al，ar）: 指定按哪种排序方式输出结果 c：总次数 t：总时间 1：锁的时间 r：总数据行 at，al，ar:t，l，r平均数 t top: 指定取前几条作为结束输出 1mysqldumpslow -s &lt;r&gt; -t &lt;10&gt; &lt;slow-mysql.log&gt; pt-query-digest 获取 建议使用 还支持对 bin log等日志的进行查看 1pt-query-digest --explain h=127.0.0.1,u=root,p=password mysql-slow.log 返回结果： 整体的统计信息，total,max,min 执行计划 实时获取 通过 information_schema.proceeslist 表进行查看获取实时数据 12345678910-- 查询出服务器中执行超过 60s 的SQLSELECT id, user, DB, command, time, state, infoFROM information_schema.PROCESSLISTWHERE TIME &gt;= 60; 特定 SQL 的优化大表的数据修改分批处理： 1000 万行记录的表中删除/更新 100 万行记录 一次只删除/更新 5000 行记录，暂停几秒 方式一： 通过存储过程实现 1DELIMITER $$USE `&lt;myDB&gt;`$$DROP PROCEDURE IF EXISTS `p_delete rows`$$CREATE DEFINER=`root`@`127.0.0.1` PROCEDURE `p_delete_rows`()BEGIN DECLARE v_rows INT； SET v_rows=1; WHILE v_roWs &gt;0 DO -- 根据业务修改下面的语句 DELETE FROM sbtest1 WHERE id&gt;=90000 AND id &lt;=19000 LIMIT 5000; SELECT ROW_COUNT() INTO v_rows; -- sleep SELECT SLEEP(5); END WHILE;END$$DELIMITER； 方式二： 程序中执行类似存储过程的逻辑每次获取少量数据，之后执行处理逻辑 修改大表的表结构 对表的列的字段类型进行修改，改变字段的宽度会锁表，无法解决主从数据库延迟问题 方式一： 主从切换 方式二： 手动创建新表进行迁移，减少主从延迟，操作复杂 可使用 pt-online-schema-change 工具 完成方式二的复杂逻辑 123456pt-online-schema-change --alter=&quot;MODIFY C VARCHAR(150) NOT NULL DEFAULT &#x27;&#x27;&quot; --user=root \\\\ --password=PassWord D=&lt;mydb&gt;,t=&lt;mytab&gt; --charset=utf8 \\\\ --execute 优化 not in 和 &lt;&gt; 查询(#) 通过外连接 + NULL 进行处理，即为将子查询转化成表的连接查询； 常用的调整手段； 12345678910-- 原始SQLSELECT customer_id, first_name, last_name, emailFROM customerWHERE customer_id NOT IN (SELECT customer_id FROM payment)-- 通过 Left join 配合 null 的优化SELECT a.customer_id,a.first_name,a.last_name,a.emailFROM customer aLEFT JOIN payment b ON a.customer_id =b.customer_idWHERE b.customer_id IS NULL 使用汇总表优化查询(#) 汇总表概述： 汇总表就是提前以要统计的数据进行汇总并记录到表中以备后续的查询使用。 汇总表的数据可以使非实时的。 实际使用汇总表的策略 使用汇总表记录从该天开始之前的所有统计信息，之后每天进行更新维护； 查询时借助统计信息表的数据和当天的数据汇总返回； 汇总表被当做一种缓存，当天数据类似增量信息； 从原来的表中提取出汇总信息，不会变更原来的表结构，侵入性低，是一种常用的扩展手段。 12345678910111213-- 原始的统计SQLSELECT COUNT(*) FROM product_ comment WHERE productid=999-- 创建汇总表CREATE TABLE product_ comment_cnt(product id INT,cnt INT);-- 修改后的 SQL, UNION ALL 确定SELECT SUM(cnt) FROM(SELECT cnt FROM product_comment_cnt WHERE product_id=999UNION ALL SELECT COUNT(*) FROM product_comment WHERE product_id=999AND timestr&gt;DATE(NOW())) a 只要一行数据时使用 LIMIT 1 已经知道结果只会有一条结果，加上 LIMIT 1 可以增加性能。MySQL 数据库引擎会在找到一条数据后停止搜索，而不是继续往后查少下一条符合记录的数据。 用 Not Exists 代替 Not In Not Exists 允许用户使用相关子查询已排除一个表中能够与另一个表成功连接的所有记录。Not Exists 用到了连接，能够发挥已经建好的索引的作用，而 Not In 不能使用索引。Not In 是最慢的方式，要同每条记录比较，在数据量比较大的查询中不建议使用这种方式。 12345Select a.mobileidfrom Log_user awhere not exists (select b.mobileid from magazineitem b where b.mobileid=a.mobileid); RefMySQL实战45讲_MySQL_数据库-极客时间 MySQL实战45讲","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}]},{"title":"MySql-索引","date":"2021-04-08T17:06:02.000Z","path":"2021/04/09/MySql-索引/","text":"概述删除长期未使用的索引 不用的索引的存在会造成不必要的性能损耗 MySQL 5.7 可以通过查询 sys 库的 chema_unused_indexes 视图来查询哪些索引从未被使用 避免定义重复的索引 借助 pt-duplicate-key-checker 查找，并生成删除重复索引的 SQL 语句 1pt-duplicate-key-checker -h &lt;host&gt; -u&lt;user&gt; -p&lt;passwd&gt; 索引好处： 提高访问速度； 优化查询； 将随机 IO 改为顺序 IO 索引缺点： 占用空间，索引建立的越多越占用空间； 更改频繁时，每次修改都需要重建索引； 索引的适用场景 选择性较高的场景； 不适合索引的场景： 数据规模小的情况； 选择性较低的列； 索引的底层实现 Hash 索引 适⽤于只有等值查询的场景，⽐如 Memcached 及其他⼀些 NoSQL 引擎。 (2)) 有序数组 有序数组在等值查询和范围查询场景中的性能就都⾮常优秀, 有序数组索引只适⽤于静态存储引擎。 二叉树 B+ 树 索引的创建 index_type： 可选择 BTREE，HASH 12345CREATE [UNIQUE | FULLTEXT | SPATIAL] INDEX index_name [index_type] ON tbl_name (key_part,...) [index_option] [algorithm_option | lock_option] ... Functional Key Parts: MySQL8.0+ 支持，不只是列/列特定的前缀，支持 123456789CREATE TABLE t1 ( col1 VARCHAR(10), col2 VARCHAR(20), INDEX (col1, col2(10)));CREATE TABLE t1 (col1 INT, col2 INT, INDEX func_index ((ABS(col1))));CREATE INDEX idx1 ON t1 ((col1 + col2));CREATE INDEX idx2 ON t1 ((col1 + col2), (col1 - col2), col1);ALTER TABLE t1 ADD INDEX ((col1 * 40) DESC); JSON 格式的索引 12345678910CREATE TABLE employees ( data JSON, INDEX idx ((CAST(data-&gt;&gt;&quot;$.name&quot; AS CHAR(30)) COLLATE utf8mb4_bin)));INSERT INTO employees VALUES (&#x27;&#123; &quot;name&quot;: &quot;james&quot;, &quot;salary&quot;: 9000 &#125;&#x27;), (&#x27;&#123; &quot;name&quot;: &quot;James&quot;, &quot;salary&quot;: 10000 &#125;&#x27;), (&#x27;&#123; &quot;name&quot;: &quot;Mary&quot;, &quot;salary&quot;: 12000 &#125;&#x27;), (&#x27;&#123; &quot;name&quot;: &quot;Peter&quot;, &quot;salary&quot;: 8000 &#125;&#x27;);SELECT * FROM employees WHERE data-&gt;&gt;&#x27;$.name&#x27; = &#x27;James&#x27;; 索引查看12345678910111213-- 查看指定表的索引SHOW INDEX FROM &lt;table-name&gt; FROM &lt;database-name&gt;;-- 查看指定数据库的索引统计SELECT TABLE_NAME ,INDEX_NAME ,SEQ_IN_INDEX ,COLUMN_NAME ,CARDINALITY ,INDEX_TYPE FROM INFORMATION_SCHEMA.STATISTICS WHERE table_schema = &#x27;DatabaseName&#x27; 重复索引的查看，见 pt-duplicate-key-checker 索引分类索引分类 B-Tree: b 树索引 R-tree: 空间索引 hash: 散列索引 full-text: 全文索引 主键索引(聚集)作为表的主键，可以有包含多个列，该索引是唯一索引，在 InnoDB 存储引擎中为聚集索引。 主键⻓度越⼩，普通索引的叶⼦节点就越⼩，普通索引占⽤的空间也就越⼩。 所以，从性能和存储空间⽅⾯考量，⾃增主键往往是更合理的选择。 聚集索引和非聚集索引的区别 聚集索引在叶子节点存储的是表中的数据 非聚集索引在叶子节点存储的是主键和索引列 Q：基于主键索引和普通索引的查询有什么区别？ A：如果语句是 select * from table where id=8888，只需要搜索 ID 这棵 B+ 树； 如果语句是 select * from table where key=5，需要先搜索 key 索引树，得到 id 的值为 500，再到 ID 索引树搜索⼀次。这个过程称为 回表。 也就是说，基于⾮主键索引的查询需要多扫描⼀棵索引树。 普通索引相较于唯一索引，使用了 change buffer 与唯一索引的相比，这两类索引在查询能⼒上是没差别，主要考虑的是对更新性能的影响。 Change buffer 使用场景： 如果所有的更新后⾯，都⻢上伴随着对这个记录的查询，那么应该关闭 change buffer。⽽在其他情况下，change buffer 都能提升更新性能。 唯一索引唯⼀索引⽤不上 change buffer 的优化机制，因此如果业务可以接受，从性能⻆度出发建议优先考虑⾮唯⼀索引。 对于唯⼀索引来说，由于索引定义了唯⼀性，查找到第⼀个满⾜条件的记录后，就会停⽌继续检索。 覆盖索引包含了所有查询字段 (where,select,ordery by,group by 包含的字段) 的索引。 对于频繁的查询优先考虑使用覆盖索引。 无需二次扫表，直接扫描索引中的 B+ 树即可获取全部数据； 覆盖索引的好处： 避免 Innodb 表进行索引的二次查询: Innodb 是以聚集索引的顺序来存储的，对于 Innodb 来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询，减少了 IO 操作，提升了查询效率。 可以把随机 IO 变成顺序 IO 加快查询效率: 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。 创建的注意事项： 选取字符串的前多少位作为索引项()； 阿里巴巴开发手册规定 【强制】在 varchar 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度即可。 注意 最左前缀 的使用，以及基于此衍生出来的联合索引组合； 能够形成覆盖索引，从而避免二次扫表问题。 联合索引联合索引定义的顺序： 建立索引的目的是：希望通过索引进行数据查找，减少随机 IO，增加查询性能，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。 区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数） 尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO 性能也就越好） 使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引） 索引列的选取： 常见索引列建议 出现在 SELECT、UPDATE、DELETE 语句的 WHERE 从句中的列 包含在 ORDER BY、GROUP BY、DISTINCT 中的字段 并不要将符合 1 和 2 中的字段的列都建立一个索引， 通常将 1、2 中的字段建立联合索引效果更好 多表 join 的关联列 字符串索引直接创建完整索引，这样可能⽐较占⽤空间； 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使⽤覆盖索引 倒序存储，再创建前缀索引，⽤于绕过字符串本身前缀的区分度不够的问题； 创建 引，查询性能稳定，有额外的存储和计算消耗，跟第三种⽅式⼀样，都不⽀持 。hash字段索范围扫描 前缀索引前缀索引限定索引的长度 不指定前缀长度，默认包含整个字符串； 1234ALTER TABLE SUser ADD INDEX idx1(email);-- only prefixALTER TABLE SUser ADD INDEX idx2(email(6)); 由于 email(6) 这个索引结构中每个邮箱字段都只取前 6 个字节，所以占⽤的空间会更⼩，这就是使⽤前缀索引的优势。 但可能会增加额外的记录扫描次数。 如果使⽤的是 index1(即 email 整个字符串的索引结构)，执⾏顺序是这样的： 从 index1 索引树找到满⾜索引值是 zhangssxyz@xxx.com ’的这条记录，取得ID2的值； 到主键上查到主键值是ID2的⾏，判断email的值是正确的，将这⾏记录加⼊结果集； 取index1索引树上刚刚查到的位置的下⼀条记录，发现已经不满⾜email=’&#122;&#x68;&#x61;&#x6e;&#103;&#115;&#x73;&#120;&#121;&#x7a;&#x40;&#x78;&#x78;&#120;&#46;&#x63;&#x6f;&#x6d;’的条件了，循环结束。 这个过程中，只需要回主键索引取⼀次数据，所以系统认为只扫描了⼀⾏。 如果使⽤的是index2(即email(6)索引结构)，执⾏顺序是这样的： 1 . 从index2索引树找到满⾜索引值是’zhangs’的记录，找到的第⼀个是ID1； 2 . 到主键上查到主键值是ID1的⾏，判断出email的值不是’&#x7a;&#x68;&#97;&#110;&#x67;&#x73;&#x73;&#x78;&#x79;&#x7a;&#64;&#120;&#120;&#x78;&#46;&#99;&#111;&#x6d;’，这⾏记录丢弃； 3 . 取index2上刚刚查到的位置的下⼀条记录，发现仍然是’zhangs’，取出ID2，再到ID索引上取整⾏然后判断，这次值对了，将这⾏记录加⼊结果集； 4 . 重复上⼀步，直到在idxe2上取到的值不是’zhangs’时，循环结束。 在这个过程中，要回主键索引取4次数据，也就是扫描了4⾏。 通过这个对⽐，你很容易就可以发现，使⽤前缀索引后，可能会导致查询语句读数据的次数变多。 使⽤前缀索引，定义好⻓度，就可以做到既节省空间，⼜不⽤额外增加太多的查询成本。 根据区分度来选择前缀的长度； 建立索引列的区分度越⾼越好。区分度越⾼，意味着重复的键值越少。可以通过统计索引上有多少个不同的值来判断要使⽤多⻓的前缀。 可以使⽤下⾯这个语句，算出这个列上有多少个不同的值，95% 作为参考； 12345SELECT COUNT(DISTINCT LEFT(email, 4)) AS L4, COUNT(DISTINCT LEFT(email, 5)) AS L5, COUNT(DISTINCT LEFT(email, 6)) AS L6, COUNT(DISTINCT LEFT(email, 7)) AS L7,FROM SUser; 前缀索引对覆盖索引的影响 如果使⽤ index1(即email整个字符串的索引结构)的话，可以利⽤覆盖索引，从index1查到结果后直接就返回了，不需要回到ID索引再去查⼀次。⽽如果使⽤index2(即email(6)索引结构)的话，就不得不回到ID索引再去判断email字段的值。 即使你将index2的定义修改为email(18)的前缀索引，这时候虽然index2已经包含了所有的信息，但InnoDB还是要回到id索引再查⼀下，因为系统并不确定前缀索引的定义是否截断了完整信息。 也就是说，使⽤前缀索引就⽤不上覆盖索引对查询性能的优化了，这也是你在选择是否使⽤前缀索引时需要考虑的⼀个因素。 12ALTER TABLE SUser ADD INDEX idx_email(eamil(20));--SELECT id, name, email FROM SUserWHERE email=&#x27;zhangssxyz@xxx.com&#x27;; 区分度不足的处理前缀的区分度不够时如何处理 比如国家的身份证号、电话号码 方式一： 倒序存储 由于身份证号的最后6位没有地址码这样的重复逻辑，所以最后这 6 位很可能就提供了⾜够的区分度，先使⽤ count(distinct) ⽅法做验证 12345-- MySQL 函数 revere 操作，插入处理INSERT INTO T(col.., id_card) VALUES (xxx, revere(&#x27;input_id_card_string&#x27;));-- 查询的处理SELECT &lt;field_list&gt; FROM t WHERE id_card=reverse(&#x27;input_id_card_string&#x27;); 方式二： 使用 Hash 字段 在表上再创建⼀个整数字段，来保存身份证的校验码，同时在这个字段上创建索引。 之后每次插⼊新记录的时候，都同时⽤ crc32() 函数得到校验码填到这个新字段。由于校验码可能存在冲突，也就是说两个不同的身份证号通过 crc32() 函数得到的结果可能是相同的，所以你的查询语句 where 部分要判断 id_card 的值是否精确相同。 这样，索引的⻓度变成了 4 个字节，⽐原来⼩了很多。 12345678-- 新增加一列，4byte 存放索引 hash 后的值ALTER TABLE t ADD id_card_crc INT UNSIGNED, ADD index(id_card_crc);-- 插入逻辑， crc32 hash 值以及原始的值INSERT INTO T(co1..., id_card_crc, id_card) VALUES (xxx, crc32(&#x27;input_card_string&#x27;), &#x27;input_id_card_string&#x27;); -- INDEX can duplication-- 对应的查询语句，先对 hash 进行查询后对原始字符串查询SELECT * FROM t WHERE id_card_crc=crc32(&#x27;input_id_card_string&#x27;) AND id_card=&#x27;input_id_card_string&#x27;); 倒序存储和 hash 存储两种方式的比较： 相同点是，都不⽀持范围查询。倒序存储的字段上创建的索引是按照倒序字符串的⽅式排序的，已经没有办法利⽤索引⽅式查出身份证号码在[ID_X, ID_Y]的所有市⺠了。同样地，hash 字段的⽅式也只能⽀持等值查询。 它们的区别，主要体现在以下三个⽅⾯： ① 从占⽤的额外空间来看，倒序存储⽅式在主键索引上，不会消耗额外的存储空间，⽽ hash 字段⽅法需要增加⼀个字段。当然，倒序存储⽅式使⽤ 4 个字节的前缀⻓度应该是不够的，如果再⻓⼀点，这个消耗跟额外这个 hash 字段也差不多抵消了。 ② 在CPU消耗⽅⾯，倒序⽅式每次写和读的时候，都需要额外调⽤⼀次 reverse 函数，⽽ hash 字段的⽅式需要额外调⽤⼀次 crc32() 函数。如果只从这两个函数的计算复杂度来看的话，reverse 函数额外消耗的 CPU 资源会更⼩些。 ③ 从查询效率上看，使⽤ has h字段⽅式的查询性能相对更稳定⼀些。因为 crc32 算出来的值虽然有冲突的概率，但是概率⾮常⼩，可以认为每次查询的平均扫描⾏数接近 1。⽽倒序存储⽅式毕竟还是⽤的前缀索引的⽅式，也就是说还是会增加扫描⾏数。 索引的使用修改 SQL 语句尽量使其走索引； 对于查询优化器选择索引错误，通过修改 SQL 引导其进行选择正确的索引，如通过 FORCE INDEX() 来指定使用特定的索引进行查询； 为查询较多的字段添加索引，可考虑使用覆盖索引； 要避免编写使索引失效的 SQL 语句； 最左匹配原则 全值匹配我最爱，最左前缀要遵守； 带头大哥不能死，中间兄弟不能断； 原因： B+ 树的索引结构导致 原因： 复合索引(组合)，先对最左边的字段进行排序，在第一个字段排序的基础上再对后面的字段排序。 类似 orderby ，只保证第一个字段有序，通常对于第二个字段用不到索引； B+ 树这种索引结构，可以利⽤索引的“最左前缀”，来定位记录 只要满⾜最左前缀，就可以利⽤索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。 如何控制索引的顺序： 如果通过调整顺序，可以少维护⼀个索引，那么这个顺序往往就是需要优先考虑采⽤的。 考虑的原则就是空间了。⽐如上⾯这个市⺠表的情况，name 字段是⽐ age 字段⼤的，那我就建议你创建⼀个(name,age)的联合索引和⼀个(age)的单字段索引 索引失效 索引列上少计算，范围之后全失效； LIKE 百分写最右，覆盖索引不写星； 不等空值还有 OR, 索引失效要少用； VAR 引号不可丢，SQL 高级也不难！ 条件字段函数及计算操作 如果对字段做了函数计算，就⽤不上索引了。 SQL语句条件⽤的是 where t_modified=‘2018-7-1’ 的话，B+ 树提供的这个快速定位能⼒，来源于同⼀层兄弟节点的有序性。 显示调用函数，在 t_modified 字段加了 month() 函数操作，导致了全索引扫描 123456789101112CREATE TABLE `tradelog` ( `id` int(11) NOT NULL, `tradeid` varchar(32) DEFAULT NULL, `operator` int(11) DEFAULT NULL, `t_modified` datetime DEFAULT NULL, PRIMARY KEY (`id`), KEY `tradeid` (`tradeid`), KEY `t_modified` (`t_modified`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;-- Query，对索引列使用了函数SELECT COUNT(*) FROM tradelog WHERE month(t_modified)=7; 按照业务进行修改，使其走索引： 12345-- 更改索引列中使用的函数成为索引列的范围比较查询SELECT COUNT(*) FROM tragelogWHERE (t_modified &gt;= &#x27;2016-7-1&#x27; AND t_modified&lt;&#x27;2016-8-1&#x27;) OR (t_modified &gt;= &#x27;2017-7-1&#x27; AND t_modified&lt;&#x27;2017-8-1&#x27;) OR (t_modified &gt;= &#x27;2018-7-1&#x27; AND t_modified&lt;&#x27;2018-8-1&#x27;); 隐式类型转换 字符串隐式的转换成数字进行操作； 1234567select * from tradelog where tradeid=110717;select “10” &gt; 9; -- 1, String ⇒ numberSELECT * FROM tradelog WHERE CAST(tradid AS signed int)=110717;-- 修改后的 SQLSELECT * FROM tradelog WHERE tradeid=&#x27;110717&#x27;; 隐式字符编码转换 123456789101112CREATE TABLE `trade_detail` ( `id` int(11) NOT NULL, `tradeid` varchar(32) DEFAULT NULL, `trade_step` int(11) DEFAULT NULL COMMENT &quot;操作步骤&quot;, `step_info` varchar(32) DEFAULT NULL COMMENT &quot;步骤信息&quot;, PRIMARY KEY (`id`), KEY `tradeid` (`tradeid`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;-- log 和 业务的详情表字符集不同select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2; /* 语句 Q1 */ 两个字符集不同： 1234select * from trade_detail where tradeid=$L2.tradeid.value;-- 等同于，对列粒度的字符进行编码转换比较select * from trade_detail where CONVERT(traideid USING utf8mb4)=$L2.tradeid.value; 字符集 utf8mb4 是 utf8 的超集，所以当这两个类型的字符串在做⽐较的时候，MySQL 内部的操作是，先把 utf8 字符串转成 utf8mb4 字符集，再做⽐较。 方案一： 把 trade_detail 表上的 tradeid字 段的字符集也改成 utf8mb4，这样就没有字符集转换的问题了。 1ALTER TABLE trade_detail MODIFY tradeid varchar(32) character SET utf8mb4 DEFAULT NULL; 方案二： 在无法修改字符集的情况下，或者表中的数据很多的情况下 12select d.* from tradelog l , trade_detail d where d.tradeid=CONVERT(l.tradeid USING utf8) and l.id=2; 简单的计算 1SELECT * FROM trade_detail WHERE trade_step=trade_step + 1; 索引下堆(5.6)// TODO 索引问题重建索引@Q: 为什么要重建索引? 索引可能因为删除，或者⻚分裂等原因，导致数据⻚有空洞，重建索引的过程会创建⼀个新的索引，把数据按顺序插⼊，这样⻚⾯的利⽤率最⾼，让索引更紧凑、更省空间。 通过两个 alter 语句重建索引 k，以及通过两个 alter 语句重建主键索引是否合理。 重建索引 k 的做法是合理的，可以达到省空间的⽬的。但是，重建主键的过程不合理。不论是删除主键还是创建主键，都会将整个表重建。所以连着执⾏这两个语句的话，第⼀个语句就⽩做了。这两个语句，可以⽤这个语句代替 ： alter table Tengine=InnoDB。 记录⽇志⽤的表, 会定期删除过早之前的数据。 最后这个表实际内容的⼤⼩ 10G, 索引却有 30G.. 是 InnoDB 这种引擎导致的,虽然删除了表的部分记录,但是它的索引还在, 并未释放. 只能是重新建表才能重建索引. 索引选择异常@Q: 索引选择异常和处理？ 处理查询优化器选择错误索引？ 查询优化器选择的不是最优的索引情况的处理。 ⼤多数时候优化器都能找到正确的索引，但偶尔还是会碰到我们上⾯举例的这两种情况：原本可以执⾏得很快的SQL语句，执⾏速度却⽐你预期的慢很多 方式一：采⽤ force index() 强⾏选择⼀个索引 MySQL 会根据词法解析的结果分析出可能可以使⽤的索引作为候选项，然后在候选列表中依次判断每个索引需要扫描多少⾏。如果 force index 指定的索引在候选索引列表中，就直接选择这个索引，不再评估其他索引的执⾏代价。 12345-- FORCE INDEX 的使用SELECT * FROM t FORCE INDEX(a) WHERE a BETWEEN 1 AND 1000 AND b BETWEEN 50000 AND 100000ORDER BY bLIMIT 1; 但其实使⽤ force index 最主要的问题还是变更的及时性。因为选错索引的情况还是⽐较少出现的，所以开发的时候通常不会先写上 force index。⽽是等到线上出现问题的时候，你才会再去修改 SQL 语句、加上 force index。但是修改之后还要测试和发布，对于⽣产系统来说，这个过程不够敏捷。 方法二：可以考虑修改语句，引导 MySQL 使⽤期望的索引 ⽐如，在这个例⼦⾥，显然把“order by b limit 1” 改成 “order by b,a limit 1” ，语义的逻辑是相同的。 现在 order by b,a 这种写法，要求按照b,a排序，就意味着使⽤这两个索引都需要排序。因此，扫描⾏数成了影响决策的主要条件，于是此时优化器选了只需要扫描 1000 ⾏的索引 a。 当然，这种修改并不是通⽤的优化⼿段，只是刚好在这个语句⾥⾯有 limit 1，因此如果有满⾜条件的记录， order by b limit 1 和 order by b,a limit 1 都会返回 b 是最⼩的那⼀⾏，逻辑上⼀致，才可以这么做。 123456789101112SELECT * FROM t FORCE INDEX(a)WHERE a BETWEEN 1 AND 1000 AND b BETWEEN 50000 AND 100000ORDER BY b,a -- modifyLIMIT 1;-- 另一种引导我们⽤limit 100让优化器意识到，使⽤b索引代价是很⾼的。其实是我们根据数据特征诱导了⼀下优化器，也不具备通⽤性。SELECT * FROM (SELECT * FROM t WHERE (a BETWEEN 1 AND 1000) AND (B BETWEEN 50000 AND 100000) ORDER BY b LIMIT 100) alias LIMIT 1; 方法三：在有些场景下，可以新建⼀个更合适的索引，来提供给优化器做选择，或删掉误⽤的索引 这种情况其实⽐较少，尤其是经过 DBA 索引优化过的库，再碰到这个 bug，找到⼀个更合适的索引⼀般⽐较难。 如果我说还有⼀个⽅法是删掉索引 b，你可能会觉得好笑。但实际上我碰到过两次这样的例⼦，最终是 DBA 跟业务开发沟通后，发现这个优化器错误选择的索引其实根本没有必要存在，于是就删掉了这个索引，优化器也就重新选择到了正确的索引。 B 树与索引B+ 树特点： 关键字分布在整棵树的所有节点。 任何一个关键字，出现且只出现在一个节点中。 搜索有可能在非叶子节点结束。 其搜索性能等价于在关键字全集内做一次二分查找。 B+树基本特点 非叶子节点的子树指针与关键字个数相同。 非叶子节点的子树指针 P[i]，指向关键字属于 [k[i],K[i+1]) 的子树（注意：区间是前闭后开)。 为所有叶子节点增加一个链指针。 所有关键字都在叶子节点出现。 B 树与 B+ 树的区别 关键字与孩子节点的个数不同； B+树的磁盘读写代价更低：B+ 树的内部没有指向关键字具体信息的指针，其内部节点相对 B 树更小，把所有关键字存放在同一块盘中，B+ 树比 B 树所能容纳的关键字数量也越多； 查询的稳定性： B+ 树所欲数据都存放在叶子节点，B+ 树无论如何都要扫表到叶子节点才能返回数据， 所有关键字查询的路径长度相同；B 树可以中途跳出，查询效率不够稳定； B+ 树适合用于遍历和范围的选择，底层叶子节点是双向链表，只需要去遍历叶子节点就可以实现整棵树的遍历； MongoDB 的索引为什么选择 B 树，而 MySQL 的索引是 B+树 MongoDB 非传统的 RDBMS，是以 Json 格式作为存储的 NoSQL，目的就是高性能、高可用、易扩展。摆脱了关系模型，所以范围查询和遍历查询的需求就没那么强烈。 RefMySQL :: MySQL 8.0 Reference Manual :: C Indexes MySQL 的索引官方文档 深入理解MySQL索引-InfoQ 深入理解 MySQL 索引","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}]},{"title":"MySQL 事务","date":"2021-04-08T17:04:36.000Z","path":"2021/04/09/MySQL-事务/","text":"概述 事务是数据库区别于文件系统的一个关键特性。 事务的分类 ① 扁平事务，使用最频繁； ② 带有保存点的扁平事务； ③ 链事务，下一个事务将能够看到上一个事务的结果，只能恢复到最近一个的保存点； ④ 嵌套事务； 任何子事务都在顶层事务提交后才真正的提交；是一棵树状的结构； 只有叶子节点的事务才能访问数据库、发送消息、获取其他类型的资源； ⑤ 分布式事务；需要根据数据所在位置访问网络中的不同节点；保存点在事务内部是递增的；可以借助消息队列实现分布式事务。 相关的 SQL // TODO 关联 Spring 提供的几个事务级别 1SHOW VARIABLES LIKE &#x27;ios%&#x27;; 四大特性 原子性(Atomic)： 所有操作要么全部成功，要么全部失败 一致性(Consistency)： 数据从一个一致性状态转移到另一个一致性状态，一致指的是 数据的完整性约束 没有被破坏 **隔离性(Isolation)**： 并发执行事务时，一个事务应该不影响其他事务的执行 持久性(Duration)： 对 DB 的修改永久，恢复性能 事务的实现方式实现的原理： InnoDB 中的 undo.log, redo.log 日志文件。 隔离性： 通过锁实现 原子性和持久性： 通过 redo 物理日志实现； 事务的一致性： 通过 undo log 实现； redo log blog 可通过参数调节控制 redo log 刷新到磁盘的策略； log block： redo log 的块大小与磁盘扇区大小一样都是 512 字节，保证了原子性，不需要 doublewrite 技术； 为物理日志，恢复速度比逻辑日志快，是幂等的。 重做日志记录了事务的行为，可以很好地通过其对也进行 “重做” 操作 undo log 帮助事务回滚； 帮助实现 MVCC； 是实现快照读的一种必要机制； 存放在数据库内部的一个特殊字段上； 功能一： 是逻辑日志，将数据库逻辑地恢复到原来的样子； 功能二： 当用户读取一行记录时，若该记录已经被其他事务占用，当前事务可以通过 undo 读取之前的行版本信息，以此实现非锁定读取。 分类： insert undo log update undo log delete 操作不直接删除记录，而只是将记录标记为已删除。 undo 信息的数据字典： 真正删除这行记录的操作其实被 “延时” 了，最终在 purage 操作中完成。 两阶段提交第一阶段： 所有参与全局事务的节点都开始准备(PREPARE) ，告诉事务管理器准备好了； 第二阶段： 事务管理器告诉资源管理器质性 ROLLBACK 还是 COMMIT，分布式事务需要多一次的 PREPARE 操作，待收到所有节点的统一信息后，再进行 COMMIT 或是 ROLLBACK 操作。 事务相关的 SQL 语句 一条语句失败并抛出异常，不会导致先前已经执行的语句自动会馆， 1234567891011121314-- salfpoint ROLLBACK-- 删除一个保存点RELEASE SAVEPOINT t1;-- 定义一个保存点SAVEPOINT t2;-- 回滚到某个保存点, 此时事务没有结束ROLLBACK TO SAVEPOINT t2;-- 设置级别SET [GLOBAL | SESSION] TRANSACTION ISOLATION LEVEL &#123;...&#125; 分布式事务 XA XA 事务由一个或多个资源管理器、一个事务管理器以及一个应用程序组成。 Serializable 级别 长事务 执行时间较长的事务； 进行的优化：在 1 亿用户表中，这个操作被封装在一个事务中完成，通过为其转化成小批量的事务进行处理； 好处一： 便于回滚每完成一个小事务，将完成的结果存放在 batchcontext 表中，表示已完成批量事务的最大账号 ID。 在发生错误时，可以从这个已完成的最大事务 ID 继续进行批量的小事务，重新开启事务的代价就显得比较低。 好处二： 用户可以知道现在大概已经执行到了哪个阶段 1UPDATE account SET account_total=account_total+1 + (1+interest_rate); 并发问题更新丢失： Dirty Read 读取到未提交的数据，之后回滚 ，修改成 READ UNCOMMITTED 隔离级别可以处理 1SELECT @@tx_isolation; 二级封锁协议 在一级的基础上，要求读取数据 A 时必须加 S 锁，读取完马上释放 S 锁。 可以解决读脏数据问题，因为如果一个事务在对数据 A 进行修改，根据 1 级封锁协议，会加 X 锁，那么就不能再加 S 锁了，也就是不会读入数据。 . 不可重复读 session1 执行事务期间，另一个 session2 事务对session1 读取的数据修改并提交 将事务隔离级别升级为 REPEATABLE READ 即可处理该问题 幻读 侧重于删除和增加 Transaction A 读取与搜索条件相匹配的若干行， Transaction B 插入或删除行修改 Transaction A 的结果集。 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插⼊的数据的。因此，幻读在“当前读”下才会出现。 上⾯session B的修改结果，被session A之后的select语句⽤“当前读”看到，不能称为幻读。幻读仅专指“新插⼊的⾏”。 幻读有什么问题？ ⾸先是语义上的。session A在T1时刻就声明了，“我要把所有d=5的⾏锁住，不准别的事务进⾏读写操作”。⽽实际上，这个语义被破坏了。 其次，是数据⼀致性的问题。 我们知道，锁的设计是为了保证数据的⼀致性。⽽这个⼀致性，不⽌是数据库内部数据状态在此刻的⼀致性，还包含了数据和⽇志在逻辑上的⼀致性。 原因很简单。在T3时刻，我们给所有⾏加锁的时候，id=1这⼀⾏还不存在，不存在也就加不上锁。 也就是说，即使把所有的记录都加上锁，还是阻⽌不了新插⼊的记录，这也是为什么“幻读”会被单独拿出来解决的原因。 到这⾥，其实我们刚说明完⽂章的标题 ：幻读的定义和幻读有什么问题。 隔离级别隔离得越严实，效率就会越低。 READ UNCOMMITTED: ⼀个事务还没提交时，它做的变更就能被别的事务看到。 READ COMMIT: ⼀个事务提交之后，它做的变更才会被其他事务看到。 REPEATABLE READ: ⼀个事务执⾏过程中看到的数据，总是跟这个事务在启动时看到的数据是⼀致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可⻅的。 SERIALIZABLE: 对于同⼀⾏记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前⼀个事务执⾏完成，才能继续执⾏。主要用于实现 InnoDB 的分布式事务。 InnoDB 在 REPEATABLE READ 级别下，使用 Next-Key Lock 锁算法，避免幻读的产生。 隔离级别与事务问题 事务隔离的实现 read view 算法 在 MySQL 中，实际上每条记录在更新的时候都会同时记录⼀条回滚操作。记录上的最新值，通过回滚操作，都可以得到前⼀ 个状态的值。 回滚⽇志什么时候删除呢？ 在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要⽤到这些回滚⽇志时，回滚⽇志会被删除。 什么时候才不需要了呢？ 当系统⾥没有⽐这个回滚⽇志更早的 read-view 的时候。 为何尽量不使用长事务？ ⻓事务意味着系统⾥⾯会存在很⽼的事务视图。由于这些事务随时可能访问数据库⾥⾯的任何数据，所以这个事务提交之前，数据库⾥⾯它可能⽤到的回滚记录都必须保留，这就会导致⼤量占⽤存储空间。 除此之外，⻓事务还占⽤锁资源，可能会拖垮库。 其他开启事务的方式 显式启动事务语句： begin 或者 start transaction, 提交 commit，回滚 rollback； set autocommit=0： 该命令会把这个线程的⾃动提交关掉。这样只要执⾏⼀个 select 语句，事务就启动，并不会⾃动提交，直到主动执⾏ commit 或 rollback 或断开连接。 建议使⽤⽅法⼀，如果考虑多⼀次交互问题，可以使⽤ commit work and chain 语法。在 autocommit=1 的情况下⽤ begin 显式启动事务，如果执⾏ commit 则提交事务。如果执⾏ commit work and chain 则提交事务并⾃动启动下⼀个事务。 快照InnoDB⾥⾯每个事务有⼀个 唯⼀的事务ID，叫作transaction id。它是在事务开始的时候向InnoDB的事务系统申请的，是按申请顺序严格递增的。 ⽽每⾏数据也都是有多个版本的。每次事务更新数据的时候，都会⽣成⼀个新的数据版本，并且把transaction id赋值给这个数据版本的事务ID，记为row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。 也就是说，数据表中的⼀⾏记录，其实可能有多个版本(row)，每个版本有⾃⼰的row trx_id。 InnoDB利⽤了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能⼒。 更新数据都是先读后写的，⽽这个读，只能读当前的值，称为“当前读”(currentread)。 InnoDB的⾏数据有多个版本，每个数据版本有⾃⼰的row trx_id，每个事务或者语句有⾃⼰的⼀致性视图。普通查询语句是⼀致性读，⼀致性读会根据row trx_id和⼀致性视图确定数据版本的可⻅性。 对于可重复读，查询只承认在事务启动前就已经提交完成的数据； 对于读提交，查询只承认在语句启动前就已经提交完成的数据； ⽽当前读，总是读取已经提交完成的最新版本。你也可以想⼀下，为什么表结构不⽀持“可重复读”？这是因为表结构没有对应的⾏数据，也没有row trx_id，因此只能遵循当前读的逻辑。 RR 下解决幻读表象：快照读(非阻塞读)–伪MVCC 内在：是因为事务对数据加了next-key锁(行锁+gap锁) -gap锁会用在非唯一索引或者不走索引的当前读中 RC、RR 下的 InnoDB 的非阻塞读实现 RR 下可能读取到老的版本 RR 创建快照的时机决定了事务的版本 123456session1:UPDATE ... -- 1session2:SELECT -- 3SELECT ... LOCK IN SHARE MODE; -- 2 数据行中三个行隐藏参数： DB_TRX_ID: 最近一次对本行数据进行修改的数据 ID DB_ROW_PTR: 回滚指针， 指向 undo 日志 DB_ROW_ID: 无主件时隐式的 ID (2) undo 日志： 老版本 针对 Insert undo log, 针对 update undo log (3) read view: 可见性算法 MVCC: 读不加锁，读写不冲突，读多写少 伪 MVCC： 无法多版本共存 RR 避免 幻读 产⽣幻读的原因是，⾏锁只能锁住⾏，但是新插⼊记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB只好引⼊新的锁，也就是间隙锁(Gap Lock)。 但是间隙锁不⼀样，跟间隙锁存在冲突关系的，是“往这个间隙中插⼊⼀个记录”这个操作。间隙锁之间都不存在冲突关系。 间隙锁和 next-key lock 的引⼊，帮我们解决了幻读的问题，但同时也带来了⼀些“困扰”。 间隙锁的引⼊，可能会导致同样的语句锁住更⼤的范围，这其实是影响了并发度的。 ⾏锁确实⽐较直观，判断规则也相对简单，间隙锁的引⼊会影响系统的并发度，也增加了锁分析的复杂度，但也有章可循 next-key 锁 行锁： Gap 锁： 锁定一个范围，不包含当前 () GAP 锁出现的时机 出现的场景： WHERE + INDEX where 条件全部命中，不会加 Gap Lock, 只会加 Record Lock where 条件部分命中，或全部不命中，加 Gap Lock; Gap 锁会用在非唯一索引或者不走 index 的当前读中： 非唯一索引 不走索引的当前读，尽量避免 MVCC Multiversion concurrency control 多版本并发控制。 并发访问（读或者写）数据库时，对正在事务内处理的数据做多版本的管理，用来避免由于写操作的堵塞，而引发读操作失败的并发问题。 Ref 《MySQL技术内幕：InnoDB存储引擎(第二版)》姜承尧","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}]},{"title":"Java-Map类源码","date":"2021-04-08T16:59:34.000Z","path":"2021/04/09/Java-Map类源码/","text":"HashMap(JDK7)Hash 表结构几个重要的通用操作， hash 函数、hash 冲突、rehash 及扩容； 存储结构与初始化（1） 结构 底层结构中为每一个 Node 添加指向 prev, next 的引用 1transient Entry[] table; Entry 存储着键值对，为链表结构，数组每个位置相当于一个桶，桶中存放链表。借助其来处理 Hash 冲突。 123456class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; Entry&lt;K,V&gt; next; int hash;&#125; （2） 初始化 ① 无参初始化； ② Map 传入初始化； ③ 指定初始化容量； ④ 执行初始化容量和负载因子； 1Map&lt;Obejct, Object&gt; map = new HashMap&lt;&gt;(x * 4/3); // loadFactor to prevent grow 操作1、hash 函数 | 元素定位 ① 整体的定位 12int hash = hash(key);int i = indexFor(hash, table.length); ② 具体的 hash 确定 通过多次移位和异或进行 hash 扰动，使其尽量不依赖于传入的 hashCode 的不均匀性； 123456789101112final int hash(Object k) &#123; int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125;final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value);&#125; ③ 根据 hash 确定桶下标 123static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; 2、get() 查找需要分成两步进行： 计算键值对所在的桶； 在链表上顺序查找，时间复杂度和链表的长度成正比； 1234567public V get(Object key) &#123; if (key == null) return getForNullKey(); Entry&lt;K,V&gt; entry = getEntry(key); return null == entry ? null : entry.getValue();&#125; 3、put (1) 总体实现流程 ①判断初始化，未初始化则做对应的初始化 ②Null 值特殊处理，Null 无法调用 hashCode()，防止 NullPointerException ③确定桶下标 ④遍历链表，若重复直接覆盖并返回 ⑤为新加入元素，插入 &lt;K,V&gt; ⑥根据情况看是否需要扩容处理 1234567891011121314151617181920212223242526public V put(K key, V value) &#123; if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; // 键为 null 单独处理 if (key == null) return putForNullKey(value); int hash = hash(key); // 确定桶下标 int i = indexFor(hash, table.length); // 先找出是否已经存在键为 key 的键值对，如果存在的话就更新这个键值对的值为 value for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; // 插入新键值对 addEntry(hash, key, value, i); return null;&#125; （2） 对于 NULL 的插入处理 无法调用 null 的 hashCode() 方法，也就无法确定该键值对的桶下标，只能通过强制指定一个桶下标来存放。HashMap 使用第 0 个桶存放键为 null 的键值对。 12345678910111213private V putForNullKey(V value) &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; /*0 bucket*/ if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(0, null, value, 0); return null;&#125; （3） 处理 Hash 碰撞 头插法处理； 在容量阈值达到时，进行 2 倍扩容操作； 是一种添加之后，再进行扩容的行为，依赖的是上次添加完毕的情况； 12345678910111213141516void addEntry(int hash, K key, V value, int bucketIndex) &#123; if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); &#125; createEntry(hash, key, value, bucketIndex);&#125;void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; // 头插法，链表头部指向新的键值对 table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; 4、扩容 | rehash （1） 影响的变量 ① loadFactor： 负载因子时间上和空间上的一种平衡: ↑ 查找性能差, 空间利用率高； ↓ 查找性能高, 空间利用率低； ② size | threshold： size/capacity 达到负载因子时进行对应的扩容 ③ modCount：用来处理扩容时出现并发访问造成数据不一致，从而 fail-fast 1234567static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;static final float DEFAULT_LOAD_FACTOR = 0.75f;transient int size;int threshold;final float loadFactor;transient int modCount; （2） 扩容实现 在负载因子达到给定值的情况下进行扩容； 扩容为原来数组的两倍； 需要将原来的 Entry 重新插入到新建的表中； 1234567void addEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); if (size++ &gt;= threshold) resize(2 * table.length); ...&#125; （3） rehash | 计算桶下标 在进行扩容时，需要把键值对重新放到对应的桶上。HashMap 使用了一个特殊的机制，可以降低重新计算桶下标的操作。 根据 hash 值在当前的高位上是否为 0，进行不同的处理。 为 0： 不需要移动 为 1： 移动偏移 原来容量对应的桶上； 123456789101112131415161718192021222324252627282930void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor);&#125;void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125;&#125; 其他（1） JDK7 与 JDK8 中 HashMap 的比较 ① 底层结构上： JDK8 采用 数组 + 链表 + 红黑树实现，长度过长转换成红黑树有效防范了 Hash 碰撞攻击； 将原来的 Entry 改为 Node，表示红黑树节点、链表节点； ② hash 函数： JDK8 只需要一次移位和异或即可，既能有效处理冲突上还保证了执行效率； ③ 处理 hash 冲突上： 在为链表时，通过尾插法进行处理，避免出现逆序且链表死循环问题； （2） 不安全体现 扩容时因为头插法而形成环形引用，造成无限循环，CPU 100%； put 操作可能造成丢失修改； HashMap(JDK8)底层结构与初始化（1） 整体结构 ① table： 存储节点类型，用于多态扩展 ② loadFactor： 负载因子，控制扩容的时机 ③ modCount： 控制迭代访问， fail-fast 12345transient Node&lt;K,V&gt;[] table; transient int size;transient int modCount;int threshold;final float loadFactor; （2） 节点与树化 ① 控制树化的时机： TREEIFY_THRESHOLD(8)： 链表元素大于该值的时候进行树化 UNTREEIFY_THRESHOLD(6)： TreeNode 中元素删除到该值时，将结构退化为链表，一般比 TREEIFY_THRESHOLD(8) 小，避免复杂度震荡 MIN_TREEIFY_CAPACITY(64): 在整个 HashMap 的容量小于该值的时候，即使单个桶中的元素达到 TREEIFY_THRESHOLD(8)，不会进行树化，会直接进行 rehash。 123static final int TREEIFY_THRESHOLD = 8;static final int UNTREEIFY_THRESHOLD = 6; /* prevent complexity oscillation */static final int MIN_TREEIFY_CAPACITY = 64; /* prevent resize and treeify conflict */ ② 链表节点结构： 123456class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next;&#125; ③ 红黑树节点结构： 继承 LinkedHashMap.Entry，便于从 tree 回退到 listNode 1234567static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red;&#125; 操作1、hash 函数 | 元素定位 一次异或一次移位进行 hash 扰动，避免依赖于原来 hashCode 处理键冲突，提高 hash 值的扩散性； 对于 null 的 KEY，直接使其 hash 值为 0，从而避免 NullPointerException； 1234int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125; 2、get() 执行过程： 定位到对应的桶看是否有该 KEY 首个节点为该 KEY，则直接返回 若为 TreeNode，进行红黑树的查找逻辑 若为 ListNode 进行链表的迭代遍历查找 123456789101112131415161718192021222324public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 3、put （1） 执行流程： ① 如果 HashMap 未被初始化过，则初始化； ② 对 Key 求 Hash 值，然后再计算下标； ③ 如果没有碰撞直接放入桶中； ④ 如果碰撞了，以链表的方式链接到后面； ⑤ 如果链表长度超过阀值，就把链表转成红黑树； ⑥ 如果链表长度低于 6，就把红黑树转回链表； ⑦ 如果节点已经存在就替换旧值； ⑧ 如果桶满了（容量 16 *加载因子 0.75 ），就需要 resize（扩容 2 倍后重排） （2） 额外功能： null 值处理： 直接通过 hash(key) 给 NULL KEY 为 0 的值； onlyIfAbsent： 实现 putIfAbsent() 方法逻辑，保留旧的值； afterNodeAccess，afterNodeInsertion： hook 钩子函数进行特殊处理，可用于 LinkedHashMap 实现 LRU； 123456789101112131415161718192021222324252627282930313233343536373839404142final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) /* 初始情况 */ n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) /* 插入的桶无元素 */ tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; /* 第一个位置与插入的重复 */ ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) /* 是否树化 */ e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); /* 树化 */ break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) /* 总体的容量扩容 */ resize(); afterNodeInsertion(evict); return null;&#125; （3） Hash 冲突处理 对于链表的 hash 冲突 部分 putVal() 方法： 为了实现树化，需要统计链表中的个数，直接遍历到链表尾部，进行插入； 123456789101112for (int binCount = 0; ; ++binCount) &#123; // binCount 记录链表中的个数 if ((e = p.next) == null) &#123; /* 迭代到链表尾部 */ p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); /* 树化 */ break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e;&#125; 3、 扩容 | rehash 两种可能情况： 仍然在原来的位置 在原来的位置上偏移原来的容量 （1） 链表拆分 将原来的链表拆成两条链表，低位链表的数据将会到新数组的当前下标位置，高位链表的数据将会到新数组的当前下标+当前数组长度的位置； 12345678910111213141516171819202122232425262728Node&lt;K,V&gt; loHead = null, loTail = null;Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next;do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125;&#125; while ((e = next) != null);if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead;&#125;if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead;&#125; 4、 树化 （1） 链表转为红黑树 在挂接的链表大于 TREEIFY_THRESHOLD 时进行树化逻辑； 容器整体大于 MIN_TREEIFY_CAPACITY 时才允许树化，否则进行 resize； 1234567891011121314151617181920void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); /* 当前容量太小，直接扩容，防 resize 和 treeify 频繁性能损耗 */ else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); &#125;&#125; （2） 红黑树转变成链表 在红黑树节点数少于 UNTREEIFY_THRESHOLD(6) 时，进行结构转变； 12345678910111213141516171819// TreeNode.splitif (loHead != null) &#123; if (lc &lt;= UNTREEIFY_THRESHOLD) tab[index] = loHead.untreeify(map); else &#123; tab[index] = loHead; if (hiHead != null) // (else is already treeified) loHead.treeify(tab); &#125;&#125;if (hiHead != null) &#123; if (hc &lt;= UNTREEIFY_THRESHOLD) tab[index + bit] = hiHead.untreeify(map); else &#123; tab[index + bit] = hiHead; if (loHead != null) hiHead.treeify(tab); &#125;&#125; ConcurrentHashMap(JDK7) ConcurrentHashMap 和 HashMap 实现上类似，最主要的差别是 ConcurrentHashMap 采用了分段锁（Segment），每个分段锁维护着几个桶（HashEntry），多个线程可以同时访问不同分段锁上的桶，从而使其并发度更高（并发度就是 Segment 的个数）。 底层结构与初始化（1） 全局结构 segmentShift | segmentMask： 用于快速定位到 Segment 的位置 1234final int segmentShift;final int segmentMask;final Segment&lt;K,V&gt;[] segments; // keep objectstatic final int DEFAULT_CONCURRENCY_LEVEL = 16; （2） Segment 结构 相当于一个带有锁的 HashMap(7) ① 继承自 ReentrantLock 从而实现并发加分段锁访问； ② 保存了 count 用于整个容器的统计，以及作为扩容的参考值； 默认情况下并发级别为 16； 123456789static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; static final int MAX_SCAN_RETRIES = Runtime.getRuntime().availableProcessors() &gt; 1 ? 64 : 1; transient volatile HashEntry&lt;K,V&gt;[] table; transient int count; // use for size() transient int modCount; transient int threshold; final float loadFactor;&#125; （3） HashEntry 结构 123456static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next;&#125; 操作1、hash &amp; 定位 先整体进行了多次异或操作，进行 hash 扰动，将每位都用上。 通过 hash 借助 sementShift 和 segmentMask 来定位到对应的段上。 123final Segment&lt;K,V&gt; segmentFor(int hash) &#123; return segments[(hash &gt;&gt;&gt; segmentShift) &amp; segmentMask];&#125; 定位到 Entry，定位 Segment 使用的是元素的 hashcode 通过再散列后得到的值的高位，而定位 HashEntry 直接使用的是再散列后的值。其目的是避免两次散列后的值一样，虽然元素在 Segment 里散列开了，但是却没有 HashEntry 里散列开。 12hash &gt;&gt;&gt; segmentShift) &amp; segmentMask // 定位Segment所使用的hash算法int index = hash &amp; (tab.length - 1); // 定位HashEntry所使用的hash算法 2、get() 无锁获取值实现： 基于 volatile 来替代锁实现，由 JMM 提供的 happen before 原则保证可见性； 通过 hash 得出对应的散列值，之后通过 hash 定位到对应的段； 在桶中再进行 hash 得到对应的桶，只有在读取到 NULL 值时才进行加锁； 12345678910public V get(Object key) &#123; int hash = hash(key.hashCode()); return segmentFor(hash).get(key, hash);&#125;// Segment 中的总数transient volatile int count;// HashEntry 中的值volatile V value; 3、put() 判断是否需要扩容，不是添加之后进行判断的，在插入前进行判断，避免无效的扩容。 扩容仅仅是对当前的 Segment 进行扩容，无需对整个容器扩容。 4、size() 在执行 size 操作时，需要遍历所有 Segment 然后把 count 累计起来。 可能的两步操作： ① 先尝试通过 RETRIES_BEFORE_LOCK(2) 次借助 segment 中 volatile 保存的 count 相加，基于 modCount 来实现的； ② 若无法则对整个容器进行加锁统计所有 Segment 的大小； 12345678910111213141516171819202122232425262728293031323334353637383940static final int RETRIES_BEFORE_LOCK = 2;public int size() &#123; final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn&#x27;t retry try &#123; for (;;) &#123; // 过多 CAS 转换成对所有 Segment 加锁获取 if (retries++ == RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation &#125; sum = 0L; size = 0; overflow = false; for (int j = 0; j &lt; segments.length; ++j) &#123; Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) &#123; sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; &#125; &#125; // 连续两次得到的结果一致，则认为这个结果是正确的 if (sum == last) break; last = sum; &#125; &#125; finally &#123; if (retries &gt; RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); &#125; &#125; return overflow ? Integer.MAX_VALUE : size;&#125; 其他（1） JDK7 与 JDK8 中 ConcurrentHashMap 的比较 ① 底层结构： 链表过长转换成红黑树 ② 同步机制： JDK7 采用分段锁实现，而 JDK8 可以有两种方案 尝试通过 CAS 来支持更高的并发度； 在 CAS 失败时通过内置锁 synchronized 锁住链表 | 红黑树的头结点； ConcurrentHashMap(JDK8) 五十几个内部类，Guava 中的 Cache 基于此实现。 无法存放 NULL。 底层结构与初始化（1） 底层结构 ① nextTable： 用于并发下的扩容操作 ② transferIndex： 在 rehash 情况下的标记索引 ③ counterCells： 用于并发下获取容量，不精确，与 LongAdder 类似 1234567transient volatile Node&lt;K,V&gt;[] table;transient volatile Node&lt;K,V&gt;[] nextTable;transient volatile long baseCount;transient volatile int sizeCtl;transient volatile int transferIndex;transient volatile int cellsBusy;transient volatile CounterCell[] counterCells; （2） 初始化 五种初始化方式： 前四种同 HashMap 初始化方式； ⑤ 初始容量、负载因子、并发级别： 含有并发级别控制； 并发级别基本不用，初始化用于与 initialCapacity 进行比较取最大值； 与 JDK7 进行兼容的处理逻辑； 操作1、hash 函数|定位 通过传入对象的 hashCode 来进行对应的处理 123static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; 2、get() 12345678910111213141516171819public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 3、put() ① 判断 Node[] 数组是否初始化，没有则进行初始化操作 ② 通过 hash 定位数组的索引坐标，是否有 Node 节点，如果没有则使用 CAS 进行添加（链表的头节点），添加失败则进入下次循环。 ③ 检查到内部正在扩容，就帮助它一块扩容。 ④ 如果 f != null 则使用 synchronized 锁住 f 元素（链表/红黑二叉树的头元素） 4.1如果是Node（链表结构）则执行链表的添加操作。 4.2如果是TreeNode（树型结构）则执行树添加操作。 ⑤ 判断链表长度已经达到临界值 8 (default)节点数超过这个值就需要将链表转换为树结构。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; // may enter this loop many times Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) /* P1. not init */ tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; /* P2. 第一个桶位置为空, CAS添加 */ if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; else if ((fh = f.hash) == MOVED) /* P3. 其他线程正在扩容 */ tab = helpTransfer(tab, f); else &#123; /* 发生 hash 碰撞, 处理 listnode OR treenode, 只有此时才加锁, 其他情况都 CAS 循环尝试 */ V oldVal = null; synchronized (f) &#123; /* 使用第一个元素作为锁, 粒度比 Segment 分段锁更加细 */ if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; binCount = 1; /* 链表的计数器 */ for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; /* 遍历到链表的结尾，在链表尾部添加节点 */ pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; /* 为树形结构的处理 */ Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) /* 判断是否需要树化, 在方法中还需处理当前是否到达树化的最小容量，否则进行扩容操作 */ treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125;&#125;addCount(1L, binCount); return null;&#125; 3、扩容 // todo HashTable 与 JDK7 的 HashMap 基本一致； 线程安全的同步容器； 底层结构与初始化（1） 底层结构 123456class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Entry&lt;K,V&gt; next;&#125; （2） 初始化容量为 11 123public Hashtable() &#123; this(11, 0.75f);&#125; 操作1、hash | 定位 直接通过取模实现； 效率较 HashMap 低； 12int hash = key.hashCode();int index = (hash &amp; 0x7FFFFFFF) % tab.length; 2、 扩容 | rehash 容量为 2*oldCapacity+1； 初始容量和扩容机制都与 HashMap 不同； 1int newCapacity = (oldCapacity &lt;&lt; 1) + 1; 3、 迭代方式 通过 Enumerator 实现，而非 Iterator。 1234567&lt;T&gt; Iterator&lt;T&gt; getIterator(int type) &#123; if (count == 0) &#123; return Collections.emptyIterator(); &#125; else &#123; return new Enumerator&lt;&gt;(type, true); &#125;&#125; 其他（1） 与 HashMap的区别 ① HashTable 设计上基于 Dictionary 实现，线程安全，执行效率低下，而 HashMap 基于 AbstractMap 实现，线程不安全，执行效率比 HashTable 高； ② NULL 值： HashMap 允许 NULL 值， HashTable 不允许； ③ 初始化与 hash 定位： HashTable 初始容量为 11，HashMap 初始容量为 16，HashTable 通过 % 的方式进行定位，HashMap 通过移位异或进行定位； ④ 迭代访问上： 两者实现的迭代不同，一个基于 Iterator，一个基于 Enumerator； LinkedHashMap默认保持元素插入属性的 Map； 可先通过 HashMap 来进行统计一些必要的数据，之后通过对 HashMap 的 KEY, VALUE 进行一些排序，将其变为有序的，使用 LinkedHashMap 来将这些顺序串联起来，之后进行对应的逻辑处理； 底层结构（1） 全局属性 ① accessOrder： 控制开启访问作为次序，可借助 LinkedHashMap 实现 LRU Cache; ② head|tail： 控制按照插入 | 访问顺序迭代； 123transient LinkedHashMap.Entry&lt;K,V&gt; head;transient LinkedHashMap.Entry&lt;K,V&gt; tail;final boolean accessOrder; （2） 节点 before | after 用于将节点连接起来方便遍历； HashMap.TreeNode 扩展此节点； 123456static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125;&#125; 应用（1） LRU Cache 通过 HashMap 的 putVal() 中提供的两个 hook 钩子函数实现特有的功能； 根据 assessOrder 值不同，迭代出不同的结果。为 true，执行 LRU 顺序，访问过后移到链表尾部，头部为最近最久未使用节点； 为 false，执行插入顺序，与 List 语义相同； 123456789101112131415161718192021222324void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else &#123; p.before = last; last.after = p; &#125; tail = p; ++modCount; &#125;&#125; 在 put 等操作之后执行，当 removeEldestEntry() 方法返回 true 时会移除最晚的节点，也就是链表首部节点 first。 evict 只有在构建 Map 的时候才为 false，在这里为 true。 1234567void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125; removeEldestEntry() 默认为 false，如果需要让它为 true，需要继承 LinkedHashMap 并且覆盖这个方法的实现，这在实现 LRU 的缓存中特别有用，通过移除最近最久未使用的节点，从而保证缓存空间足够，并且缓存的数据都是热点数据。 123protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; return false;&#125; （1-2） LinkedHashMap 实现的线程安全的 LRUCache 1234567891011121314151617181920212223private static final float DEFAULT_LOAD_FACTOR = 0.75f;private int capacity;private LinkedHashMap&lt;K,V&gt; cache;public LRUCache(int capacity) &#123; this.capacity = capacity; cache = new LinkedHashMap((int) Math.ceil(capacity/DEFAULT_LOAD_FACTOR) + 1, DEFAULT_LOAD_FACTOR, true)&#123; private static final long serialVersionUID = 223232L; @Override protected boolean removeEldestEntry(Map.Entry eldest) &#123; return cache.size() &gt;= capacity; &#125; &#125;;&#125;public synchronized V get(K key) &#123; return cache.get(key);&#125;public synchronized V put(K key, V val) &#123; return cache.put(key, val);&#125; （2） 借助多态将 HashMap 中的数据按照一定的规则进行排序 如 HashMap 存放的是词频，可以根据词频进行排序，之后迭代访问时就是词频从高到低的序列； 其他（1） 与 HashMap 的区别 ① 设计层面上： LinkedHashMap 是 HashMap 的子类； ② 底层结构及功能扩展上： LinkedHashMap 节点中添加了 before, after 用于保证顺序； ③ put 操作： LinkedHashMap 在继承的基础上重写 HashMap 中的 hook(钩子) 方法，在 LinkedHashMap 中向哈希表中插入新 Entry 的同时，还会通过 Entry 的 addBefore 方法将其链入到双向链表中。 ④ get 操作： LinkedHashMap 中重写了 HashMap 中的 get 方法，通过 HashMap 中的 getEntry 方法获取 Entry 对象。 在此基础上，进一步获取指定键对应的值。 ⑤ 在扩容操作上： 虽然 LinkedHashMap 完全继承了 HashMap 的 resize 操作，但是鉴于性能和 LinkedHashMap 自身特点的考量， LinkedHashMap 对其中的重哈希过程(transfer 方法)进行了重写。 TreeMap红黑树的实现 // todo X、其他 与 HashMap 的区别 ① 顺序性： TreeMap 可对按照 Key 的自然顺序或是传入的比较器进行排序； ② 存取效率上： O(1) VS O(logN)； ThreadLocalMap对于 ThreadLocal： 当使用 ThreadLocal 维护变量时，ThreadLocal 为每个使用该变量的线程提供独立的变量副本，所以每一个线程都可以独立地改变自己的副本，而不会影响其它线程所对应的副本； 是一种无锁同步方案； 是一种用空间换取时间来保证线程安全的方案； 多线程情况下，对应的子线程的 ThreadLocal 无法获取到父线程的 ThreadLocal，需要第三方工具包支持，可选择 alibaba 的 transmittable-thread-local 包； 1、底层结构 （1） 全局结构 1234static final int INITIAL_CAPACITY = 16;Entry[] table;int size = 0;int threshold; （2） Entry 继承自 WeakReference ，在无活跃线程或栈中持有时，在 GC 时就会被回收； 节点只保存值，ThreadLocalMap 不是使用链地址法来解决 Hash 冲突； 多个线程，只设置一个 ThreadLocal 变量，在这个线程中的 ThreadLocal 变量的值始终是只有一个的，即以前的值被覆盖了的。这里是因为 Entry 对象是以该 ThreadLocal 变量的引用为 key 的，所以多次赋值以前的值会被覆盖。 1234567static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; /* ThreadLocal as key */ super(k); value = v; &#125;&#125; （3） 定位 通过此来进行 hash 桶的定位； 1final int threadLocalHashCode = nextHashCode(); （4） 负载因子 0.66 空间利用率相较于 HashMap 的 0.75 较低，但加快查询； 同时配合开放地址法来快速定位到空闲的桶； 123private void setThreshold(int len) &#123; threshold = len * 2 / 3;&#125; 操作1、hash() 没有扰动函数，通过 ThreadLocal 中保存的 threadLocalHashCode 来实现； threadLocalHashCode 根据维护的 AtomicInteger nextHashCode 获取； 12345678// ThreadLocalprivate final int threadLocalHashCode = nextHashCode();private static AtomicInteger nextHashCode = new AtomicInteger();private static int nextHashCode() &#123; return nextHashCode.getAndAdd(HASH_INCREMENT);&#125;// ThreadLocalMap.getEntryint i = key.threadLocalHashCode &amp; (table.length - 1); 2、get() 12345678private Entry getEntry(ThreadLocal&lt;?&gt; key) &#123; int i = key.threadLocalHashCode &amp; (table.length - 1); Entry e = table[i]; if (e != null &amp;&amp; e.get() == key) return e; else return getEntryAfterMiss(key, i, e);&#125; 3、set() ① 先获取到 ThreadLocal 键的 thresholdLocalHashCode ，以此来确定对应的桶下标； ② 如果在该位置正好与当前进行重合，直接覆盖； ③ 如果该位置无元素，则将其放入该位置； ④ 上面两个都不满足的情况下，使用线性探测法不断寻找到对应的满足上述两个条件的位置； ⑤ 上述都不满足，创建一个节点，并清理一些桶位，之后进行重新 hash ； 1234567891011121314151617181920212223242526272829303132private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; // We don&#x27;t use a fast path as with get() because it is at // least as common to use set() to create new entries as // it is to replace existing ones, in which case, a fast // path would fail more often than not. Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; // bucket position e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) // add node need to clean some object rehash();&#125; 4、扩容 | rehash （1） rehash 123456void rehash() &#123; expungeStaleEntries(); // Use lower threshold for doubling to avoid hysteresis if (size &gt;= threshold - threshold / 4) resize();&#125; （2） resize 扩容为原来的两倍 123456789101112131415161718192021222324252627void resize() &#123; Entry[] oldTab = table; int oldLen = oldTab.length; int newLen = oldLen * 2; Entry[] newTab = new Entry[newLen]; int count = 0; for (int j = 0; j &lt; oldLen; ++j) &#123; Entry e = oldTab[j]; if (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == null) &#123; e.value = null; // Help the GC &#125; else &#123; int h = k.threadLocalHashCode &amp; (newLen - 1); while (newTab[h] != null) h = nextIndex(h, newLen); newTab[h] = e; count++; &#125; &#125; &#125; setThreshold(newLen); size = count; table = newTab;&#125; 其他（1）与 HashMap 的比较 ① 底层结构： ThreadLocalMap 只有数组，HashMap 通过数组 + 链表(Tree) 方式 节点引用类型： ThreadLocalMap 节点为弱引用，会下一次 GC 被回收 ② hash： 并发下通过 AtomicInteger 实现 ③ hash 冲突处理： ThreadLocalMap 通过线性探测法实现的，HashMap 通过链地址法实现； WeakHashMap底层结构（1）节点 与 Entry 继承 WeakReference，当前 Entry 需要引用队列来进行处理； 1234567891011121314151617// 成员ReferenceQueue&lt;Object&gt; queue = new ReferenceQueue&lt;&gt;();// 单独的 Entryprivate static class Entry&lt;K,V&gt; extends WeakReference&lt;Object&gt; implements Map.Entry&lt;K,V&gt; &#123; V value; final int hash; Entry&lt;K,V&gt; next; Entry(Object key, V value, ReferenceQueue&lt;Object&gt; queue, int hash, Entry&lt;K,V&gt; next) &#123; /* must use reference queue */ super(key, queue); this.value = value; this.hash = hash; this.next = next; &#125;&#125; 操作1、hash | 定位 通过四次异或来进行 hash 扰动，使其少依赖于原始的 hashCode() 12345678final int hash(Object k) &#123; int h = k.hashCode(); h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125;int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; 2、put NULL 值： 对 NULL 的 KEY 处理机制，将 NULL 作为指向一个对象进行存储 1234static final Object NULL_KEY = new Object();private static Object maskNull(Object key) &#123; return (key == null) ? NULL_KEY : key;&#125; Ref 《Java并发编程的艺术》方腾飞 / 魏鹏 / 程晓明","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"源码","slug":"源码","permalink":"http://example.com/tags/%E6%BA%90%E7%A0%81/"}]},{"title":"Java-集合类源码","date":"2021-04-08T15:18:17.000Z","path":"2021/04/08/Java-集合类源码/","text":"ListArrayList基本性质： 底层基于数组保存； 增删慢、随机查询快； 线程不安全； 底层结构与初始化（1） 结构 123transient Object[] elementData; int size;transient int modCount = 0; （2） 加载和初始化 懒加载形式，在 add() 时进行对应的初始化； 共支持三种初始化方式： ① 无参构造： 默认不进行数据的初始化； ② 给定容量： 程序中通过给定容量来优化； ③ 通过放入 Collection 接口进行初始化； 123456789101112131415161718192021222324252627282930313233static final int DEFAULT_CAPACITY = 10;private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125;public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); &#125;&#125;public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; &#125;&#125;import com.google.common.collect.Lists;// 使用 Guava 创建指定容量的 ListList&lt;String&gt; list = Lists.newArrayListWithCapacity(oldList.size()); 操作1、add ① 默认插入尾部，O(1) 实现； ② 任意位置插入 将插入位置后的所有元素右移一位，之后在插入位置赋值； 插入的开销与插入的位置有关； 123456789101112131415public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! System.arraycopy(elementData, index, elementData, index + 1, size - index); /* 右移一位 */ elementData[index] = element; size++;&#125; 2、remove 需要调用 System.arraycopy() 将 index + 1 后面的元素都复制到 index 位置上，该操作的时间复杂度为 O(N)，开销大； 同 add() 操作，删除与位置紧密相关； 12345678910public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; 3、 扩容 扩容后的大小为 oldN * 1.5 + 1； 通过 Arrays.copyOf() 复制到新数组中； 可通过指定初始容量的方式，来减少扩容的次数，减少不必要的开销； 12345678910111213141516171819202122232425262728293031public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); /* init capacity */ &#125; ensureExplicitCapacity(minCapacity);&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); /* copy to impl. */ elementData = Arrays.copyOf(elementData, newCapacity);&#125; 4、 迭代访问 采用快速失败模式实现； 通过成员变量 modCount 与 expectedModCount 比较实现； 主要用在序列化获得迭代操作时进行判断，对应抛出 ConcurrentModificationException； 5、序列化 只序列化数组中存放值的这些部分。 ArrayList 基于数组实现，并且具有动态扩容特性，因此保存元素的数组不一定都会被使用，那么就没必要全部进行序列化。 1transient Object[] elementData; // not serialize 通过 writeObject() 和 readObject() 来控制只序列化数组中有元素填充那部分内容。 1234567891011121314151617181920212223242526272829303132333435363738private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; int expectedModCount = modCount; /* keep to compare */ s.defaultWriteObject(); /* only have space */ s.writeInt(size); for (int i=0; i&lt;size; i++) &#123; /* only write have element */ s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; /* use for fail-fast */ throw new ConcurrentModificationException(); &#125;&#125;private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; s.defaultReadObject(); s.readInt(); // ignored if (size &gt; 0) &#123; // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125;&#125;// 将 list 序列化到指定文件ArrayList list = new ArrayList();ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(file));oos.writeObject(list); 其他（1） ArrayList 与 Array 的比较 ① 存储类型： Array 可以存放基本和对象类型， ArrayList 只能存放对象类型； ② 存放元素的大小： ArrayList 动态可变，Array 不可变; ③ 其他方法和特性： ArrayList 提供 addAll()，removeAll()，iterator() 等方法; 对于基本类型数据，集合使用自动装箱来减少编码工作量。但当处理固定大小的基本数据类型的时候，这种方式相对比较慢。 （2）ArrayList 与 LinkedList 的比较 都是线程不安全的容器，都实现了 List 接口具备 List 的特性。 ① 底层结构： ArrayList 基于索引的数据接口，底层是动态数组实现，LinkedList 以元素列表的形式存储数据，是双向链表实现； ② 操作性质： 随机访问： ArrayList 支持随机访问，LinkedList 不支持； 元素删除： LinkedList 在任意位置添加删除元素更快； 操作是否与元素位置的影响： 比较插入和删除是否受元素位置影响，ArrayList 插入和删除受元素位置影响，add(e) 默认追加到末尾，在指定位置 i 插入和删除时复杂度为 O(n-i)，而 LinkedList 链表存储，插入和删除不受位置影响； ③ 内存占用上： LinkedList 存放两个指针，相同数据量下占用更多的空间； （3） ArrayList 与 Vector 的比较 同步性： Vector 是同步的，因此开销就比 ArrayList 要大，访问速度更慢。 扩容： Vector 每次扩容请求其大小的 2 倍空间，而 ArrayList 是 1.5 倍。且 Vector 可以设置增长空间的大小。 LinkedList基本性质： 基于双向链表实现，只能顺序访问，但是可以快速地在链表中间插入和删除元素。 线程不安全； LinkedList 可以用作栈、队列和双向队列。 底层结构与初始化（1） 结构 通过记录 first, last, size 便于边界操作（注：用于队列、栈、双端队列） 队列中每个节点都存放元素，存在初始化情况； 12345678transient int size = 0;transient Node&lt;E&gt; first;transient Node&lt;E&gt; last;class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev;&#125; （2） 初始化 不支持初始情况下给定对应的容量，即基于链表都为无界队列； 123456public LinkedList() &#123;&#125;public LinkedList(Collection&lt;? extends E&gt; c) &#123; this(); addAll(c);&#125; 操作1、add 添加元素, 最后元素与中间元素, 可处理头结点位置。 12345678910111213141516171819202122232425262728293031public void add(int index, E element) &#123; checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index));&#125;void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125;void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) /* init 处理 */ first = newNode; else pred.next = newNode; size++; modCount++;&#125; 2、remove 操作不受指定位置的影响； 为双向链表中指定节点的删除； 1234567891011121314151617181920212223242526272829public E remove(int index) &#123; checkElementIndex(index); return unlink(node(index));&#125;E unlink(Node&lt;E&gt; x) &#123; // assert x != null; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; size--; modCount++; return element;&#125; Vector基本性质： 底层基于数组保存元素； 随机查询快，增删慢； 线程安全； Java 中的 Stack 通过继承 Vector 实现的； 底层结构与初始化（1） 结构 ① elementCount：初始容量为 10，非懒加载实现； ② capacityIncrement；可以设置每次容量的增长数量； ③ 无 modCount： 同步容器； 123Object[] elementData;int elementCount;int capacityIncrement; （2） 初始化 支持 ArrayList 的各种初始化； 支持设置每次的扩容时的容量增长； 1234567891011public Vector() &#123; this(10);&#125;public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement;&#125; 操作1、add / get 对修改底层结构的函数进行加锁同步访问。 12345678910111213public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125;public synchronized E get(int index) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index);&#125; 2、扩容机制 默认扩容为 oldN * 2； 可以通过用户设置的正常数量进行控制扩容大小； 1234567891011void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? /* 默认扩容 1 倍 */ capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);&#125; 其他替代方案 因为 Vector 通过加锁实现，粒度大效率低。 （1） 获得对应线程不安全容器的同步容器 12List&lt;String&gt; list = new ArrayList&lt;&gt;();List&lt;String&gt; synList = Collections.synchronizedList(list); （2） 使用并发容器，如 CopyOnWriteArrayList 1List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); CopyOnWriteArrayList基本性质： 易引起 YongGC, FullGC 不可用于实时的数据， 写操作复制防止并发修改不一致 适合读多写少的情景 读操作无需加锁，写操作加锁 三个设计思想： 读写分离 最终一致性 新开辟空间，解决并发冲突 底层结构与初始化（1） 结构 ① ReentrantLock： 通过此来实现并发访问 1234final transient ReentrantLock lock = new ReentrantLock();transient volatile Object[] array;static final long lockOffset;static final sun.misc.Unsafe UNSAFE; （2） 初始化 三种初始化方式： LinkedList 的初始化方式 支持泛型数组初始化 123public CopyOnWriteArrayList(E[] toCopyIn) &#123; setArray(Arrays.copyOf(toCopyIn, toCopyIn.length, Object[].class));&#125; 操作(1) add 并发下安全的容器，需要处理并发访问问题、处理复制问题。 包含 lock 加锁获取与释放： ① 获取原数组 ② 复制出 len+1 的数组 ③ 为新数组末尾复制 ④ 修改内部数组指向 123456789101112131415161718boolean add(E e) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); /* 获取原数组, volatile 保证可见性 */ int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); /* 复制数组 */ newElements[len] = e; setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125;final void setArray(Object[] a) &#123; array = a;&#125; (2) get 无需加锁直接访问, 在 add 操作的同时可访问旧有的数据 ⇒ 实时性得不到保证 。 123456E get(int index) &#123; return get(getArray(), index);&#125;E get(Object[] a, int index) &#123; return (E) a[index];&#125; 3、迭代方式 通过安全失败实现，将当前数组放入到 Iterator 实现类中作为快照访问。 123public Iterator&lt;E&gt; iterator() &#123; return new COWIterator&lt;E&gt;(getArray(), 0);&#125; 迭代器中保存某个时间点下底层数组的快照，通过 cursor 来进行向前迭代访问。 1234567891011final Object[] snapshot;int cursor;COWIterator(Object[] elements, int initialCursor) &#123; cursor = initialCursor; snapshot = elements;&#125;E next() &#123; if (! hasNext()) throw new NoSuchElementException(); return (E) snapshot[cursor++];&#125; 其他（1） 读写分离 写操作在一个复制的数组上进行，读操作还是在原始数组中进行，读写分离，互不影响； 写操作需要加锁，防止并发写入时导致写入数据丢失； 写操作结束之后需要把原始数组指向新的复制数组； （2）适用场景 CopyOnWriteArrayList 在写操作的同时允许读操作，大大提高了读操作的性能，因此很适合读多写少的应用场景。 （3） 缺陷 内存占用：在写操作时需要复制一个新的数组，使得内存占用为原来的两倍左右； 数据不一致：读操作不能读取实时性的数据，因为部分写操作的数据还未同步到读数组中。 所以 CopyOnWriteArrayList 不适合内存敏感以及对实时性要求很高的场景。 SetHashSet底层结构与初始化通过一个 HashMap 实现，对应的 Value 为指定的一个 Object 12private transient HashMap&lt;E,Object&gt; map;private static final Object PRESENT = new Object(); 操作1、add 123public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 2、remove 123public boolean remove(Object o) &#123; return map.remove(o)==PRESENT;&#125; LinkedHashSet基于 LinkedHashMap 实现, HashSet 的子类。 1234567891011SetHashSet(int initialCapacity, float loadFactor, boolean dummy) &#123; map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor);&#125;public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable &#123; public LinkedHashSet() &#123; super(16, .75f, true); &#125;&#125; TreeSet与 HashSet 的区别 ① 底层结构：HashSet 基于 hash 表实现，元素无序， 一些方法 add(), remove(), contains() 复杂度为 O(1)； ② 有序性： TreeSet 基于红黑树实现，元素有序， add(), remove(), contains() 复杂度为 O(logN)； QueuePriorityQueue基本性质： 有序的优先队列； 不可以存放 NULL，NULL 无自然顺序； 非线程安全，入队和出队的时间复杂度为 O(logN)； 基于堆结构实现，默认情况下为最小堆； 底层结构与初始化（1） 底层结构 数组保存的完全二叉树，首元素存放元素值。 堆顶元素有序，默认情况下为最小堆。 comparator： 默认自定义比较器优先于存入对象的自然排序进行比较 1234transient Object[] queue; int size = 0;final Comparator&lt;? super E&gt; comparator;transient int modCount = 0; （2） 初始化 可指定初始容量与比较器； 1234567static final int DEFAULT_INITIAL_CAPACITY = 11;PriorityQueue(Comparator&lt;? super E&gt; comparator) &#123; this(DEFAULT_INITIAL_CAPACITY, comparator);&#125;PriorityQueue(int initialCapacity, Comparator&lt;? super E&gt; comparator); 操作（1） offer 实现： 先将元素放到完全二叉树的尾节点 之后不断上浮调整结构使其符合堆特性 辅助-shiftUp 上浮函数，用于维护最小堆的结构。 赋值替换交换优化； 找出正确位置并放入； 123456789101112void siftUpComparable(int k, E x) &#123; Comparable&lt;? super E&gt; key = (Comparable&lt;? super E&gt;) x; while (k &gt; 0) &#123; int parent = (k - 1) &gt;&gt;&gt; 1; Object e = queue[parent]; if (key.compareTo((E) e) &gt;= 0) break; queue[k] = e; k = parent; &#125; queue[k] = key;&#125; （2） poll 弹出当前堆顶元素。 实现： 保存堆顶元素； 将堆顶与最末叶子节点交换，之后通过堆顶下沉实现结构的维护； siftDown，下沉函数，最小堆的结构； 通过赋值来替换掉交换操作； 找到元素应该放入的正确位置放入； 1234567891011121314151617void siftDownComparable(int k, E x) &#123; Comparable&lt;? super E&gt; key = (Comparable&lt;? super E&gt;)x; int half = size &gt;&gt;&gt; 1; // loop while a non-leaf while (k &lt; half) &#123; int child = (k &lt;&lt; 1) + 1; // assume left child is least Object c = queue[child]; int right = child + 1; if (right &lt; size &amp;&amp; ((Comparable&lt;? super E&gt;) c).compareTo((E) queue[right]) &gt; 0) c = queue[child = right]; if (key.compareTo((E) c) &lt;= 0) break; queue[k] = c; k = child; &#125; queue[k] = key;&#125; （3） remove remove(o) 删除一个对象，为 Collection 中的方法，需要先进行向下调整后进行向上调整； 实现： 最末叶子节点赋值到当前删除的位置； 让原来的最末叶子节点向下调整； 若结构不合法则向上调整； 123456789101112131415161718E removeAt(int i) &#123; // assert i &gt;= 0 &amp;&amp; i &lt; size; modCount++; int s = --size; if (s == i) // removed last element queue[i] = null; else &#123; E moved = (E) queue[s]; /* 最末叶子节点赋值到当前删除的位置 */ queue[s] = null; siftDown(i, moved); /* 让原来的最末叶子节点向下调整 */ if (queue[i] == moved) &#123; /* 结构不合法向上调整 */ siftUp(i, moved); if (queue[i] != moved) return moved; &#125; &#125; return null;&#125; （4） heapify 初始传入为 Collection 进行堆化处理，借助原始结构，从中间处向上不断下沉处理，相比较每次插入到最末叶子节点进行向上调整效率更高； 完全二叉树中间节点位置 size / 2 - 1； 12345678void initFromCollection(Collection&lt;? extends E&gt; c) &#123; initElementsFromCollection(c); heapify();&#125;void heapify() &#123; for (int i = (size &gt;&gt;&gt; 1) - 1; i &gt;= 0; i--) /* 完全二叉树从上层节点不断向下调整实习 */ siftDown(i, (E) queue[i]);&#125; 3、扩容 小数据量快速 2 * oldCapacity + 2 扩容，容量大于 64 后进行 1.5 * oldCapacity 扩容； 1234567891011void grow(int minCapacity) &#123; int oldCapacity = queue.length; // Double size if small; else grow by 50% int newCapacity = oldCapacity + ((oldCapacity &lt; 64) ? (oldCapacity + 2) : (oldCapacity &gt;&gt; 1)); // overflow-conscious code if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); queue = Arrays.copyOf(queue, newCapacity);&#125; 4、迭代访问 通过双向队列 ArrayDeque 实现 1234567class Itr implements Iterator&lt;E&gt; &#123; int cursor = 0; int lastRet = -1; ArrayDeque&lt;E&gt; forgetMeNot = null; E lastRetElt = null; int expectedModCount = modCount;&#125; 其他使用场景 贪心算法选择局部最优； 图论中使用进行优化； 实现哈夫曼树等结构； ArrayDeque基于数组实现的双向队列； 可使用 Stack 的 API; 底层结构与初始化(1) 底层结构 一个数组，两个索引指向队列的头节点和尾部节点。 1234transient Object[] elements;transient int head;transient int tail;private static final int MIN_INITIAL_CAPACITY = 8; (2) 初始化 12345678910public ArrayDeque() &#123; elements = new Object[16];&#125;public ArrayDeque(int numElements) &#123; allocateElements(numElements);&#125;public ArrayDeque(Collection&lt;? extends E&gt; c) &#123; allocateElements(c.size()); addAll(c);&#125; UtilArrays1、sort JDK 中的排序都为稳定排序 算法的执行逻辑： 小数据量使用 INSERT 排序 一定规模数据量使用 QUICK 排序 大数据量使用 MERGE 排序 并非所有大数据量都是 Merge Sort，在不具备结构性时转换成 Quick Sort； （1） 归并排序 ① 小数据量转快速排序 ② 通过分配当前大小进行 merge ③ 判断排序数组的结构，不具有时使用快速排序 1234567891011121314151617181920212223static void sort(int[] a, int left, int right, int[] work, int workBase, int workLen) &#123; // Use Quicksort on small arrays if (right - left &lt; QUICKSORT_THRESHOLD) &#123; sort(a, left, right, true); return; &#125; int[] run = new int[MAX_RUN_COUNT + 1]; /* aux space to merge */ int count = 0; run[0] = left; // Check if the array is nearly sorted for (int k = left; k &lt; right; run[count] = k) &#123; // ... /* * The array is not highly structured, * use Quicksort instead of merge sort. */ if (++count == MAX_RUN_COUNT) &#123; sort(a, left, right, true); return; &#125; &#125; // ...&#125; （2） 快速排序 ① 小数据量插入排序 1234567static void sort(int[] a, int left, int right, boolean leftmost) &#123; int length = right - left + 1; // Use insertion sort on tiny arrays if (length &lt; INSERTION_SORT_THRESHOLD) &#123; if (leftmost) &#123; &#125; ② 逻辑实现 通过双枢纽元分割实现； 类似 BFPRT 算法中对于枢纽元的选取，将原来期望的复杂度转换成确定的复杂度； 12345678910111213141516171819202122232425262728left part center part right part +--------------------------------------------------------------+ | &lt; pivot1 | pivot1 &lt;= &amp;&amp; &lt;= pivot2 | ? | &gt; pivot2 | +--------------------------------------------------------------+ ^ ^ ^ | | | less k great left part center part right part +----------------------------------------------------------+ | == pivot1 | pivot1 &lt; &amp;&amp; &lt; pivot2 | ? | == pivot2 | +----------------------------------------------------------+ ^ ^ ^ | | | less k great Partitioning degenerates to the traditional 3-way (or &quot;Dutch National Flag&quot;) schema: left part center part right part +-------------------------------------------------+ | &lt; pivot | == pivot | ? | &gt; pivot | +-------------------------------------------------+ ^ ^ ^ | | | less k great （3） 插入排序 为快速排序中的子过程实现； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960if (length &lt; INSERTION_SORT_THRESHOLD) &#123; if (leftmost) &#123; /* * Traditional (without sentinel) insertion sort, * optimized for server VM, is used in case of * the leftmost part. */ for (int i = left, j = i; i &lt; right; j = ++i) &#123; int ai = a[i + 1]; while (ai &lt; a[j]) &#123; a[j + 1] = a[j]; if (j-- == left) &#123; break; &#125; &#125; a[j + 1] = ai; &#125; &#125; else &#123; /* * Skip the longest ascending sequence. */ do &#123; if (left &gt;= right) &#123; return; &#125; &#125; while (a[++left] &gt;= a[left - 1]); /* * Every element from adjoining part plays the role * of sentinel, therefore this allows us to avoid the * left range check on each iteration. Moreover, we use * the more optimized algorithm, so called pair insertion * sort, which is faster (in the context of Quicksort) * than traditional implementation of insertion sort. */ for (int k = left; ++left &lt;= right; k = ++left) &#123; int a1 = a[k], a2 = a[left]; if (a1 &lt; a2) &#123; a2 = a1; a1 = a[left]; &#125; while (a1 &lt; a[--k]) &#123; a[k + 2] = a[k]; &#125; a[++k + 1] = a1; while (a2 &lt; a[--k]) &#123; a[k + 1] = a[k]; &#125; a[k + 1] = a2; &#125; int last = a[right]; while (last &lt; a[--right]) &#123; a[right + 1] = a[right]; &#125; a[right + 1] = last; &#125; return;&#125; 2、binarySearch 计算 mid mid = (low + high) &gt;&gt;&gt; 1 12345678910111213141516171819202122// Like public version, but without range checks.private static int binarySearch0(Object[] a, int fromIndex, int toIndex, Object key) &#123; int low = fromIndex; int high = toIndex - 1; while (low &lt;= high) &#123; int mid = (low + high) &gt;&gt;&gt; 1; // @SuppressWarnings(&quot;rawtypes&quot;) Comparable midVal = (Comparable)a[mid]; @SuppressWarnings(&quot;unchecked&quot;) int cmp = midVal.compareTo(key); if (cmp &lt; 0) low = mid + 1; else if (cmp &gt; 0) high = mid - 1; else return mid; // key found &#125; return -(low + 1); // key not found.&#125; 3、asList / subList 不推荐使用，返回的 List 修改会有问题。 Collections1、提供一些容器的空实现：作为容器为空的情况下的返回值，规避空指针问题。 1234public static final List EMPTY_LIST = new EmptyList&lt;&gt;();public static final &lt;T&gt; List&lt;T&gt; emptyList() &#123; return (List&lt;T&gt;) EMPTY_LIST;&#125; 2、提供单个元素的集合： 方便传递方法的参数 123public static &lt;T&gt; List&lt;T&gt; singletonList(T o) &#123; return new SingletonList&lt;&gt;(o);&#125; 3、sort JDK8 中借助 List 中自带的 sort() 函数调用实现； 4、binarySearch 对 List 进行二分搜索 根据底层是数组还是链表采用不同的处理： ① 数组： 数组随机访问定位实现 ② 链表： 接着 ListIterator 实现二分查找 在 binarySearch（）⽅法中，它要判断传⼊的list 是否 RamdomAccess 的实例，如果是，调⽤ indexedBinarySearch() ⽅法，如果不是，那么调⽤ iteratorBinarySearch() ⽅法 1234567public static &lt;T&gt; int binarySearch(List&lt;? extends Comparable&lt;? super T&gt;&gt; list, T key) &#123; if (list instanceof RandomAccess || list.size()&lt;BINARYSEARCH_THRESHOLD) return Collections.indexedBinarySearch(list, key); else return Collections.iteratorBinarySearch(list, key);&#125; 5、提供将不安全的容器转换为同步容器 1public static &lt;T&gt; List&lt;T&gt; synchronizedList(List&lt;T&gt; list)","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"源码","slug":"源码","permalink":"http://example.com/tags/%E6%BA%90%E7%A0%81/"}]},{"title":"大数据","date":"2021-03-09T15:23:13.000Z","path":"2021/03/09/大数据/","text":"使用 Notion 做笔记，博客为 Notion 的部分导出。 https://github.com/Janhen/notes","tags":[]},{"title":"Hive-基础","date":"2021-03-09T14:57:00.000Z","path":"2021/03/09/Hive-基础/","text":"Hive [TOC] 基础说明 将 SQL 转换成 MapReduce 任务的工具。 基于 Hadoop 的数据仓库工具，存储和处理海量结构化的数据，可将结构化的数据映射成一张表 FaceBook 开发的海量数据查询工具，基本实现了 SQL-92 标准。 可以与 Druid, Kudu 进行集成 与 HBase 集成 说明 处理 HDFS 中的海量数据 通过 SQL 完成计算 基于Hadoop的数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能. 最适合数据仓库程序 缺点： HQL 表达能力有限： 无法表示迭代计算，数据挖掘方面不擅长 效率低： 自动生成的 MR 效率，调优较困难 Pig: Hive 的替代品，apache 顶级项目 一种数据流语言，不是查询语言 常用语 ETL 中的一部分 HBase: 已经可以结合 Hive 使用 Thrift: 提供了可远程访问其他进程的功能，提供了使用 JDBC, ODBC 访问 Hive 的功能 HWI: 简单的网页界面 HQL： Hive: 解释器: AST 抽象语法树 编译器 优化器 执行期 MetaStore Hive 的元数据默认存储在自带的 derby 数据库中 derby: java 开发、但进程、单用户 架构原理// TODO Client： Hive、Beeline、Hue 元数据库： 存放元数据的地方，数据库、表、分区、列的名称和属性，数据所在位置等信息 Meta store 元数据服务： 提供统一的服务接口，Client 通过 metastore 访问元数据。三种模式，内嵌、本地、远程模式。 。。。 Hive Driver： 解释器、编译器、优化器、执行器 HSQL 转化为 MapReduce 的过程 HiveSQL -&gt; AST 抽象语法树 -&gt; QB 查询块 -&gt; OperatorTree 操作树 -&gt; 优化后的 OperatorTree -&gt; MapReduce 树 -&gt; 优化后的 MapReduce 任务树 Q: Hive 关联表对应 MapReduce 如何实现？ Hive 的优缺点Hive 的优点 学习成本低。Hive提供了类似SQL的查询语言，开发人员能快速上手； 处理海量数据。底层执行的是MapReduce 任务； 系统可以水平扩展。底层基于Hadoop； 功能可以扩展。 Hive允许用户自定义函数； 良好的容错性。某个节点发生故障，HQL仍然可以正常完成； 统一的元数据管理。元数据包括：有哪些表、表有什么字段、字段是什么类型 Hive 的缺点 HQL表达能力有限； 迭代计算无法表达； Hive的执行效率不高(基于MR的执行引擎)； Hive自动生成的MapReduce作业，某些情况下不够智能； Hive的调优困难； Hive 架构 CLI: Hive 命令行 Thrift Server: Hive 可选组件，一个软件框架服务，可通过编程方式访问 Hive MetaStore: 元数据存储在 RDBMS 中，元数据包括数据库名、表名和类型、字段名和数据类型、数据所在的位置等 Driver： 驱动程序 解析器： 第三方工具(antlr) 将 HQL 字符串转换成 AST，对 AST 进行语法分析，如字段是否存在、SQL 语义正确性、表的存在 编译器： 将 AST 编译成逻辑执行计划 优化器： 对逻辑执行计划进行优化，减少不必要的列、使用分区、进行谓词下推等 执行器： 将逻辑执行计划转换成可以运行的物理计划 数据类型数据类型与转换基本数据类型 整数类型： Integer 、TINYINT、SMALINT、INT、BIGINT 浮点数类型： FLOAT、DOUBLE、DECIMAL(17byte) 字符类型： STRING(任意长度)、VARCHAR(1-65535) 日期类型： 类型转换 String 可隐式转换 整形科转换为更广的类型，TINYINT → INT, INT → BIGINT 整数类型、FLOAT、STRING 类型可隐式转换成 DOUBLE cast 进行强制类型转换，失败返回空 1SELECT CAST(&#x27;11111&#x27; AS INT); 集合类型四种集合类型 array: 有序的相同类型集合 map: key 为基本类型 struct: 不同类型字段的集合 union: 不同类型元素存储在统一字段的不同行中 1SELECT NAMED_STRUCT(&quot;name&quot;, &quot;usernamexx&quot;, &quot;id&quot;, 7, &quot;salary&quot;, &quot;111111.23&quot;); 使用案例 原始的 JSON 数据 123456789101112&#123; &quot;name&quot;: &quot;songsong&quot;, &quot;friends&quot;: [&quot;bingbing&quot; , &quot;lili&quot;] , //列表Array, &quot;children&quot;: &#123; //键值Map, &quot;xiao song&quot;: 18 , &quot;xiaoxiao song&quot;: 19 &#125; &quot;address&quot;: &#123; //结构Struct, &quot;street&quot;: &quot;hui long guan&quot; , &quot;city&quot;: &quot;beijing&quot; &#125;&#125; 格式好的行数据 12345678910111213141516songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijingyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing-- 表创建create table test( name string, friends array&lt;string&gt;, children map&lt;string, int&gt;, address struct&lt;street:string, city:string&gt;)row format delimited fields terminated by &#x27;,&#x27;collection items terminated by &#x27;_&#x27;map keys terminated by &#x27;:&#x27;lines terminated by &#x27;\\\\\\\\n&#x27;;-- 测试结构查询SELECT friends[1], children[&#x27;xiao song&#x27;], address.city FROM test WHERE name = &#x27;songsong&#x27;; 表类型 外部表： 指定 external 关键字，元数据 + 数据分开管理，删除表定义，数据不会删除 内部表： 删除表的时候数据会删除 分桶表： 实现 DML 事务时必须 分区表： 123456789CREATE EXTERNAL TABLE emp_bucket ( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIM) Q: 为何分区？ 可避免全表扫描，提高查询效率，通常根据事件、地区等信息进行分区 Q: 为何分桶？ 分区或表数据量过大，分桶降数据划分成更细粒度。 通过 分桶字段.hashCode % 分桶个数。 不能使用 load data local inpath 方式加载数据 在数据仓库中 ODS 外部表，从外部进来 DW 内部表 ADS 内部表 计算过程中使用到的临时表，数据随用随删，使用内部表。 文本文件编码支持自定义文件存储格式 默认的文件分割 行与行 \\\\n 字段之间 ^A 元素之间 ^B k-v 之间： ^C 12# 显示 ^A/^B 特殊的控制字符cat -A emp 读时模式写时模式 -&gt; 写数据检查 -&gt; RDBMS 读时模式 -&gt; 读时检查 -&gt; Hive 写时模式： 在加载时发现数据不符合表的定义，则拒绝加载数据。数据在写入 数据库时对照表模式进行检。 读时模式： 加载数据时不进行数据格 式的校验，读取数据时如果不合法则显示NULL。 元数据管理 通常是独立的 RDBMS 元数据信息包括： 存在的表、表的列、权限 … Thrift 服务开启 hiveserver2，搭配 groovy / maven 使用 默认情况下，管理表在 /usr/hive/warehouse 目录下 hive.start.cleanup.scratchdir 默认为 false 每次重启 Hiveserver 时清理掉历史目录 HiveServer2 管理元数据，生产环境中常使用。 beelinebeeline 可以连接 Hive， MySQL… HCatalogHive 的元数据服务 统一的元数据服务 可不启动 MR 任务执行 主要是 DDL 对元数据的操作 hcat -e “create database tt2”; hcat -f xxx.hql 数据存储格式TEXTFILE（默认格式） 、 SEQUENCEFILE、 RCFILE、 ORCFILE、 PARQUET。 TEXTFILE、SEQUENCEFILE 的存储格式是基于行存储的； ORC和PARQUET 是基于列式存储的。 TEXTFILE 通常先导入到 textfile，之后执行 insert … select 到其他格式的表中 行和列的存储 行存储： insert 与 update 比较容易 select 需要查询大多无用的数据 textfile,sequencefile 行 rcrile, orc, parout 列存储 sequencefile: 可分割 可压缩 record, none, block 压缩 … RCFile: 列式记录文件，结合列和行存储的优势 先按水平划分，后让垂直划分 使用列唯独的压缩，有效提升存储空间利用率 textfile默认格式，数据不压缩 12345678910111213CREATE TABLE student (name STRING,age INT,cource ARRAY&lt;STRING&gt;,body MAP&lt;STRING, FLOAT&gt;,address STRUCT&lt;STREET:STRING, CITY:STRING, STATE:STRING&gt;)ROW FORMAT DELIMITEDFIELDS TERMINATED BY &#x27;\\\\001&#x27;COLLECTION ITEMS TERMINATED BY &#x27;002&#x27;MAP KEYS TERMINATED BY &#x27;\\\\003&#x27;LINES TERMINATED BY &#x27;\\\\n&#x27;STORED AS TEXTFILE; ORCFile 表位 ORC 可支持事务操作 组成 文件脚注(file footer)： postscript： stripe: 条带 ，默认 250M Index Data: 1W行一个, 条带统计信息, 数据在条带中的位置 Row data: 水平 –&gt; 垂直, 列为单位存储数据 Stripe Footer: stripe 元数据信息 三个级别的索引： 文件级别、条带级、行组级 无需指定分割符，自动处理 Parquet apache 顶级项目site 由 Twitter 和 Cloudera 合作开发 支持使用重复级别/定义级别的方法来对数据结构进行编码 通用型强 与语言和平台无关 二进制存储的 文件中包含数据和元数据 Row group: 文件有多个行组组成，写入数据最大的缓存单元，50M ~ 1GB 之间 Column Chunk: 存储当前行组内的某一行数据 最小的 I/O 并发单元 Page: 压缩读取数据的最小单元 8K ~ 1M 之间，越大压缩率越高 Footer: 数据的 schema 信息 每个行组的元数据信息： 每个 column chunk 的元数据信息： 比较压缩比 ORC &gt; Parquet &gt; text 执行查询 ORC 与 Parquet 相当 TextFile文件更多的是作为跳板来使用(即方便将数据转为其他格式) 有update、delete和事务性操作的需求，通常选择ORCFile 没有事务性要求，希望支持 Impala、Spark，多种计算框架/查询引擎，建议选择 Parquet 其他Hive 与 RDBMS 的对比Hive 是基于 Hadoop 的数据仓库分析工具， 可以将 SQL 转换成 MR 任务运行。 查询语言类似， HQL, 与 SQL 高度类似，实现了 SQL-92 的标准。 数据规模，Hive 处理海量数据，RDBMS 只能处理有限的数据集 执行引擎，Hive 的执行引擎可以是 MR / Tex / Spark / Flink, RDBMS使用自己的执行引擎 数据存储，Hive 的数据一般存储在 HDFS 上，RDBMS 通常存储在贝蒂文件系统或者裸设备 执行速度，Hive 执行速度相对比较慢(MR/数据量)，RDBMS 相对快，Hive 没有索引，默认使用 MapReduce 作为执行引擎，会产生较高的延迟。RDBMS 一般会定义索引，执行延迟较低。 可扩展性，Hive 方便进行水平扩展，通常 RDBMS 对水平扩展支持不友好。Hive 建立在 Hadoop 上，可扩展性与 Hadoop 的可扩展性是一致的，RDBMS 优于 ACID 语义的严格限制，扩展行有限。 数据更新，Hive 对数据更新不友好，RDBMS 支持频繁快速的数据更新 Hive 与 RDBMS 对比 Property Hive RMDBMS 查询语言 HQL SQL 数据存储位置 HDFS Raw Device / 本地文件系统 数据格式 用户定义 系统决定 数据更新 不支持 支持 索引 无 有 执行引擎 MapReduce Executor 执行延迟 高 低 可扩展性 高 低 数据规模 大 小 子查询 只可 from 子句中 完全支持 HiveQL 于 SQL 的比较 比较项 SQL HiveQL ANSI SQL 更新 支持 UPDATEVINSERT\\DELETE 。。。 不完全支持 insert OVERWRITEVINTO TABLE 事务 支持 可支持(分桶、ORCFile 文件格式) 模式 写模式 读模式 数据保存 块设备、本地文件系统 HDFS 延时 低 高 多表播入 不支持 支持 子查询 完全支持 只能用在 From 子句中 视图 Updatable Read-only 可扩展性 低 高 数据规模 小 大 Ref Github-Hive LanguageManual","tags":[{"name":"Hive","slug":"Hive","permalink":"http://example.com/tags/Hive/"}]},{"title":"Hive-优化","date":"2021-03-09T13:33:13.000Z","path":"2021/03/09/Hive-优化/","text":"[TOC] 影响 Hive 效率的几乎从不是数据量过大，而是数据倾斜、数据冗余、Job / IO过多、MapReduce 分配不合理等。 架构优化执行引擎选择通过参数 hive.execution.engine 控制，可选 MapReduce, Tez, Spark, Flink 作为执行引擎，在离线数仓中，批处理方面主要使用 Spark 优化器的使用在真正执行计算之前，生成和优化逻辑执行计划与物理执行计划。 Vectorize 矢量化查询执行 Cost-Based Optimization: CBO 成本优化器 矢量化查询执行 要求执行引擎为Tez, 执行通过一次批量执行1024行而不是每行一行来提高扫描，聚合，过滤器和连接等操作的性能, 可一显着缩短查询执行时间。 需要使用 orc 格式存储数据 123-- 开启 set hive.vectorized.execution.enabled = true;set hive.vectorized.execution.reduce.enabled = true; 成本优化器 基于apache Calcite的，Hive的CBO通过查询成本(有analyze收集的统计信息)会生成有效率的执行计划，最终会减少执行的时间和资源的利用 可定期执行表的分析，分析后数据存放在元数据库中 123456789-- 从 v0.14.0默认SET hive.cbo.enable=true; true-- 默认falseSET hive.compute.query.using.stats=true; -- 默认falseSET hive.stats.fetch.column.stats=true; -- 默认trueSET hive.stats.fetch.partition.stats=true; 文件格式Parquet 和 ORC 都是 Apache 旗下的开源列式存储格式。列式存储比起传统的行式存 储更适合批量 OLAP 查询，并且也支持更好的压缩和编码。 选择 Parquet 的原因主要是它支持 Impala 查询引擎，并且对 update、delete 和事务性操作需求很低。 选择 ORCFile 支持事务操作。 数据压缩压缩的配置可以在hive的命令行中或者hive-site.xml文件中进行配置。 1SET hive.exec.compress.intermediate=true DEFLATE GZIP: 扩展名 .gz Bzip2: 支持分割, 扩展名 .gz LZO： LZ4： Snappy: 不支持分割 可在mapred-site.xml, hive-site.xml 配置，命令行配置 12345SET hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;SET hive.exec.compress.output=true;SET mapreduce.output.fileoutputformat.compress.codec =org.apache.hadoop.io.compress.SnappyCodc 分区、分桶表设计成分区表可以提升查询的性能，对于一个特定分区的查询，只会加载对应分区路 径的文件数据 尽量避免层级较深的分区 日期或时间。如year、month、day或者hour， 地理位置。如国家、省份、城市 业务逻辑。如部门、销售区域、客户 分桶表 分桶表的组织方式是将HDFS上的文件分割成多个文件。 分桶可以加快数据采样，也可以提升join的性能(join的字段是分桶字段)，因为分桶可 以确保某个key对应的数据在一个特定的桶内(文件)，巧妙地选择分桶字段可以大幅 度提升join的性能。 分桶字段可以选择经常用在过滤操作或者join操作的字段。 参数优化本地模式支持将作业动态地转为本地模式, 当Hive处理的数据量较小时，启动分布式去处理数据会有点浪费。 一个作业只要满足下面的条件，会启用本地模式 输入文件的大小小于 hive.exec.mode.local.auto.inputbytes.max 配置的大小 map 任务的数量小于 hive.exec.mode.local.auto.input.files.max 配置的大小 reduce 任务的数量是1或者0 123SET hive.exec.mode.local.auto=true; -- 默认 falseSET hive.exec.mode.local.auto.inputbytes.max=50000000;SET hive.exec.mode.local.auto.input.files.max=5; -- 默认 4 严格模式强制不允许用户执行3种有风险的HiveQL语句，一旦执行会直接失败。 查询分区表时不限定分区列的语句； 两表 join 产生了笛卡尔积的语句； 用 order by 来排序，但没有指定 limit 的语句。 12-- DEFAULT strictset hive.mapred.mode=nostrict JVM 重用Hadoop可以重用 JVM，通过共享 JVM 以串行而非并行的方式运行 map 或者 reduce。 避免 JVM 启动进程所耗费的时间会比作业执行的时间还要长。 JVM的重用适用于同一个作业的 map 和 reduce，对于不同作业的 task 不能够共享 JVM。 开启JVM重用将一直占用使用到的 task 插槽，以便进行重用，直到任务完成后才能释放。 如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。 12-- 代表同一个MR job中顺序执行的5个task重复使用一个JVM，减少启动和关闭的开销SET mapreduce.job.jvm.numtasks=5; 并行执行Hive 将查询转换成一个或多个阶段，MapReduce 阶段、抽样阶段、合并阶段、Limit 阶段.. 默认情况下一次只执行一个阶段，对于特定 Job 有多个阶段，阶段间非完全相互依赖，并行执行，可以缩短 job 的执行时间。 1234-- 默认falseSET hive.exec.parallel=true; -- 默认8SET hive.exec.parallel.thread.number=16; 推测执行在分布式集群环境下，因为程序Bug、负载不均衡、资源分布不均、网络情况等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务, 这些任务会拖慢作业的整体执行进度。 Hadoop采用了推测执行机制，它根据一定的规则推测出 “拖后腿” 的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。 123set mapreduce.map.speculative=trueset mapreduce.reduce.speculative=trueset hive.mapred.reduce.tasks.speculative.execution=true 合并小文件在map执行前合并小文件，减少map数 12-- 缺省参数set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat 在Map-Reduce的任务结束时合并小文件 1234567891011-- 在 map-only 任务结束时合并小文件，默认trueSET hive.merge.mapfiles = true;-- 在 map-reduce 任务结束时合并小文件，默认falseSET hive.merge.mapredfiles = true;-- 合并文件的大小，默认256MSET hive.merge.size.per.task = 268435456;-- 当输出文件的平均大小小于该值时，启动一个独立的 map-reduce 任务进行文件mergeSET hive.merge.smallfiles.avgsize = 16777216; Fetch 模式Fetch 模式是指 Hive 中对某些情况的查询可以不必使用 MapReduce 计算 在开启 fetch 模式之后，在全局查找、字段查找、limit 查找等都不启动 MapReduce 123-- Default Value: minimal in Hive 0.10.0 through 0.13.1, -- more in Hive 0.14.0 and laterhive.fetch.task.conversion=more SQL 优化列裁剪和分区裁剪列裁剪： SELECT 只查需要的列。少用 SELECT * 分区裁剪: 只读取需要的列。在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where 后面，那么就会先全表关联，之后再过滤。 1234# 列裁剪优化相关的配置项, default truehive.optimize.cp=true# 分区裁剪优化, default truehive.optimize.pruner=true 谓词下推在 RDBMS 中，如 MySQL 也有 Predicate Pushdown(PPD) 的概念，将 SQL 语句中的 where 谓词逻辑尽可能提前执行，减少下游处理的数据量。 如下的 SQQ， forum_topic 表过滤的 where 语句卸载子查询内部，而不是外部，Hive 谓词下推逻辑优化器是 PredicatePushDown，该优化器将 OperatorTree 中的 FilterOperator 向上提。 123456789select a.uid,a.event_type,b.topic_id,b.titlefrom calendar_record_log aleft outer join (select uid,topic_id,title from forum_topicwhere pt_date = 20190224 and length(content) &gt;= 100) b on a.uid = b.uidwhere a.pt_date = 20190224 and status = 0;# 谓词下推优化的配置项, default truehive.optimize.ppd = true sort by 代替 order by为了控制 map 端数据分配到 reducer 的 key, 需要配置 distribute by 一起使用，如果不加 distribute by 的话，map 端数据就会随机分配到 reducer。 123456-- 以UID为key，以上传时间倒序、记录类型倒序输出记录数据select uid,upload_time,event_type,record_datafrom calendar_record_logwhere pt_date &gt;= 20190201 and pt_date &lt;= 20190224distribute by uidsort by upload_time desc,event_type desc; group by 代替 count(distinct)去重计算数据量大时不好处理，数据量大的时候用一个 ReduceTask 来完成，导致整个 Job 很难完成 。 一般 COUNT DISTINCT 使用先 GROUP BY 再 COUNT 的方式替换。使用 group by 替换后， SQL 如下，会启动两个 MR Job，确保启动 Job 开销远小于计算耗时的时候使用。 12345select count(1) from ( select uid from calendar_record_log where pt_date &gt;= 20190101 group by uid) t; group by 配置调整并不是所有的聚合操作都需要在 Reduce 端完成，可现在 Map 端进行部分聚合，最后在 Reduce 端得出最终结果 hive.map.aggr=true: 是否在 Map 端进行聚合 hive.groupby.napaggr.checkinterva=10000: 在 Map 端进行聚合操作的条目数目 hive.groupby.skewindata=true: 有数据倾斜的时候进行负载均衡，默认 false 当选项设定为 true，生成的查询计划会有两个 MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。 join 基础优化map join 分桶 join 大表 join 大表处理空值或无意义值(1) 空 key 过滤 大表 Join 大表时，key 有大量的异常数据，相同的 key 发送到相同的 reducer 上，从而导致内存不够，结果 Join 的时候耗时长，可先通过 SQL 对其进行过滤。 (2) 空 key 转化 空 Key 转化，key 非异常数据，必须包含在 join 的结果中，可对为空的 key 设置随机值，使数据随机均匀分到不同的 reducer 上，防止数据的倾斜问题 … 单独处理倾斜 key调整 Map 数通常情况下，作业通过 Input 目录产生一个/多个 map 任务 input 文件总个数、文件大小，集群设置的文件快大小。 Q: 是不是map数越多越好？ 答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。 Q: 是不是保证每个map处理接近128m的文件块，就高枕无忧了？ 答案也是不一定。比如有一个127m的文件，正常会用一个 map 去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果 map 处理的逻辑比较复杂，用一个 map 任务去做，肯定也比较耗时。 增加 Map 的方法： 调整 maxSize 最大值，使 maxSize 小于 blockSize 增加 map 个数 // TODO maxSize 对应的配置参数… 1computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))&#x3D;blocksize&#x3D;128M 调整 Reduce 数过多的 reduce 问题 过多的 reduce， 会过多启动、初始化 reduce 消耗时间和资源 有多少个 reduce 就会有多少个输出文件，生成很多小文件，如果这些小文件作为下一个任务的输入，会出现小文件过多的问题 hive.exec.reducers.bytes.per.reducer=256000000: 每个 reduce 处理的数据量默认为 256MB hive.exec.reducers.max=1009: 每个任务最大的 reduce 数，默认 1009 mapreduce.job.reduces： 设置每个 job 的 reduce 个数 12-- 设置每个 job 的 reduce 个数set mapreduce.job.reduces=15 优化小结 Hadoop/Hive 处理数据过程，有几个显著特征： 不怕数据多，就怕数据倾斜 对 job 数比较多的作业运行效率相对比较低 对 sum、count 等聚合操作而言，不存在数据倾斜问题 count(distinct) 效率较低，数据量大容易出问题 优化可以从几个方面着手： 好的模型设计，事半功倍 解决数据倾斜问题。根据配置和业务进行处理 减少 job 数 设置合理的map、reduce task 数 对小文件进行合并，是行之有效的提高 Hive 效率的方法 优化把握整体，单一作业的优化不如整体最优 RefHive 自带的序列化与反序列化 https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide#DeveloperGuide-HiveSerDe Hive 参数说明的官方文档：https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties Hive SQL的编译过程 Hive/HiveSQL常用优化方法全面总结","tags":[{"name":"Hive","slug":"Hive","permalink":"http://example.com/tags/Hive/"}]},{"title":"Hive 中的函数","date":"2021-03-09T13:30:52.000Z","path":"2021/03/09/Hive-函数/","text":"[TOC] 函数分类 标准函数： 一列或多列作为参数传入，返回值是一个值的函数 如 to_date(string timestamp), sqrt(double a) 聚合函数： 接收0行、多行的列，返回单一的值 表生成函数： 接收 0 、 多个输入，产生多列 、 多行输出 如 explode 123show functions;desc function upper;desc function extended upper; 基础函数日期函数 current_date: current_timestamp： year(string date) month(string date) hour(string date) minute(string date) second(string date) from_unixtime(bigint unixtime[, string format])：转换从 1970-01-01 00:00:00 UTC 开始的秒为日期 datediff(string enddate, string startdate): 计算时间差 day(string date) / dayofmonth(date): 查询当月第几天 weekofyear(string date): 如weekofyear(“1970-11-01 00:00:00”) = 44, weekofyear(“1970-11-01”) = 44. last_day(string date)：查询月末的最后一天 date_add(date/timestamp/string startdate, tinyint/smallint/int days)： 增加指定多少天 date_sub(date/timestamp/string startdate, tinyint/smallint/int days)： 减少指定多少天 date_sub(current_date, dayofmonth(current_date)-1)：当月第一天 add_months(string start_date, int num_months, output_date_format)， 如 add_months(date_sub(current_date, dayofmonth(current_date)-1), 1)： 下月第一天 to_date(string timestamp)： 字符串转日期，2.1.0- 返回 String 类型，2.1.0+ 返回 date 类型 date_format(date/timestamp/string ts, string fmt)： 日期、时间戳、字符串类型格式化输出标准时间格式 next_day(string start_date, string day_of_week)： 返回第一个日期，该日期晚于start_date并命名为day_of_week。day_of_week 为两个或三个字符，如 “Mo”, “tue”, “FRIDAY”，next_day(‘2015-01-14’, ‘TU’) = 2015-01-20 1234567891011121314151617181920212223242526SELECT current_date;SELECT CURRENT_TIMESTAMP();-- unix 时间戳转换成日期SELECT FROM_UNIXTIME(11111111);SELECT FROM_UNIXTIME(11111, &#x27;yyyyMMdd&#x27;);SELECT FROM_UNIXTIME(1111111, &#x27;yyyy-MM-dd HH:mm:ss&#x27;);-- 日期转成时间戳SELECT UNIX_TIMESTAMP(&#x27;2019-09-15 14:23:00&#x27;);-- 时间差, 返回日期相差的天数SELECT datediff(&#x27;2020-04-18&#x27;, &#x27;2019-11-21&#x27;);SELECT abs(datediff(&#x27;2020-04-18&#x27;, &#x27;2019-11-21&#x27;));-- 日期为所处月的第几天SELECT dayofmonth(current_date);-- 日期所处月的最后一天日期SELECT last_day(current_date);-- 当月第一天SELECT DATE_SUB(current_date, dayofmonth(current_date) - 1);-- 下月第一天SELECT add_months(date_sub(current_date, dayofmonth(current_date) - 1), 1);-- must 字符串转换成 date yyyy-MM-ddSELECT to_date(&#x27;2020-01-01&#x27;);SELECT to_date(&#x27;2020-01-01 12:12:12&#x27;);-- 日期格式化成指定的字符串SELECT date_format(current_timestamp(), &#x27;yyyy-MM-dd HH:mm:ss&#x27;);SELECT date_format(current_date(), &#x27;yyyyMMdd&#x27;);SELECT date_format(&#x27;2020-06-01&#x27;, &#x27;yyyy-MM-dd HH:mm:ss&#x27;); 常用的日期处理 1234567-- 近3天的-- 近一周的where dt &gt;= date_add(next_day(&#x27;$do_date&#x27;, &#x27;mo&#x27;), -7) and dt &lt;= &#x27;$do_date&#x27;-- 近一月的where dt &gt;= date_format(&#x27;$do_date&#x27;, &#x27;yyyy-MM-01&#x27;) and dt &lt;= &#x27;$do_date&#x27; 条件函数 IF .. else case when.. end &lt;column-name&gt;: 多个条件的时候使用 nvl(T value, T default_value): value 为空的时候返回默认值 COALESCE(T v1, T v2, ...): 返回参数中第一个非空值 nullif(x, y): 相等为空，否则为x isnull / isnull( a ): assert_true(boolean condition): 不满足抛出异常 12345678910111213141516171819202122232425262728293031323334353637383940414243444546-- 测试表定义create table if not exists emp( empno int comment &#x27;员工号&#x27;, ename string comment &#x27;员工姓名&#x27;, job string comment &#x27;工作名称&#x27;, mgr int comment &#x27;&#x27;, hiredate date comment &#x27;雇佣日期&#x27;, sal int comment &#x27;薪水&#x27;, comm int, deptno int comment &#x27;部门号&#x27;) row format delimited fields terminated by &quot;,&quot;;select * from emp;-- if (boolean testCondition, T valueTrue, T valueFalseOrNull)-- 将 emp 表的员工工资等级分类：0-1500、1500-3000、3000以上SELECT sal, if(sal &lt;= 1500, &#x27;primary&#x27;, if(sal &lt;= 3000, &#x27;middle&#x27;, &#x27;advanced&#x27;))FROM emp;-- case when 判断SELECT ename, deptno, CASE WHEN deptno = 10 THEN &#x27;accounting&#x27; WHEN deptno = 20 THEN &#x27;research&#x27; WHEN deptno = 30 THEN &#x27;sales&#x27; ELSE &#x27;unknown&#x27; END deptnameFROM emp;-- 返回参数中的第一个非空值；如果所有值都为 NULL，那么返回NULLselect sal, coalesce(comm, 0)from emp;-- null 判断select *from empwhere isnotnull(comm);-- 空值转换函数 nvl(T value, T default_value)select empno, ename, job, mgr, hiredate, deptno, sal + nvl(comm, 0) sumsalfrom emp;-- nullif(x, y) 相等为空，否则为x SELECT nullif(&quot;b&quot;, &quot;b&quot;), nullif(&quot;b&quot;, &quot;a&quot;); 字符串函数 lower： length: concat / || ：字符拼接 concat_ws(separator, [string | array(string)]+): 可指定分隔符进行拼接 substr： 求子串, 指定开始和结束索引 split： . 需要进行正则转义 instr(string str, string substr)： 返回substr在str中第一次出现的位置 parse_url(string urlString, string partToExtract [, string keyToExtract])： 从 url 中抽取值，可抽取的值包括 HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO. 如 parse_url(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;, ‘HOST’) returns ‘facebook.com‘。 regexp_extract(string subject, string pattern, int index)： 按照正则匹配值 如 regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 2)， Index 参数为 Java 的 group regexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT):正则替换 如 regexp_replace(“foobar”, “oo|ar”, “”) returns ‘fb.’ get_json_object(string json_string, string path)： 从 JSON 字符串中获取特定路径值 JSON 路径只能为 [0-9a-z_], 没有大写或特殊字符, keys 不可以以 数字开头 str_to_map(text[, delimiter1, delimiter2])： 返回 map 123456789-- 子串截取SELECT split(&quot;www.lagou.com&quot;, &quot;\\\\\\\\.&quot;);select empno || &quot; &quot; || ename idname-- 字符串拼接 指定分割符拼接 concat_ws(separator, [string | array(string)]+)SELECT concat_ws(&#x27;.&#x27;, &#x27;www&#x27;, array(&#x27;lagou&#x27;, &#x27;com&#x27;));SELECT substr(&#x27;www.lagou.com&#x27;, 5);SELECT substr(&#x27;www.lagou.com&#x27;, -5);SELECT substr(&#x27;www.lagou.com&#x27;, 5, 5); get_json_object 获取 JSON 字符串信息 1234567891011121314151617181920212223242526272829&#123; &quot;store&quot;: &#123; &quot;fruit&quot;:[ &#123; &quot;weight&quot;: 8, &quot;type&quot;: &quot;apple&quot; &#125;, &#123; &quot;weight&quot;: 9, &quot;type&quot;: &quot;pear&quot; &#125; ], &quot;bicycle&quot;: &#123; &quot;price&quot;: 19.95, &quot;color&quot;: &quot;red&quot; &#125; &#125;, &quot;email&quot;: &quot;amy@only_for_json_udf_test.net&quot;, &quot;owner&quot;: &quot;amy&quot;&#125;-- 获取值SELECT get_json_object(src_json.json, &#x27;$.owner&#x27;) FROM src_json;amy-- 获取数组值SELECT get_json_object(src_json.json, &#x27;$.store.fruit\\\\[0]&#x27;) FROM src_json;&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;-- 获取不存在的值SELECT get_json_object(src_json.json, &#x27;$.non_exist_key&#x27;) FROM src_json;NULL 数学函数 round: round x to d decimal places，可用于控制小数保留几位 ceil: 向上取整 floor: 向下取整。 exp(DOUBLE a), exp(DECIMAL a): 指数函数 log10(DOUBLE a), log10(DECIMAL a) log(DOUBLE base, DOUBLE a) abs(DOUBLE a) 1234select round(314.15926, 2);select round(314.15926, -2);select ceil(3.1415926);select floor(3.1415926); 集合函数 size(Map&lt;K.V&gt;) / size(Array&lt;T&gt;)： 返回元素个数 map_keys(Map&lt;K.V&gt;) / map_values(Map&lt;K.V&gt;): 将 Map 所有的 key、value 进行返回 array_contains(Array&lt;T&gt;, value): 数组中是否包含某个值 sort_array(Array&lt;T&gt;)： 对数组元素进行排序 类型转换函数 binary(string|binary)： 转换成二进制 cast(expr as )： 类型转换，无法转换时，返回 NULL 1SELECT CAST(&#x27;23232&#x27; AS INT); UDTF 聚集函数(UDAF) count / sum / avg / min / max collect_set(col) collect_list(col) ntile(INTEGER x): 将有序分区划分为x个称为存储桶的组，并为该分区中的每一行分配存储桶编号。 表生成函数(UDTF) 一行输入，多行输出 配合 lateral view 进行使用，解决 UDTF 不能添加额外列的问题。 explode(ARRAY a) explode(MAP&lt;Tkey,Tvalue&gt; m) posexplode (array)： 带有原始位置的炸裂函数 json_tuple(string jsonStr,string k1,…,string kn) parse_url_tuple(string urlStr,string p1,…,string pn) 1234567891011121314151617181920-- lateral view 常与 表生成函数explode结合使用，处理上述问题-- ==&gt; 解决 UDTF 不能添加额外列的问题-- lateral view udtf(expression) tableALias as ...with t1 as ( select &#x27;OK&#x27; cola, split(&#x27;www.lagou.com&#x27;, &#x27;\\\\\\\\.&#x27;) colb)select cola, colcfrom t1 lateral view explode(colb) t2 as colc;lateral view udtf(expression) tableALias as ...-- 炸裂 map 并给定别名select explode(map(&#x27;A&#x27;,10,&#x27;B&#x27;,20,&#x27;C&#x27;,30)) as (key,value);key valueA 10B 20C 30select posexplode(array(&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;)) as (pos,val);pos val2 C1 B0 A json_tuple 获取 JSON 多个值拆开 12select a.timestamp, b.*from log a lateral view json_tuple(a.appevent, &#x27;eventid&#x27;, &#x27;eventname&#x27;) b as f1, f2; parse_url_tuple 获取 URL 中的多个信息 12SELECT b.*FROM src LATERAL VIEW parse_url_tuple(fullurl, &#x27;HOST&#x27;, &#x27;PATH&#x27;, &#x27;QUERY&#x27;, &#x27;QUERY:id&#x27;) b as host, path, query, query_id LIMIT 1; 其他脱敏函数： 对姓名、电话号码进行脱敏，不显示全部内容 mask(string str[, string upper[, string lower[, string number]]]): 如 mask(“abcd-EFGH-8765-4321”, “U”, “l”, “#”) results in llll-UUUU-####-####. collect_list： 列出该字段的所有值，不去重 current_user()： 当前用户 md5(string/binary) version()† 窗口与分析函数 和聚合函数的不同之处是：对于每个组返回多行，而聚合函数对于每个组只返回一行。数据窗口大小可能会随着行的变化而变化。 窗口函数 使用窗口函数之前一般要要通过 over() 进行开窗 窗口函数是针对每一行数据的，如果 over 中没有参数，默认是全部的结果集 partition by 子句 在 over 窗口中进行分区，对某一列进行分区统计，窗口的大小就是分区的大小 order by 子句 对输入的数据进行排序，有 order by 缺少 window 子句，默认窗口为 range between unbounded preceding and current row over 子句 后面可指定标准的聚集函数， count, sum, min, max, avg 使用 PARTITION BY 语句，具有任何原始数据类型的一个或多个分区列。 带有 PARTITION BY 和 ORDER BY 以及任何数据类型的一个或多个分区和/或排序列， 带有窗口规格，Windows 可以在 WINDOW 子句中单独定义。 Window 子句 窗口规范支持以下格式： 指定 order by 并缺少 window 子句时，window 默认被指定为 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW 当同时缺少 order by 和 window 子句时，默认窗口指定为 ROW BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING 不支持 Rank 函数， rank、ntile、denseRank、cusmeDis、percentRank，不支持 Lead 和 Lag 函数 123(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING 窗口范围 unbounded preceding: 组内第一行数据 n preceding: 组内当前行的前n行数据 current row: 当前行数据 n following: 组内当前行的后 n 行数据 unbounded following: 组内最后一行数据 123456789101112131415161718192021222324252627282930-- 查询员工姓名、薪水、部门薪水总和、占部门薪水的百分比select ename, sal, deptno, sum(sal) over (partition by deptno) depsalsum, round(sal / sum(sal) over(partition by deptno) * 100, 2) || &#x27;%&#x27; salofdeptsumpercentfrom emp;select ename, sal, deptno, sum(sal) over (partition by deptno order by ename)from emp;-- 等价。组内，第一行 ~ 当前行的和select ename, sal, deptno, sum(sal) over (partition by deptno order by ename rows between unbounded preceding and current row )from emp;-- 组内，第一行 ~ 最后一行的和select ename, sal, deptno, sum(sal) over (partition by deptno order by ename rows between unbounded preceding and unbounded following )from emp;-- 组内，按照分区前后两行和当前行的总和，前一行 + 当前行 + 后一行select ename, sal, deptno, sum(sal) over (partition by deptno order by ename rows between 1 preceding and 1 following )from emp; 排名函数 row_number(): 排名顺序增加不会重复 RANK(): 排名相等会在名次中留下空位；如1、2、2、4、5、… … DENSE_RANK(): 排名相等会在名次中不会留下空位 ；如1、2、2、3、4、… … 1234567891011121314151617CREATE TABLE IF NOT EXISTS t2( cname string comment &#x27;课程名&#x27;, sname string COMMENT &#x27;学生名&#x27;, score int COMMENT &#x27;课程分数&#x27;) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27; &#x27;;-- 求每个班级前3名的学员 with tmp as ( SELECT cname, sname, score, dense_rank() over (partition by cname order by score) rank FROM t2)select cname, sname, score, rankfrom tmpwhere rank &lt;= 3; 序列函数 lag: 返回当前数据行的上一行数据 常用 lead: 返回当前数据行的下一行数据 常用 first_value: 取分组内排序后，截止到当前行，第一个值 last_value: 分组内排序后，截止到当前行，最后一个值 ntile: 将分组的数据按照顺序切分成n片，返回当前切片值 12345678910111213141516171819202122232425262728create table userpv( cid string comment &#x27;&#x27;, ctime date comment &#x27;时间&#x27;, pv int COMMENT &#x27;页面访问次数&#x27;) row format delimited fields terminated by &#x27;,&#x27;;-- 上两行数据， 下三行数据select cid, ctime, pv, lag(pv, 2) over (partition by cid order by ctime) lagpv, lead(pv, 3) over (partition by cid order by ctime) leadpvfrom userpv;-- first_value 分组排序截止到当前行第一个值 / last_value 分组排序后截止到当前行最后一个值select cid, ctime, pv, first_value(pv) over (partition by cid order by ctime rows between unbounded preceding and unbounded following) firstpv, last_value(pv) over (partition by cid order by ctime rows between unbounded preceding and unbounded following) lastpvfrom userpv;-- ntile 按照cid进行分组并按照 ctime 排序，将分组内的数据平均分成 2 份select cid, ctime, pv, ntile(2) over (partition by cid order by ctime) ntilefrom userpv; UDF临时性添加函数 1234-- hive add jaradd jar /home/hadoop/udf.jarcreate temporary function myconcat as &#x27;con.janhen.hive.udaf.ConcatUDAF&#x27;;SHOW FUNCTIONS; 永久添加到 Hive 中 1234567# jar ==&gt; hdfs hdfs dfs -put hiveudf.jar /user/hadoop/jar/-- 加载函数create function mynvl2 as &#x27;com.janhen.bigdata.hive.nvl&#x27; using jar &#x27;hdfs:/user/hadoop/jar/hiveudf.jar&#x27;;show functions;drop function mynvl2; Ref Built-in Functions Windowing and Analytics Functions","tags":[{"name":"Hive","slug":"Hive","permalink":"http://example.com/tags/Hive/"}]},{"title":"Hive-HQL","date":"2021-03-09T13:28:56.000Z","path":"2021/03/09/Hive-HQL/","text":"[TOC] 并不是所有的 HQL 都会被 Hive 转换成 MR 作业执行 HQL 是一种 SQL 方案，支持绝大部分的 SQL-92 标准 不支持行级别草俎哦、不支持事务 对于简单不需要聚合的操作，如 SELECT .. FROM xx LIMIT n，直接通过 FetchTask 获取数据 DML数据导入装载数据 LOCAL： LOAD DATA LOCAL …：从本地文件系统加载数据到Hive表中 LOAD DATA …：从HDFS加载数据到Hive表中 INPATH：加载数据的路径 OVERWRITE：覆盖表中已有数据；否则表示追加数据 PARTITION：将数据加载到指定的分区 一旦该表存在分区，那么在数据在加载时必须加载进入指定分区中，如下： 12345678-- 加载数据并指定分区，HDFS文件，已经被转移LOAD DATA INPATH &#x27;/user/hadoop/data&#x27; INTO student_info PARTITION(province=&#x27;sichuan&#x27;, city=&#x27;chengdu&#x27;);-- 加载数据覆盖表中已有数据LOAD DATA INPATH &#x27;data/sourceA.txt&#x27; OVERWRITE INTO TABLE tabA;-- 加载数据，覆盖表的数据，到指定的分区LOAD DATA INPATH &#x27;/user/hadoop/o&#x27; OVERWRITE INTO TABLE test3 PARTITION (part = &quot;a&quot;);-- 更改表的存储位置ALTER TABLE test ADD PARTITION (x = x1, y = y2) SET LOCATION &#x27;/user/test/x1/y1&#x27;; 插入数据 1234-- 插入数据insert into table tabC partition(month=&#x27;202001&#x27;) values (5, &#x27;wangwu&#x27;, &#x27;BJ&#x27;), (4, &#x27;lishi&#x27;, &#x27;SH&#x27;), (3, &#x27;zhangsan&#x27;, &#x27;TJ&#x27;);-- 插入查询的结果数据 insert into table tabC partition(month=&#x27;202002&#x27;) select id, name, area from tabC where month=&#x27;202001&#x27;; 创建表并插入数据 1create table if not exists tabD as select * from tabC; 多表（多分区）插入模式 一次查询，产生多个不相交的输出 Hive还有一个很有用的特性，可以通过一次查询，产生多个不相交的输出。 这样只通过对source表的一次查询，就将符合条件的数据插入test表的各个分区，非常方便 12345678-- 多个查询差生多个不相交的输出FROM sourceINSERT OVERWRITE TABLE test PARTITION (part = &#x27;a&#x27;)SELECT id, name WHERE id &gt;= 0 AND id &lt; 100INSERT OVERWRITE TABLE test PARTITION (part = &#x27;b&#x27;)SELECT id, name WHERE id &gt;= 100 AND id &lt; 200INSERT OVERWRITE TABLE test PARTITION (part = &#x27;c&#x27;)SELECT id, name WHERE id &gt;= 200 AND id &lt; 300 import 导入数据 1import table student2 partition(month=&#x27;201709&#x27;) from &#x27;/user/hive/warehouse/export/student&#x27;; 数据导出 将结果导出到本地 将查询结果格式化到本地 将结果导出到 HDFS 通过 DataX、Sqoop 等工具将结果导出到 HBase、MySQL等其他地方 修改表1234567-- 修改列alter table course_common1 change column id cid int;-- 增加字段alter table course_common1 add columns (common string);-- 删除字段：replace columns-- 在元数据中删除了字段，并没有改动hdfs上的数据文件alter table course_common1 replace columns( id string, cname string, score int); DDLDB 12345678910dfs -ls /user/hive/warehouse;create database if not exit comment &#x27;test comment&#x27; location &#x27;/usr/hive/mydb2.dbe&#x27;;desc database extended mydb2;-- 连同表一起删除DROP DATABASE IF EXISTS test CASCADE;DESC EXTENDED student;DESC FORMATTED student;-- 根据其他表创建新的表CREATE TABLE IF NOT EXISTS test.student2 LIKE test.student; Table CREATE TABLE [IF NOT EXISTS]：创建表 EXTERNAL： 外部表创建，生产中一般创建的都是外部表，删除表不删除数据 comment： 表注释 partition by： 对表中数据进行分区 clustered by： 建立分桶表 sorted by： 对表中的一个或多个字段进行排序，较少使用 存储子句: 可指定 SerDe, 默认没有使用 ROW FORMAT 或者 ROW FORMAT DELIMITED，会默认使用 SerDe。建表时需要为表指定列在指定列的同 时也会指定自定义的 SerDe。hive使用 Serde 进行行对象的序列与反序列化。 1234567ROW FORMAT DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] stored as SEQUENCEFILE|TEXTFILE|RCFILE LOCATION： 表在 HDFS 上的位置 TBLPROPERTIES：定义表的属性 AS： 接查询语句，根据查询结果建表 LIKE： 复制现有的表结构，不会复制数据 内外部表表的类型有两种，分别是内部表(管理表)、外部 表。 默认情况下，创建内部表 删除内部表，表的元数据和数据一起删除 删除外部表，删除表的定义，数据保留 生产环境中，多使用外部表 外部表不能执行 TRUNCATE 表类型转换 1234-- 内部表转外部表ALTER TABLE t1 SET tblproperties(&#x27;EXTERNAL&#x27;=&#x27;TRUE&#x27;);-- 外部表转内部表。EXTERNAL 大写，false 不区分大小alter table t1 set tblproperties(&#x27;EXTERNAL&#x27;=&#x27;FALSE&#x27;); 分区表 按照分区字段将表中的数据放置在不同的目录中，提高SQL查询的 性能 Hive没有索引，分区的作用和索引非常类似，可将其看做一种简易索引。对于直接命中分区的查询，Hive不会执行MapReduce作业。 分区字段不是表中已经存在的数据，可以将分区字段看成伪列。 分区查看 12-- 查看分区SHOW PARTITIONS student_info; 新增分区，加载数据 123456789-- 增加分区，不加载数据alter table t3 add partition(dt=&#x27;2020-06-03&#x27;);-- 增加多个分区alter table t3 add partition(dt=&#x27;2020-06-05&#x27;) partition(dt=&#x27;2020-06-06&#x27;);-- 增加分区，加载数据alter table t3 add partition(dt=&#x27;2020-06-07&#x27;) location &#x27;/user/hive/warehouse/mydb.db/t3/dt=2020-06-07&#x27; partition(dt=&#x27;2020-06-08&#x27;) location &#x27;/user/hive/warehouse/mydb.db/t3/dt=2020-06-08&#x27;;-- 单独为外部表的分区键指定值和存储位置：ALTER TABLE student _info ADD PARTITION (province = sichuan, city = chengdu) LOCATION &#x27;hdfs://master:9000/student/sichuan/chengdu&#x27;; 修改分区的hdfs路径 1alter table t3 partition(dt=&#x27;2020-06-01&#x27;) set location &#x27;/user/hive/warehouse/t3/dt=2020-06-03&#x27;; 删除分区 1alter table t3 drop partition(dt=&#x27;2020-06-03&#x27;), partition(dt=&#x27;2020-06-04&#x27;); 动态分区 Hive 会根据 SELECT 语句中的最后一个查询字段作为动态分区的依据，而不是根据字段名来选择。如果指定了 n 个动态分区的字段，Hive 会将 select 语句中最后 n 个字段作为动态分区的依据。 Hive 默认没有开启动态分区，在执行这条语句前，必须对Hive进行一些参数设置： 123-- 开启自动分区set hive.exec.dynamic.partition = true;INSERT OVERWRITE TABLE test PARTITION(time) SELECT id, modify_time FROM source; 分桶表 分区不能更细粒度的划分数据，就需要使用分桶 技术将数据划分成更细的粒度。 使用 cluster by &lt;col-name&gt; into &lt;num&gt; buckets 分桶的原理 MR 中： key.hashCode % reduceTask Hive 中： 分桶字段.hashCode % 分桶个数 分桶表创建 123456create table course( id int, name string, score int ) clustered by (id) into 3 buckets row format delimited fields terminated by &quot;\\\\t&quot;; 分桶表加载数据 1234-- 普通表加载数据 load data local inpath &#x27;/home/hadoop/data/course.dat&#x27; into table course_common;-- 通过 insert ... select ... 给桶表加载数据 insert into table course select * from course_common; DQLSELECT SQL语句对大小写不敏感 各子句一般要分行 where 过滤正则匹配过滤 1234-- 正则匹配，使用 rlike。正则表达式，名字以A或S开头select ename, salfrom empwhere ename rlike &#x27;^(A|S).*&#x27;; lateral view lateral view 首先将UDTF应用于基础表的每一行，然后将结果输出行与输入行连接起来以形成具有提供的表别名的虚拟表。 语法 从 0.12.0 开始列别名可省略，从 UTDF 返回的 StructObjectInspector 的字段名称继承 12lateralView: LATERAL VIEW udtf(expression) tableAlias AS columnAlias (&#x27;,&#x27; columnAlias)*fromClause: FROM baseTable (lateralView)* 使用案例 123456789101112131415161718192021222324252627CREATE TABLE IF NOT EXISTS pageAds( pageid string, adid_list Array&lt;int&gt;);pageid adid_listfront_page [1,2,3]contact_page [3,4,5]-- 页面对应的广告SELECT pageid, adidFROM pageAds LATERAL VIEW explode(adid_list) adTable AS adid;-- 查看特定广告的展示次数SELECT adid, count(1) AS adcntFROM pageAds LATERAL VIEW explode(adid_list) adTable AS adid;pageid(string) adid(int)&quot;front_page&quot; 1&quot;front_page&quot; 2&quot;front_page&quot; 3&quot;contact_page&quot; 3&quot;contact_page&quot; 4&quot;contact_page&quot; 5adid adcnt1 12 13 24 15 1 多个 lateral view from clause 可有多个 lateral view 后续的 LATERAL VIEWS可以引用 LATERAL VIEW 左侧出现的任何表中的列。 表连接123456-- 内连接 select * from u1 join u2 on u1.id = u2.id;-- 左外连接 select * from u1 left join u2 on u1.id = u2.id;-- 全外连接 select * from u1 full join u2 on u1.id = u2.id; 多表连接 Hive 总是按照从左到右的顺序执行，Hive 会对每对 JOIN 连接对象启动一个 MapReduce 任务。 会首先启动一个 MapReduce job 对表 t 和表 c 进行连接操作；然后再 启动一个 MapReduce job 将第一个 MapReduce job 的输出和表 s 进行连接操作； 然后再继续直到全部操作； 12345select *from techer t left join course c on t.t_id = c.t_id left join score s on s.c_id = c.c_id left join student stu on s.s_id = stu.s_id; 笛卡尔积 满足下列条件 没有连接条件 连接条件无效 所有表中的所有行互相连接 Hive 默认不支持笛卡尔积 12set hive.strict.checks.cartesian.product=false;select * from u1, u2; 排序子句MR 全局排序 排序字段需要出现在 select 字段中 ORDER BY 执行全局排序，只有一个 reduce 输出规模较大时，耗时高 12345678-- 多列排序 select empno, ename, job, mgr, sal + nvl(comm, 0) salcomm, deptno from emp order by deptno, salcomm desc; MR 的内部排序(sort by) sort by 为每个 reduce 产生排序文件，在 reduce 内部进行排序，得到局部有序的结果 123456-- 设置reduce个数set mapreduce.job.reduces=2;-- 按照工资降序查看员工信息 select * from emp sort by sal desc;-- 将查询结果导入到文件中（按照工资降序）。生成两个输出文件，每个文件内部数据按 工资降序排列 insert overwrite local directory &#x27;/home/hadoop/output/sortsal&#x27; select * from emp sort by sal desc; MR 分区排序(distribute by) 将特定的行发送到特定的 reducer 中 distribute by 要写在 sort by 之前 可结合 sort by 操作，使分区数据有序 类似于 MR 中的分区操作 按照指定的条件将数据分组，常结合 sort by 使用 123456789101112131415161718192021-- 先按 deptno 分区，在分区内按照 sal + comm 排序set mapreduce.job.reduces=3;SELECT empno, ename, job, deptno, sal + nvl(comm, 0) salcommFROM emp DISTRIBUTE BY deptno SORT BY sal comm DESC;-- 将数据分到 3 个区中，每个分区都有数据insert overwrite local directory &#x27;/home/hadoop/output/distBy1&#x27; select empno, ename, job, deptno, sal + nvl(comm, 0) salcomm from emp distribute by deptno sort by salcomm desc; Cluster By distribute by 与 sort by 为同一个字段时，使用 cluster by 简化语法 只能是升序，不能指定排序规则 其他HQL 与 MR1234567Hive SQL &#x3D;&#x3D;&gt; AST(抽象语法树) &#x3D;&#x3D;&gt; QB(查询块) &#x3D;&#x3D;&gt; OperatorTree(操作树) &#x3D;&#x3D;&gt; 优化后的操作树 &#x3D;&#x3D;&gt; MapReduce 任务树 &#x3D;&#x3D;&gt; 优化后的 MapReduce 任务树 过程描述如下： SQL Parser：Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象语法树AST Tree； Semantic Analyzer：遍历AST Tree，抽象出查询的基本组成单元QueryBlock； Logical plan：遍历QueryBlock，翻译为执行操作树OperatorTree； Logical plan optimizer: 逻辑层优化器进行OperatorTree变换，合并不必要的ReduceSinkOperator，减少shuffle数据量； Physical plan：遍历OperatorTree，翻译为MapReduce任务； Logical plan optimizer：物理层优化器进行MapReduce任务的变换，生成最终的执行计划。 Join 与 MR 如果其中有一张表为小表，直接使用 map 端 join 的方式（map端加载小表）进行聚合。 如果两张都是大表，那么采用联合 key，联合 key 的第一个组成部分是 join on 中的公共字段，第二部分是一个 flag，0 代表表 A，1 代表表 B，由此让 Reduce 区分 join 表的信息；在Mapper中同时处理两张表的信息，将 join on 公共字段相同的数据划分到同一个分区中，进而传递到一个 Reduce 中，然后在 Reduce 中实现聚合。 如果对于每个表在 join 子句中使用相同的列，则 Hive 将多个表上的联接转换为单个map / reduce作业 123SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1) 对于 join 使用不同的列， 第一个 map / reduce 作业将 a 与 b 联接在一起，然后将结果与 c 联接到第二个 map / reduce 作业中。 第一个将 a 与 b 连接起来，并缓冲a的值，同时在减速器中流式传输b的值 第二个将缓冲第一个连接的结果，同时将c的值通过简化器流式传输 123SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2) 可指定流式传输的表 1SELECT /*+ STREAMTABLE(a) */ a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1) 如果除一个要连接的表之外的所有表都很小，则可以将其作为仅 Map 作业执行。 无需进行 reduce, 对于 A 的 mapper B 都会完全读取。 12SELECT /*+ MAPJOIN(b) */ a.key, a.valueFROM a JOIN b ON a.key = b.key; Join 相关的配置参数： hive.auto.convert.join： 如果可能，在运行时自动将联接转换为mapjoins hive.auto.convert.join.noconditionaltask：Hive是否启用基于输入文件大小的有关将公共联接转换为mapjoin的优化。 hive.auto.convert.join.noconditionaltask.size： 如果hive.auto.convert.join.noconditionaltask关闭，则此参数不起作用。 执行过程HQL 的执行过程 通常情况下NULL参与运算，返回值为 NULL NULL&lt;=&gt;NULL 的结果为 true，一般对 NULL 的比较实用 ISNULL 函数 Json 数据处理Hive 处理 json 数据的方式 内建的函数 get_json_object 自定义 UDF 函数 使用序列化反序列化工具 方式一: 内建的函数处理 处理简单的 json 串。 get_json_object(string json_string, string path): 解析 json 字符串 json_string，返回 path 指定的内容； json_tuple(jsonStr, k1, k2, ...): ：参数为一组键k1，k2，…和json字符串，返回值的元组。该方法比 get_json_object 高效，可以在一次调用中输入多个键, 对嵌套结果的解析操作复杂； explode / lateral view，使用explod将Hive一行中复杂的 array 或 map 结构拆分成多行。 12345678910111213141516171819202122232425CREATE TABLE IF NOT EXISTS jsont1( username string, age int, sex string, json string ) row format delimited fields terminated by &#x27;;&#x27;;load data local inpath &#x27;/root/lagoudw/data/weibo.json&#x27; overwrite into table jsont1;-- get 单层值 select username, age, sex, get_json_object(json, &quot;$.id&quot;) id, get_json_object(json, &quot;$.ids&quot;) ids, get_json_object(json, &quot;$.total_number&quot;) num from jsont1;-- get 数组select username, age, sex, get_json_object(json, &quot;$.id&quot;) id, get_json_object(json, &quot;$.ids[0]&quot;) ids0, get_json_object(json, &quot;$.ids[1]&quot;) ids1, get_json_object(json, &quot;$.ids[2]&quot;) ids2, get_json_object(json, &quot;$.ids[3]&quot;) ids3, get_json_object(json, &quot;$.total_number&quot;) num from jsont1;-- json_tuple 一次处理多个字段select json_tuple(json, &#x27;id&#x27;, &#x27;ids&#x27;, &#x27;total_number&#x27;) from jsont1; 含其他字段时，不能直接展开，需要使用 explod 展开 123456789101112-- 拆分 jsonselect username, age, sex, id, ids, num from jsont1 lateral view json_tuple(json, &#x27;id&#x27;, &#x27;ids&#x27;, &#x27;total_number&#x27;) t1 as id, ids, num;-- 拆分 JSON -&gt; 拆分 jsonarraywith tmp as( select username, age, sex, id, ids, num from jsont1 lateral view json_tuple(json, &#x27;id&#x27;, &#x27;ids&#x27;, &#x27;total_number&#x27;) t1 as id, ids, num ) select username, age, sex, id, ids1, numfrom tmp lateral view explode(split(regexp_replace(ids, &quot;\\\\\\\\[|\\\\\\\\]&quot;, &quot;&quot;), &quot;,&quot;)) t1 as ids1; 方式二: 使用 UDF 处理 能处理大部分数据，更灵活。 12345678910111213141516171819-- 创建临时函数add jar /root/lagoudw/jars/bigdata-hive-1.0-SNAPSHOT.jar;create temporary function json_json_array as &quot;com.janhen.bigdata.hive.ParseJsonArray&quot;;select username, age, sex, parse_json_array(json, &quot;ids&quot;) ids from jsont1;select username, age, sex, ids1 from jsont1 lateral view explode(parse_json_array(json, &quot;ids&quot;)) t1 as ids1;select username, age, sex, id, num from jsont1 lateral view json_tuple(json, &#x27;id&#x27;, &#x27;total_number&#x27;) t1 as id, num;-- 合并select username, age, sex, ids1, id, num from jsont1 lateral view explode(parse_json_array(json, &quot;ids&quot;)) t1 as ids1 lateral view json_tuple(json, &#x27;id&#x27;, &#x27;total_number&#x27;) t1 as id, num; 方式三: 使用SerDe处理 对象的序列化用途： 把对象转换成字节序列后保存到文件中 对象数据的网络传送 可以在表创建的时候指定 SerDe，之后无需指定分割符之类的信息 Read : HDFS files =&gt; InputFileFormat =&gt; &lt;key, value&gt; =&gt; Deserializer =&gt; Row object Write : Row object =&gt; Seriallizer =&gt; &lt;key, value&gt; =&gt; OutputFileFormat =&gt; HDFS files 123456789101112&#123;&quot;id&quot;: 1,&quot;ids&quot;: [101,102,103],&quot;total_number&quot;: 3&#125;&#123;&quot;id&quot;: 2,&quot;ids&quot;: [201,202,203,204],&quot;total_number&quot;: 4&#125;&#123;&quot;id&quot;: 3,&quot;ids&quot;: [301,302,303,304,305],&quot;total_number&quot;: 5&#125;&#123;&quot;id&quot;: 4,&quot;ids&quot;: [401,402,403,304],&quot;total_number&quot;: 5&#125;&#123;&quot;id&quot;: 5,&quot;ids&quot;: [501,502,503],&quot;total_number&quot;: 3&#125;create table jsont2( id int, ids array&lt;string&gt;, total_number int)ROW FORMAT SERDE &#x27;org.apache.hive.hcatalog.data.JsonSerDe&#x27;;load data local inpath &#x27;/data/lagoudw/data/json2.dat&#x27; into table jsont2; JSON 处理方式比较 1、简单格式的json数据，使用 get_json_object、json_tuple 处理 2、对于嵌套数据类型，可以使用 UDF 3、纯 json 串可使用 JsonSerDe 处理更简单 Ref LanguageManual LateralView LanguageManual Joins","tags":[{"name":"Hive","slug":"Hive","permalink":"http://example.com/tags/Hive/"},{"name":"SQL","slug":"SQL","permalink":"http://example.com/tags/SQL/"}]},{"title":"Scala集合","date":"2021-03-09T13:26:10.000Z","path":"2021/03/09/Scala集合/","text":"[TOC] 集合操作都有可变和不可变两种 集合的三大类：Seq、Set、Map scala.collection.mutable：定义了可变集合的特质和具体实现类 scala.collection.immutable：定义了不可变集合的特质和具体实现类 所有的集合都扩展自 Iterable 特质。 String 属于 IndexedSeq Queue 队列和 Stack 堆这两个经典的数据结构属于 LinearSeq List列表属于 Seq 中的 LinearSeq Seq按照一定顺序排列的元素序列； 元素的顺序是确定的，每个元素对应一个索引值； 两个重要的子特质： IndexedSeq：提供了快速随机访问元素的功能，它通过索引来查找和定位的 LinearSeq：提供了访问 head、tail 的功能，它是线型的，有头部和尾部的概念，通过遍历来查找。 ListList一旦被定义，其值就不能改变。 有头部和尾部的概念 head 返回的是列表第一个元素的值 tail 返回的是除第一个元素外的其它元素构成的新列表 定义了一个空列表对象Nil，定义为List[Nothing] 借助 Nil 可将多个元素用操作符 :: 添加到列表头部，常用来初始化列表； 操作符 ::: 用于拼接两个列表； 使用案例 12345678910111213141516171819202122232425262728293031323334353637def main(args: Array[String]): Unit = &#123; // :: 操作符表示向集合中添加元素 val list1 = 1 :: 2 :: 3 :: 4 :: Nil val list2 = 5 :: 6 :: 7 :: 8 :: Nil println(s&quot;list1: $list1&quot;) println(s&quot;list2: $list2&quot;) // 使用 ::: 操作符进行了拼接 list val list3 = list1 ::: list2 println(s&quot;list3: $list3&quot;) println(s&quot;list3.head: $&#123;list3.head&#125;&quot;) println(s&quot;list3.last: $&#123;list3.last&#125;&quot;) println(s&quot;list3.init: $&#123;list3.init&#125;&quot;) // 返回除最后一个元素之外的其他元素构成的新列表 println(s&quot;list3.tail: $&#123;list3.tail&#125;&quot;) // 返回除第一个元系之外的其他元素构成的新列表 val list4 = List(4, 2, 6, 1, 7, 9) println(s&quot;sorted: $&#123;quickSort(list4)&#125;&quot;)&#125;def quickSort(list: List[Int]): List[Int] = &#123; list match &#123; case Nil =&gt; Nil case head :: tail =&gt; val (less, greater) = tail.partition(_ &lt; head) quickSort(less) ::: head :: quickSort(greater) &#125;&#125;list1: List(1, 2, 3, 4)list2: List(5, 6, 7, 8)list3: List(1, 2, 3, 4, 5, 6, 7, 8)list3.head: 1list3.last: 8list3.init: List(1, 2, 3, 4, 5, 6, 7)list3.tail: List(2, 3, 4, 5, 6, 7, 8)sorted: List(1, 2, 4, 6, 7, 9) 源码 12345678910111213141516trait TraversableLike[+A, +Repr] extends Any with HasNewBuilder[A, Repr] with FilterMonadic[A, Repr] with TraversableOnce[A] with GenTraversableLike[A, Repr] with Parallelizable[A, ParIterable[A]]&#123; self =&gt; import Traversable.breaks._ def partition(p: A =&gt; Boolean): (Repr, Repr) = &#123; val l, r = newBuilder for (x &lt;- this) (if (p(x)) l else r) += x (l.result, r.result) &#125;&#125; Queue +=: ++=: 1234567891011121314151617181920212223val queue1 = new mutable.Queue[Int]()println(s&quot;queue1: $queue1&quot;)queue1 += 1queue1 ++= List(2, 3, 4)println(s&quot;queue1: $queue1&quot;)val dequeue: Int = queue1.dequeue()println(s&quot;dequeue: $dequeue&quot;)println(s&quot;queue1: $queue1&quot;)queue1.enqueue(5, 6, 7)println(s&quot;queue1: $queue1&quot;)println(s&quot;queue1.head: $&#123;queue1.head&#125;&quot;)println(s&quot;queue1.last: $&#123;queue1.last&#125;&quot;)queue1: Queue()queue1: Queue(1, 2, 3, 4)dequeue: 1queue1: Queue(2, 3, 4)queue1: Queue(2, 3, 4, 5, 6, 7)queue1.head: 2queue1.last: 7 Set &amp; / intersect: ++ / | / union： -- / &amp;~ / diff Map123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869val map = Map(&quot;a&quot; -&gt; 1, &quot;b&quot; -&gt; 2)map.keys.foreach(println(_))map.values.foreach(println)map.foreach(e =&gt; println(e._1 + &quot; ==&gt; &quot; + e._2))println(s&quot;map1(&#x27;b&#x27;): $&#123;map(&quot;b&quot;)&#125;&quot;)// 访问不存在的 Key 值时，抛出异常// println(map(&quot;c&quot;))// get 方法访问元素，返回一个 Option 对象val num: Option[Int] = map.get(&quot;c&quot;)num match &#123; case None =&gt; println(&quot;None&quot;) case Some(x) =&gt; println(x)&#125;// 获取Key值所对应的Value值，如果键Key不存在，那么就返回指定的默认值val num2: Int = map.getOrElse(&quot;d&quot;, 0)println(s&quot;num2: $num2&quot;)val map3 = scala.collection.mutable.Map(&quot;a&quot; -&gt; 1, &quot;b&quot; -&gt; 2)println(map3)map3(&quot;a&quot;) = 10println(s&quot;map3: $map3&quot;)map3(&quot;c&quot;) = 3println(s&quot;map3: $map3&quot;)// +=添加元素，-=删除元素map3 += (&quot;d&quot; -&gt; 4, &quot;f&quot; -&gt; 5)println(s&quot;map3: $map3&quot;)map3 -= &quot;d&quot;println(s&quot;map3: $map3&quot;)// Key, Value swapval kv: mutable.Map[Int, String] = for ((k, v) &lt;- map3) yield (v, k)println(kv)// Key, Value swap(R)map3.map(x =&gt; (x._2, x._1)).foreach(println(_))// 拉链操作创建Mapval a = Array(1, 2, 3)val b = Array(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)val c: Array[(Int, String)] = a.zip(b)val d: Map[Int, String] = a.zip(b).toMapprintln(s&quot;zip: $&#123;c.toBuffer&#125;&quot;)println(s&quot;zip: $d&quot;)ab12a ==&gt; 1b ==&gt; 2map1(&#x27;b&#x27;): 2Nonenum2: 0Map(b -&gt; 2, a -&gt; 1)map3: Map(b -&gt; 2, a -&gt; 10)map3: Map(b -&gt; 2, a -&gt; 10, c -&gt; 3)map3: Map(b -&gt; 2, d -&gt; 4, a -&gt; 10, c -&gt; 3, f -&gt; 5)map3: Map(b -&gt; 2, a -&gt; 10, c -&gt; 3, f -&gt; 5)Map(2 -&gt; b, 5 -&gt; f, 10 -&gt; a, 3 -&gt; c)(2,b)(5,f)(10,a)(3,c)zip: ArrayBuffer((1,a), (2,b), (3,c))zip: Map(1 -&gt; a, 2 -&gt; b, 3 -&gt; c) 操作 collect： collect通过执行一个并行计算（偏函数），得到一个新的数组对象 reduce： sorted / sortwith / sortby： 与 Java 集合转换12345import scala.collection.JavaConverters._val list: java.util.List[Int] = List(1,2,3,4).asJavaprintln(s&quot;java list: $list&quot;)val buffer: scala.collection.mutable.Buffer[Int] = list.asScalaprintln(s&quot;scala buffer: $buffer&quot;)","tags":[{"name":"Scala","slug":"Scala","permalink":"http://example.com/tags/Scala/"}]},{"title":"Scala语言基础","date":"2021-03-09T13:23:27.000Z","path":"2021/03/09/Scala语言基础/","text":"Scala 基础语言起源 12345马丁·奥德斯基（Martin Odersky）是编译器及编程的狂热爱好者。主流JVM的Javac编译器就是马丁·奥德斯基编写出来的，JDK5.0、JDK8.0的编译器就是他写的。 长时间的编程之后，他希望发明一种语言，能够让写程序这样的基础工作变得高效，简单。 当接触到Java语言后，对Java这门语言产生了极大的兴趣，所以决定将函数式编程语言的特点融合到Java中，由此发 明了Scala。 语言特性： OOP 函数式编程 静态类型，参考 Haskell、Errlang 并发性，使用 Actor 作为并发模型，可复用线程 应用场景： Kafka、Spar 等框架底层都是使用 Scala 作为底层源码开发语言 融合大数据生态，Flink 支持 Scala 开发 Scala 的 REPL REPL 是一个交互式解析器环境，R(read)、E(evaluate) 、P（print）、L（loop） 输入值，交互式解析器会读取输入内容并对它求值，再打印结果，并重复此过程。 在命令行输入Scala可启动Scala REPL。 基础语法 区分大小写 - Scala语言对大小写敏感 类名 - 对于所有的类名的第一个字母要大写。 方法名 - 所有方法名的第一个字母用小写。 程序文件名 - Scala程序文件的后缀名是 .scala，程序文件的名称可以不与对象名称完全匹配。 main()方法 - Scala程序从main()方法开始处理 常用类型 Int: Unit: 无值，用于不返回任何结果的方法的返回类型。 Null Nothing： 在Scala类层级的最低端，它是任何其他类型的子类型 Any: 是Scala中所有类的超类 AnyRef: 是Scala中所有引用类的超类 Scala和Java一样，有8种数值类型 Byte、Short、Int、Long、Float、Double、Char、Boolean 类型； Scala 并不刻意的区分基本类型和引用类型。 每一种数据类型都有对应的Rich类型，如RichInt、RichChar等，为基本类型提供了更多的有用操作。 类层次结构Scala中，所有的类，包括值类型和引用类型，都最终继承自一个统一的根类型Any。 Null Null是所有引用类型的子类型 Null类只有一个实例对象null null可以赋值给任意引用类型，但是不能赋值给值类型。 Nothing Nothing位于Scala类继承关系的底部，它是其他所有其他类型的子类型 Nothing对泛型结构有用 。比如，空列表Nil的类型就是List[Nothing] Nothing的可以给出非正常终止的信号。比如，使用Nothing处理异常 在Scala中，鼓励使用val。 简单数据类型可以省略，对于复杂的数据类型建议明确声明； 操作符 Scala中的操作符都是方法 对象相等性 Scala中，要比较两个基础类型的对象是否相等，可以使用 == 或 !=； == 或 != 还可以比较不同类型的两个对象 字符串字符串插值器 s 插值器： 对内嵌的每个表达式求值，对求值结果调用toString f 插值器： 除s插值器的功能外，还能进行格式化输出，在变量后用%指定输出格式 raw 插值器： 按照字符串原样进行输出 控制结构和函数if 表达式 if 表达式有返回值。 如果if 和 else 的返回值类型不一样，那么就返回两个返回值类型公共的父类。 for 表达式 for (i &lt;- 表达式 / 集合)，让变量 i遍历&lt;-右边的表达式/集合的所有值。 Scala为for循环提供了很多的特性，这些特性被称之为 for守卫式 或 for推导式。 123456789101112131415161718192021for (i &lt;- 1 until 10) &#123; println(s&quot;i = $i&quot;)&#125;// 双重循环。条件之间使用分号分隔for (i &lt;- 1 until 5; j &lt;- 2 until 5)&#123; println(i * j )&#125;// 守卫语句。for (i &lt;- 1 to 10; j &lt;- 1 to 10 if i==j)&#123; println(s&quot;i * j = $i * $j = $&#123;i * j&#125;&quot;)&#125;// for推导式, yield 接收返回的结果val result = for (i &lt;- 1 to 10) yield i// 使用大括号将生成器、守卫、定义包含在其中；并以换行的方式来隔开它们for &#123; i &lt;- 1 to 3 from = 4 - i j &lt;- from to 3 &#125; println(s&quot;i = $i; j = $j&quot;) while 表达式 while语句的本身没有任何返回值类型，即while语句的返回结果是Unit类型的 () 。 Scala内置控制结构特地去掉了 break 和 continue。 终止循环的方式： 使用Boolean类型的控制变量 使用 return 使用 breakable和break，需要导入scala.util.control.Breaks包 懒值当 val 被声明为lazy时(var不能声明为lazy)，初始化将被推迟，直到首次对此取值，适用于初始化开销较大的场景。 惰性求值 可根据 if 短路求值，避免不必要的 expensive 计算操作 12345678910def evaluate(input: Int): Unit = &#123; println(s&quot;evaluate called with $input&quot;) if (input &gt;= 10 &amp;&amp; expensiveComputation()) println(&quot;doing work...&quot;) else println(&quot;skipping&quot;)&#125;evaluate(0)evaluate(100) 通过 scala 的懒加载处理 123lazy val perform = expensiveComputation() if (input &gt;= 10 &amp;&amp; perform) println(“doing work…”) 文件操作导入scala.io.Source后，可引用Source中的方法读取文本文件的内容 Scala没有内建的对写入文件的支持。要写入文本文件，可使用 java.io.PrintWriter 数组和元组使用ArrayBuffer时，需要导包 import scala.collection.mutable.ArrayBuffer； 多维数组 123456val dim = Array.ofDim[Double](3,4)dim(1)(1) = 11.11for (i &lt;- 0 to 2; j &lt;- 0 to 3) &#123; print(dim(i)(j) + &quot; &quot;) if (j == 3) println()&#125; 元组和操作 内建了 22 个 Tuple 类 类与对象在Scala中，类并不用声明为public； val修饰的变量（常量），值不能改变，只提供getter方法，没有setter方法； var修饰的变量，值可以改变，对外提供getter、setter方法； 自定义getter和setter方法 Scala 类中的每一个属性，编译后会有一个私有的字段和相应的getter、setter方法生成。 12345678//getter方法println(person age)//setter方法person age_= (18)//getter方法println(person.age) JavaBean默认情况下，Scala 不遵循 JavaBean 约定，必须要使用 @scala.reflect.BeanProperty 注解来生成满足 JavaBean 约定的 getter 和 setter 方法 构造器没有定义构造器，Scala类中会有一个默认的无参构造器； 的构造器分为两种：主构造器和辅助构造器； 主构造器的定义与类的定义交织在一起，将主构造器的参数直接放在类名之后。 当主构造器的参数不用var或val修饰时，参数会生成类的私有val成员。 Scala中，所有的辅助构造器都必须调用另外一个构造器 单例对象没有提供Java那样的静态方法或静态字段； 可以采用object关键字实现单例对象，具备和Java静态方法同样的功能； 使用object语法结构【object是Scala中的一个关键字】达到静态方法和静态字段的目的；对象本质上可以拥有类的所有特性，除了不能提供构造器参数； 任何在Java中用单例对象的地方，在Scala中都可以用object实现： 作为存放工具函数或常量的地方 高效地共享单个不可变实例 Scala中的单例对象具有如下特点： 创建单例对象不需要使用new关键字 object中只有无参构造器 主构造代码块只能执行一次，因为它是单例的 应用程序对象 可以扩展App特质（trait) 来运行。 伴生类与伴生对象单例对象与某个类具有相同的名称时，它被称为这个类的“伴生对象”； 类和它的伴生对象必须存在于同一个文件中，而且可以相互访问私有成员（字段和方法）； apply 方法object 中的特殊方法 apply方法通常定义在伴生对象中，目的是通过伴生类的构造函数功能，来实现伴生对象的构造函数功能； 当遇到类名(参数1,…参数n)时apply方法会被调用； 在创建伴生对象或伴生类的对象时，通常不会使用new class/class() 的方式，而是直接使用 class()隐式的调用伴生对象的 apply 方法 借助 apply 实现工厂设计模式 12345678910111213141516171819202122232425262728293031abstract class Animal &#123; def speak&#125;class Dog extends Animal &#123; override def speak: Unit = &#123; println(&quot;woof&quot;) &#125;&#125;class Cat extends Animal &#123; override def speak: Unit = &#123; println(&quot;meow&quot;) &#125;&#125;object Animal &#123; def apply(str: String): Animal = &#123; if (str == &quot;dog&quot;) new Dog else new Cat &#125;&#125;object Test extends App &#123; val cat = Animal(&quot;cat&quot;) cat.speak val dog = Animal(&quot;dog&quot;) dog.speak&#125; 继承override方法重写 123456789101112class Programmer(name: String, age: Int) &#123; def coding(): Unit = &#123; println(&quot;coding...&quot;) &#125;&#125;class ScalaProgrammer(name: String, age: Int, workNo: String) extends Programmer(name, age) &#123; override def coding(): Unit = &#123; super.coding() println(&quot;我在写Scala代码。。。&quot;) &#125;&#125; 类型检查与转换 isInstanceOf： 测试某个对象是否属于某个给定的类 getClass classOf 1234567891011121314151617181920212223242526272829303132333435363738class Person &#123; def say(): Unit = &#123; print(&quot;Person...&quot;) &#125;&#125;class Student extends Person &#123; override def say(): Unit = &#123; print(&quot;Student...&quot;) &#125;&#125;object InstanceDemo &#123; def main(args: Array[String]): Unit = &#123; val person: Person = new Student var student: Student = null println(student.isInstanceOf[Student]) // function check if (person.isInstanceOf[Student]) &#123; student = person.asInstanceOf[Student] student.say() &#125; println(student.isInstanceOf[Student]) // getClass check println(person.getClass == classOf[Person]) println(person.getClass == classOf[Student]) // pattern match check person match &#123; case s: Student =&gt; s.say() println(&quot;Student2 type match&quot;) case _ =&gt; println(&quot;default...&quot;) &#125; &#125;&#125; 特质作为接口使用 在trait中可以定义抽象方法，与抽象类中的抽象方法一样，只要不给出方法的具体实现即可。 类可以使用extends关键字继承trait。 在Scala中没有implement的概念，无论继承类还是trait特质，统一都是extends。 类继承trait特质后，必须实现其中的抽象方法，实现时可以省略 override 关键字。 Scala不支持对类进行多继承，但是支持多重继承trait特质，使用with关键字即可。 特质构造顺序 执行父类的构造器； 执行trait的构造器，多个trait从左到右依次执行； 构造trait时会先构造父trait，如果多个trait继承同一个父trait，则父trait只会构造一次； 所有trait构造完毕之后，子类的构造器才执行 Ordered和OrderingOrdered 特质混入 Java 的 Comparable 接口，它定义了相同类型间的比较方式，但这种内部比较方式是单一的； 12345678trait Ordered[A] extends Any with java.lang.Comparable[A] &#123; def compare(that: A): Int def &lt; (that: A): Boolean = (this compare that) &lt; 0 def &gt; (that: A): Boolean = (this compare that) &gt; 0 def &lt;= (that: A): Boolean = (this compare that) &lt;= 0 def &gt;= (that: A): Boolean = (this compare that) &gt;= 0 def compareTo(that: A): Int = compare(that)&#125; Ordering 特质混入 Comparator 接口，提供第三方比较器，可以自定义多种比较方式，在实际开发中也是使用比较多的，灵活解耦合。 12345678910111213141516171819202122232425262728293031323334353637383940@annotation.implicitNotFound(msg = &quot;No implicit Ordering defined for $&#123;T&#125;.&quot;)trait Ordering[T] extends Comparator[T] with PartialOrdering[T] with Serializable &#123; outer =&gt; def tryCompare(x: T, y: T) = Some(compare(x, y)) def compare(x: T, y: T): Int override def lteq(x: T, y: T): Boolean = compare(x, y) &lt;= 0 override def gteq(x: T, y: T): Boolean = compare(x, y) &gt;= 0 override def lt(x: T, y: T): Boolean = compare(x, y) &lt; 0 override def gt(x: T, y: T): Boolean = compare(x, y) &gt; 0 override def equiv(x: T, y: T): Boolean = compare(x, y) == 0 def max(x: T, y: T): T = if (gteq(x, y)) x else y def min(x: T, y: T): T = if (lteq(x, y)) x else y override def reverse: Ordering[T] = new Ordering[T] &#123; override def reverse = outer def compare(x: T, y: T) = outer.compare(y, x) &#125; def on[U](f: U =&gt; T): Ordering[U] = new Ordering[U] &#123; def compare(x: U, y: U) = outer.compare(f(x), f(y)) &#125; class Ops(lhs: T) &#123; def &lt;(rhs: T) = lt(lhs, rhs) def &lt;=(rhs: T) = lteq(lhs, rhs) def &gt;(rhs: T) = gt(lhs, rhs) def &gt;=(rhs: T) = gteq(lhs, rhs) def equiv(rhs: T) = Ordering.this.equiv(lhs, rhs) def max(rhs: T): T = Ordering.this.max(lhs, rhs) def min(rhs: T): T = Ordering.this.min(lhs, rhs) &#125; implicit def mkOrderingOps(lhs: T): Ops = new Ops(lhs)&#125;object Ordering extends LowPriorityOrderingImplicits &#123; def apply[T](implicit ord: Ordering[T]) = ord def by[T, S](f: T =&gt; S)(implicit ord: Ordering[S]): Ordering[T] = new Ordering[T] &#123; def compare(x: T, y: T) = ord.compare(f(x), f(y)) override def lt(x: T, y: T): Boolean = ord.lt(f(x), f(y)) override def gt(x: T, y: T): Boolean = ord.gt(f(x), f(y)) override def gteq(x: T, y: T): Boolean = ord.gteq(f(x), f(y)) override def lteq(x: T, y: T): Boolean = ord.lteq(f(x), f(y)) &#125;&#125; 使用案例 12345678910111213141516171819import scala.util.Sortingcase class Project(tag: String, score: Int) extends Ordered[Project] &#123; override def compare(that: Project): Int = &#123; tag.compareTo(that.tag) &#125;&#125;object OrderDemo &#123; def main(args: Array[String]): Unit = &#123; val list = List(Project(&quot;hadoop&quot;, 40), Project(&quot;flink&quot;, 90), Project(&quot;spark&quot;, 80), Project(&quot;hive&quot;, 60)) println(list.sorted) val pairs = Array((&quot;a&quot;, 7, 2), (&quot;b&quot;, 9, 1), (&quot;c&quot;, 8, 3)) // Ordering.by[(String,Int,Int),Int](_._2) 从 Tuple3 转到 Int 型，根据 Tuple3 第二个元素进行排序 Sorting.quickSort(pairs)(Ordering.by[(String, Int, Int), Int](_._2)) println(pairs.toBuffer) &#125;&#125; 模式匹配和样例类 Scala没有Java中的switch case Scala的模式匹配可以匹配各种情况，比如变量的类型、集合的元素、有值或无值。 模式匹配match case中，只要有一个case分支满足并处理了，就不会继续判断下一个case分支了 守卫式匹配可增加 if 条件判断 匹配类型 可以直接匹配类型，而不是值 匹配数组、元组、集合case class样例类 默认实现了常用的方法，如 getter/setter, 默认序列化 主构造器函数结构的参数不需要显示 var/val 修饰，自动使用 val 修饰 自动定义了伴生对象，提供 apply 方法，无需 new 关键字就可构造出对象 生成 toString,equals,hashCode,copy 方法 继承了 Product, Serializable 两个特质 case class 为多例的， case object 为单例的 Option与模式匹配Option通常与模式匹配结合使用，用于判断某个变量是有值还是无值。 函数与抽象化 不仅可以定义一个函数然后调用它，还可以写一个未命名的函数字面量，然后可以把它当成一个值传递到其它函数或是赋值给其它变量。 函数字面量体现了函数式编程的核心理念。字面量包括整数字面量、浮点数字面量、布尔型字面量、字符字面量、字符串字面量、符号字面量、函数字面量等。 函数类型：(输入参数类型列表) =&gt; (输出参数类型列表)只有一个参数时，小括号可省略；函数体中只有1行语句时 函数与方法 使用 val 定义的是函数(function)，使用 def 定义的是方法(method)。 Scala 中的方法与 Java 的类似，方法是组成类的一部分 Scala 中的函数则是一个完整的对象。Scala 中用 22 个特质(从 Function1 到 Function22)抽象出了函数的概念 Scala 中用 val 语句定义函数，def 语句定义方法 方法不能作为单独的表达式而存在，而函数可以； 函数必须要有参数列表，而方法可以没有参数列表； 方法名是方法调用，而函数名只是代表函数对象本身； 在需要函数的地方，如果传递一个方法，会自动把方法转换为函数 一般情况下，不对二者做区分，认为都是函数，更多的时候使用def定义函数。 方法转换成函数 12def double(x: Int) = x*xdef f1 = double _ 匿名函数 函数没有名字就是匿名函数； 匿名函数，又被称为 Lambda 表达式。 占位符 第一个下划线代表第一个参数 第二个下划线代表第二个参数 第三个……，如此类推 高阶函数 接收一个或多个函数作为输入或输出一个函数。 常用的高阶函数：map、reduce、flatMap、foreach、filter、count。 闭包 闭包是一种函数，是在其上下文中引用了自由变量的函数； 闭包引用到函数外面定义的变量，定义这个函数的过程就是将这个自由变量捕获而构成的一个封闭的函数，也可理解为”把函数外部的一个自由变量关闭进来“。 闭包满足的条件： 闭包是一个函数 函数必须要有返回值 返回值依赖声明在函数外部的一个或多个变量，用 Java 的话说，就是返回值和定义全局变量有关 柯里化 函数编程中，接收多个参数的函数都可以转化为接收单个参数的函数，这个转化过程就叫柯里化(Currying)。 柯里化函数拥有多组参数列表，每组参数用小括号括起来。 Scala 源码中的柯里化 12345trait TraversableOnce[+A] extends Any with GenTraversableOnce[A] &#123; self =&gt; def fold[A1 &gt;: A](z: A1)(op: (A1, A1) =&gt; A1): A1 = foldLeft(z)(op) def aggregate[B](z: =&gt;B)(seqop: (B, A) =&gt; B, combop: (B, B) =&gt; B): B = foldLeft(z)(seqop)&#125; 部分应用函数 部分应用函数（Partial Applied Function）也叫偏应用函数，是指缺少部分（甚至全部）参数的函数 如果一个函数有n个参数, 而为其提供少于n个参数, 那就得到了一个部分应用函数。 偏函数 并不处理所有可能的输入，而只处理那些能与至少一个 case 语句匹配的输入； 偏函数中只能使用 case 语句，整个函数必须用大括号包围。与普通的函数字面量不同，普通的函数字面量可以使用大括号，也可以用小括号； Scala中的 Partial Function 是一个 trait。 123456789101112131415trait PartialFunction[-A, +B] extends (A =&gt; B) &#123; self =&gt; import PartialFunction._ def isDefinedAt(x: A): Boolean def orElse[A1 &lt;: A, B1 &gt;: B](that: PartialFunction[A1, B1]): PartialFunction[A1, B1] = new OrElse[A1, B1] (this, that) override def andThen[C](k: B =&gt; C): PartialFunction[A, C] = new AndThen[A, B, C] (this, k) def lift: A =&gt; Option[B] = new Lifted(this) def applyOrElse[A1 &lt;: A, B1 &gt;: B](x: A1, default: A1 =&gt; B1): B1 = if (isDefinedAt(x)) apply(x) else default(x) def runWith[U](action: B =&gt; U): A =&gt; Boolean = &#123; x =&gt; val z = applyOrElse(x, checkFallback[B]) if (!fallbackOccurred(z)) &#123; action(z); true &#125; else false &#125;&#125; 使用案例 1234567891011121314// 将 Int 类型的元素加 1val partialFunction = new PartialFunction[Any, Int] &#123; override def isDefinedAt(x: Any): Boolean = &#123; x.isInstanceOf[Int] &#125; override def apply(v1: Any): Int = &#123; v1.asInstanceOf[Int] + 1 &#125;&#125;val list = List(10, &quot;hadoop&quot;, 20, &quot;spark&quot;, 30, &quot;flink&quot;)list.collect(partialFunction).foreach(println)// simple list.collect(&#123;case x: Int =&gt; x + 1&#125;).foreach(println) 隐式机制 根据隐式转换函数的签名，在程序中使用到隐式转换函数接收的参数类型定义的对象时，会自动将其传入隐式转换函数，转换为另外一种类型的对象并返回，这就是“隐式转换” 隐式转换和隐式参数是Scala中两个非常强大的功能，利用隐式转换和隐式参数，可以提供类库，对类库的使用者隐匿掉具体的细节。 使用限制 implicit 关键字只能用来修饰方法、变量、参数 隐式转换的函数只在当前范围内才有效。如果隐式转换不在当前范围内定义，那么必须通过 import 语句将其导入 隐式转换函数定义的隐式转换函数，只要在编写的程序内引入，就会被Scala自动使用。 隐式转换函数由Scala自动调用，通常建议将隐式转换函数的名称命名为“one2one”的形式。 隐式参数和隐式值在函数定义的时候，支持在最后一组参数中使用 implicit ，表明这是一组隐式参数。 在调用该函数的时候，可以不用传递隐式参数，而编译器会自动寻找一个implicit 标记过的合适的值作为参数。 查看范围 当前作用域内可见的 val 或 var 定义隐式变量 隐式参数类型的伴生对象内隐式值 RefThe Scala Programming Language Scala 官网 Scala 2.11.8 Scala官网下载Scala 2.11.8安装包 为什么 Haskell 是我们构建生产软件系统的首选 Haskell","tags":[{"name":"Scala","slug":"Scala","permalink":"http://example.com/tags/Scala/"}]},{"title":"Vagrent","date":"2021-03-09T13:17:56.000Z","path":"2021/03/09/Vagrent/","text":"[TOC] Vagrant 快速搭建虚拟机环境，可通过 Vagrantfile 配置文件进行定制，类似 Docker 管理容器。 一些特性： 多种虚拟器支持，如 VirsualBox(默认)、 Vmware、Docker、Hyper-V 方便网络配置，支持端口转发，配置私有、公有网络 方便进行宿主机与虚拟机之间共享文件提供丰富的插件，简化日常使用 环境安装Virtual Box Oracle 开源的虚拟机软件，跨平台 在 winows 上无法同时运行 Hyper-V 和 VisualBox，两者都是基于 CPU 等底层硬件的 Hypervisor 机制来实现的，而他们必须独占管理 Hypervisor。通过开启启动项选择是否加载 Hyper-V 服务，实现伪同时运行。 问题由来： Docker 安装在 win10 上需要开启 Hyper-V，方便进行本地镜像的打包部署，同时需要 VirtualBox 进行模拟集群。 问题处理： 解决Win7/8/10系统中的Hyper-V和VMware虚拟机软件共存问题 Virtualbox “Callee RC: REGDB_E_CLASSNOTREG” (0x80040154)? Hyper-V Win10 自带虚拟化工具，实现在 Win10 上运行 Docker 环境，而无需开启 Docker 的远程访问，开启后无法使用其他虚拟器 对应的 vagrant 设置内存和CPU参数参考文档 12345678910111213141516boxes &#x3D; [ &#123; :name &#x3D;&gt; &quot;docker-kubernetes&quot;, :eth1 &#x3D;&gt; &quot;192.168.205.12&quot;, :mem &#x3D;&gt; &quot;2048&quot;, :cpu &#x3D;&gt; &quot;2&quot; &#125;]... config.vm.provider &quot;hyperv&quot; do |v| v.ip_address_timeout&#x3D;121 v.memory&#x3D;opts[:mem] v.cpus&#x3D;opts[:cpu] endbcdedit &#x2F;set hypervisorlaunchtype offbcdedit &#x2F; set hypervisorlaunchtype auto XShell SSH 命令工具 一些特性 标签化页面管理，方便管理打开的连接 支持连接目录管理，方便进行多种环境管理 支持分屏，方便对集群中的主从进行区分 支持透明图，无需切换窗口查看遗忘的命令 侧栏显示连接信息，方便集群中配置 IP 地址 Vagrant 管理 通过 Vagrantfile 文件设置好一些数值进行控制虚拟机，通过命令管理虚拟机 1234567# 全局管理vagrant global-statusvagrant global-status --prunevagrant destroy &lt;vm_id&gt;vagrant halt &lt;vm_id&gt; &lt;vm_id2&gt;vagrant reload &lt;vm_id1&gt; &lt;vm_id2&gt;..vagrant up &lt;vm_id1&gt; &lt;vm_id2&gt;.. 安装虚拟机环境 快速安装环境，支持从远程获取对应的 Vagrantfile，之后拉取远程镜像；支持导入本地的 box 作为镜像； 12345678# vagrant 命令# 初始化一个安装 centos&#x2F;7 虚拟机的 Vagrantfile# 根据目录下的 Vagrantfile 进行启动# 删除虚拟机# 查案虚拟机运行状态vagrant init centos&#x2F;7vagrant upvagrant status 多种虚拟机支持 支持多种虚拟机，对应的配置memory、Cpu 方式不同 1234# 使用 vmware 虚拟机# 使用 hyper-v，需要管理员权限运行 vagrant，通过 Cmder 默认使用 admin 启动的 powershell 处理vagrant up --provider&#x3D;vmware_fusionvagrant up --provider&#x3D;hyperv Vagrant 插件vagrant-hostmanager 实现多台虚拟机之间直接通过名称访问，原理为更改 host 文件 123456789# 安装并验证插件vagrant plugin install vagrant-hostmanagervagrant plugin list# 在 Vagrantfile 中修改config.hostmanager.enabled &#x3D; trueconfig.hostmanager.manage_guest &#x3D; trueconfig.hostmanager.manage_host &#x3D; true# 执行命令，更新虚拟机上的hosts，同时更新主机上的 hostsvagrant hostmanager vagrant-vbguest 处理 VisualBox 中无法设置共享目录问题 12345vagrant plugin install vagrant-vbguestvagrant vbguest --statusvagrant vbguest --do install node1# 配置 vagrantfileconfig.vbguest.auto_update&#x3D;false vagrant-bindfs 非使用 visualBox 自带的共享目录，自定义使用文件系统 nfs，性能更高 12345678910111213141516171819202122232425vagrant plugin install vagrant-bindfs# 。。。node1. vm. synced_folder &quot;.&#x2F;app&quot;,&quot;&#x2F;mnt&#x2F;app-data&quot;, type:&quot;nfsnode1. bindfs. bind_folder &quot;&#x2F;mnt&#x2F;app-data&quot;,&quot;&#x2F;app&quot;, force_user:&quot;root&quot;, force_group:&quot;root&quot;,o:&quot;nonempty&quot;# 代理设置插件# 在Vagrantfile中的config部分添加代理配置, 全部网络都走主机代理vagrant plugin install vagrant-proxyconfvim Vagrantfile Vagrant.configure(&quot;2&quot;) do |config| if Vagrant.has_plugin?(&quot;vagrant-proxyconf&quot;) config.proxy.http &#x3D; &quot;&lt;http:&#x2F;&#x2F;192.168.0.2:3128&#x2F;&gt;&quot; config.proxy.https &#x3D; &quot;&lt;http:&#x2F;&#x2F;192.168.0.2:3128&#x2F;&gt;&quot; config.proxy.no_proxy &#x3D; &quot;localhost,127.0.0.1,.example.com&quot; end # ... other stuff end# 复用虚拟机环境插件安装vagrant plugin listvagrant plugin install vagrant-scpvagrant scp# 处理虚拟机安装进行目录文件映射权限问题vagrant plugin install vagrant-vbguestvagrant plugin list Vagrantfile 构建虚拟机的硬件情况，实现控制 CPU、内存、Ip 等资源，同时支持虚拟机启动后执行初始化脚本，实现一些必要工具的安装，如 Docker。 通过配置可实现： 进行虚拟机目录与本地目录映射 选择网络 安装成功后执行特定脚本，直接安装要必要的工具以及 Docker 环境 12345678910# config.vm.box 配置使用哪个boxconfig.vm.box &#x3D; &quot;ubuntu16.04_louis&quot;# box ∈ vagrant box listconfig.vm.hostname # 机器应该有的主机名aa.vm.hostname &#x3D; &quot;aa.test.com&quot;config.vm.network # 在机器上配置网络config.vm.network&quot;forwarded_port&quot;,guest:80,host:8080aa.vm.network &quot;private_network&quot;, ip: &quot;192.168.55.100&quot;config.vm.provider # 配置提供程序特定的配置，用于修改特定于某个 提供程序的设置config.vm.provision # 配置置备 在机器上，使软件可以自动安装并创建机器时配置config.vm.synced_folder # 配置 机器上的同步文件夹 配置实例： 参数注入 脚本执行，进行必要软件(vim,git…)、必要环境(如pip,java,docker) 的安装 资源配置，可 CPU、内存…. 等硬件资源 123456789101112131415161718boxes &#x3D; [ &#123; :name &#x3D;&gt; &quot;docker-host&quot;, :eth1 &#x3D;&gt; &quot;192.168.205.10&quot;, :mem &#x3D;&gt; &quot;1024&quot;, :cpu &#x3D;&gt; &quot;1&quot; &#125;]boxes.each do |opts| config.vm.provider &quot;virtualbox&quot; do |v| v.customize [&quot;modifyvm&quot;, :id, &quot;--memory&quot;, opts[:mem]] v.customize [&quot;modifyvm&quot;, :id, &quot;--cpus&quot;, opts[:cpu]] end config.vm.network :private_network, ip: opts[:eth1] # 从主机特定文件读入脚本执行 config.vm.provision &quot;shell&quot;, privileged: true, path: &quot;.&#x2F;setup.sh&quot; endend 网络配置根据需要设置虚拟机网络 IP 地址固定，实现虚拟机中的软件根据名称访问，设置虚拟机可以访问外部的网络。 端口转发 将宿主机的端口与虚拟机的端口绑定，从而让外部通过端口可以访问虚拟机 若 guest_ip 和 host_ip 两项配置为空，则局域网下的所有设备都可以访问该虚拟机 12345678910111213141516Vagrant.configure(2&quot;) do |config|config.vm.network&quot;forwarded_port&quot;(必选) &#x2F;&#x2F;端口转发标示,guest(必选): # 虚拟端口,host(必选): # 宿主机端口，值必须大于1024,gust_ip(可选): # 虚拟机端口绑定虚拟机ip地址,host_ip(可选): # 虚拟机端口绑定宿主机端口ip,protocol(可选): # 指定通信协议，可以使用tcp&#x2F;udp,默认tcp,auto_correct(可选): # ture&#x2F;fasle,开机是否自动检测端口冲突end# 实际配置# 配置2个端口映射，把物理机的8080映射到虚拟机80，物理机的2100映射到虚拟机的22# host_ip 在主机 IP 较为固定情况下配置使用config.vm.network :&quot;forwarded_port&quot;, guest: 80, host: 8060,host_ip: &quot;10.2.11.203&quot;config.vm.network :&quot;forwarded_port&quot;, guest: 22, host: 2100, host_ip: &quot;10.2.11.203&quot; 私有网络 虚拟机之间处在同一网段的地址可相互访问，主机可以访问虚拟机，无法通过虚拟机进行团队合作，不与宿主机的 IP 在同一个网段，防止冲突 配置 vagrant 里面的虚拟机的私有网段的时候，切记不能和企业（公司）内部的 DHCP 分配的 IP 地址在同一网段，否则会发生冲突 1234567891011# 配置 Static IPconfig.vm.network &quot;private_network&quot;, ip: &quot;192.168.50.10&quot;config.vm.network &quot;private_network&quot;, ip: &quot;192.168.55.20&quot;# 配置通过 DHCP 进行获取 IP，之后执行 &#96;vagrant reload&#96;config.vm.network &quot;private_network&quot;, type: &quot;dhcp&quot;# 实际使用Vagrant.configure(&quot;2&quot;) do |config|config.vm.network &quot;private_network&quot;, ip: &quot;192.168.50.10&quot;,auto_config: falseend 公有网络 与宿主机一样的网络配置， vagrant1.3+ 支持设置固定 IP，虚拟机 IP 与主机 IP 处在同一个网段时，实现局域网之间的互通，需要有路由器分配 IP.一般来说开发和测试使用较为封闭的网络模型是比较好的方式，通常不建议 vm 配置有 public_network 的网卡关联 配置虚拟机自动获取公司内部DHCP服务器分配的IP地址，在局域网任何一台电脑上，都可以ssh到虚拟机，或访问虚拟机上提供的服务 12345678910111213141516171819config.vm.network &quot;public_network&quot;, ip: &quot;192.168.1.120&quot;# 配置动态 IP# 配置共有网络，使用主机上可以访问外网的接口(ipconfig)# 配置默认网关config.vm.network &quot;public_network&quot;,bridge: &quot;ens33&quot;config.vm.provision &quot;shell&quot;,run: &quot;always&quot;,inline: &quot;route add default gw 10.2.11.1&quot;# 配置静态 IP# auto_config：关闭自动配置# ifconfig enp0s8 10.2.11.196 netmask 255.255.255.0 up: 配置静态ip（这里的ip不能和公司内部的地址冲突）# route add default gw 10.2.11.1 指定网关（添加默认路由）# bridge： 绑定接口（物理机哪个接口可以上网）config.vm.network &quot;public_network&quot;, auto_config: false ,bridge: &quot;ens33&quot;config.vm.provision &quot;shell&quot;,run: &quot;always&quot;,inline: &quot;ifconfig eth1 10.2.11.196 netmask 255.255.0.0 up&quot;config.vm.provision &quot;shell&quot;,run: &quot;alway&quot;,inline: &quot;route add default gw 10.2.11.1&quot;config.vm.network &quot;public_network&quot;, auto_config: false config.vm.provision &quot;shell&quot;,run: &quot;always&quot;,inline: &quot;ip addr add 172.17.10.51&#x2F;21 dev eth1&quot; config.vm.provision &quot;shell&quot;,run: &quot;alway&quot;,inline: &quot;ip route add 172.17.8.0&#x2F;21 via 172.17.0.49&quot; 共享文件配置宿主机中的数据与虚拟机的数据映射 1234567# src： 是物理机的目录，相对路径，（相对于项目目录（&#x2F;vagrant&#x2F;ubuntu））# &#x2F;srv&#x2F;website: 虚拟机的目录，绝对路径，如果没有，会自动创建config.vm.synced_folder &quot;src&#x2F;&quot;, &quot;&#x2F;srv&#x2F;website&quot;endconfig.vm.synced_folder &quot;.&quot;, &quot;&#x2F;vagrant&quot;, disabled: trueconfig.vm.synced_folder &quot;src&#x2F;&quot;,&quot;&#x2F;srv&#x2F;website&quot;,owner: &quot;root&quot;,group: &quot;root&quot; Q&amp;A 出现的问题以及对应的处理 @Q: 处理本地下载对应镜像慢问题： 直接下载、设置代理 执行 vagrant up --provider=hyperv，在控制台找到下载地址，使用本地下载工具下载(代理) 12345678# 执行下载# 获取地址重新下载# 重命名为指定格式vagrant up --provider&#x3D;hyperv# 将下载的 .box 添加# 使用下载的 .box 进行初始化 vagrant box add centos-7_hyperv hyperv.boxvagrant init centos-7_hyperv @Q: 卸载重装 Vagrant 无法删除之前构建的虚拟机 @Q: 公有网络设置静态 IP 的接口选择问题，无法选择 Wifi、以太网接口?? 待验证 选择不同的连接网卡是否可相互通信？ 选择以太网的桥接可实现内网互通 @Q: 使用以太网接口指定公司网关显示网络不可达? 公司内网的安全性?? 相当于占用内网的一个 IP @Q: 二次使用 vagrantfile 时，报错 chmod: cannot access ‘/etc/systemd/system/docker.service.d/http-proxy.conf’: No such file or directory A： 通过 ssh 进入主机，创建该文件 // todo 搜寻更好的处理方法 1touch &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d&#x2F;http-proxy.conf 原因是修改了网络配置(改成桥接)，重新配置 docker 的代理，需要创建文件的方式来配置代理，而默认情况下无权限访问 @Q: 同步文件夹显示编码问题， ==&gt; Test-Node: Rsyncing folder: /cygdrive/d/develop/Env2/Test-Node/ =&gt; /vagrant D:/ProgramFile/Vagrant/embedded/gems/2.2.5/gems/vagrant-2.2.5/lib/vagrant/util/io.rb:32:in `encode’: “5” from GBK to UTF-8 (Encoding::UndefinedConversionError) 管理员权限编辑对应的 io.rb 文件，更改 vagrant 源码 https://github.com/hashicorp/vagrant/issues/9368 Ref： 多种类型虚拟机支持 使用vagrant和vitrualBox搭建虚拟开发环境 Box-Search: hyperv Vagrant使用指南: 插件、vagrant 代理、对应虚拟机代理 windows 下 使用 vagrant 来管理 linux 虚机开发环境 HyperV - Static Ip with Vagrant 征服诱人的Vagrant！ Vagrant学习文档 VAGRANT 网络配置 ifconfig命令和ip命令及route命令： 配置公有网络设定 IP、掩码、网关","tags":[{"name":"工具","slug":"工具","permalink":"http://example.com/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"HBase","date":"2021-03-08T23:48:46.000Z","path":"2021/03/09/HBase/","text":"Hbase概述 基于 Google 的 BigTable 论⽂⽽来， 一个高可靠性、高性能、面向列、非关系型、可伸缩、支持海量存储的分布式存储系统。提供超大规模数据集的实时随机读写。 整体架构 Zookeeper： 实现 HMaster 的⾼可⽤，对 RegionServer 监控，元数据的入口和集群配置的维护。 HMaster（Master）： 为 HRegionServer 分配 Region 分配 region，进行 RegionServer 的负载均衡，失效 Region 的重新分配 HRegionServer（RegionServer）： 处理来自 Client 的读写请求，负责与底层 HDFS 交互，数据存放到 HDFS，处理 Region 变大后的拆分，负责 Storefile 的合并 Region：多个 Store 构成。一个 Store 对应 Storefile、MemStore 组成。一个 Store 对应 HBase 表的一个列族。 MemStore： StoreFile(HFile)： 磁盘上保存原属数据的实际物理文件，StoreFile 以 Hfile 的形式存储在 HDFS 中。 Client： 访问 HBase 接口，维护对应的 cache 来加速 HBase 访问，如 cache 的 .META 元数据信息 HDFS: HDFS 为 Hbase 提供可靠的底层数据存储服务 MapReduce: MapReduce 为 HBase 提供高性能的计算能力 HBase 特性HBase 是一个通过大量廉价的机器解决海量数据的高速存储和读取的分布式数据库解决方案。 必须借助 RowKey 进行大数据量的存取 特性： 数据的多版本：HBase 表中的数据有多个版本值，默认情况下是根据版本号去区分，版本号就是插⼊数据的时间戳 数据类型单⼀：所有的数据在 HBase 中是以字节数组进⾏存储 HBase 适合海量明细数据的存储，并且后期需要有很好的查询性能（单表超千万、上亿，且并发要求⾼） 列式存储：根据列族存储，列族在建表的时候必须指定。 易于扩展：基于上层处理能力(RegionServer) 的扩展，基于存储的扩展(HDFS)。 列缺失的情况下关系型数据库存在存储空间浪费问题。 数据模型逻辑存储 列族： 可稀疏存储。建表时指定。写入文件时以列族存储。 Region： 表的若干行组成，不能跨 RegionServer。 Store： 物理存储 TimeStamp: 数据的版本。 Type： RowKey： 每行数据的主键。 Column Family： 列族。 Cell： 每个版本一个 Cell。 Region：表的分区。 Hbase 原理读流程 相关组件内容： BlockCache： 查询时先到 BlockCache 中找，再到 StoreFile 中读取。从 StoreFile 读取后，将数据写入 BlockCache，之后返回结果给 Client。为读缓存。缓存为 RowKey 级别的。 MetaCache： Client 为提高查询速度缓存元数据(ZooKeeper 中的数据)。 MemStore： 每个 Store 持有一个。 读取数据的过程中 HMaster 未参与。 HBase 读取流程： 1）⾸先从 zk 找到 meta 表的 region 位置，然后读取 meta 表中的数据，meta 表中存储了用户表的 region 信息。 2）根据要查询的 namespace、表名和 rowkey 信息。找到写入数据对应的 region 信息 3）找到这个 region 对应的 regionServer，然后发送请求 4）查找对应的 region 5）先从 memstore 查找数据，若不存在，再从 BlockCache 上读取 HBase 上 Regionserver 的内存分为两个部分 部分作为Memstore，主要⽤用来写； 另外⼀一部分作为BlockCache，主要⽤用于读数据； 6）如果 BlockCache 中也没有找到，再到 StoreFile 上进行读取 从 storeFile 中读取到数据之后，不是直接把结果数据返回给 Client，而是把数据先写⼊到B lockCache 中，⽬的是为了加快后续的查询；然后在返回结果给 Client。 写流程 相关的组件： WAL： Write ahead log，预写入日志。 MemStore： 写缓存。flush 刷盘 HBase 的写入性能比读取性能好。 HBase 写入数据流程： 1）⾸先从 zk 找到 meta 表的 region 位置，然后读取 meta 表中的数据，meta 表中存储了了⽤户表的 region 信息 2）根据 namespace、表名和 rowkey 信息。找到写入数据对应的 region 信息 3）找到这个 region 对应的 regionServer，然后发送请求 4）把数据分别写到 HLog（write ahead log）和 Memstore 各一份 5）Memstore 达到阈值后把数据刷到磁盘，⽣成 storeFile 文件 6）删除 HLog 中的历史数据 HLog 与 MemStore 各写一份，flush 机制，Compact 机制 HBase Flush 机制当 MemStore 的大小超过指定值得时候，需要进行 Flush。 默认为 100M。 12# 手动 Flushflush &lt;table-name&gt; 配置 hbase.hregion.memstore.flush.size： 当 memstore 的⼤小超过这个值的时候，会 flush 到磁盘,默认为 128M hbase.regionserver.optionalcacheflushinterval： 当 memstore 中的数据时间超过 1 小时，会 flush 到磁盘 hbase.regionserver.global.memstore.size： HregionServer 的全局 memstore 的⼤小，超过该⼤小会触发 flush 到磁盘的操作,默认是堆⼤小的 40% HBase 阻塞机制触发机制： (1) memstore 中数据达到 512MB 计算公式 1hbase.hregion.memstore.flush.size*hbase.hregion.memstore..block.multiplier (2) RegionServer 全部 memstore 达到规定值 配置参数 hbase.hregion.memstore.flush.size： hbase.hregion.memstore.block.multiplier： Compact 合并机制1major_compact &lt;table-name&gt; Minor compact 小合并： 多个 HFile(StoreFIle) 合并为一个 HFile。 删除和更新知识做标记，为物理删除，触发频率高。 触发机制： (1) MemStore Flush (2) 定期检查线程调用，默认 10s Major Compact 大合并 合并 Store 中所有的 HFile 为一个 HFile。 会真正移除删除标记的数据。触发频率低。默认 7 天执行。一般手动控制执行，防止业务高峰。 配置参数 hbase.hstore.compaction.min： 待合并⽂文件数据必须⼤大于等于该值 hbase.hstore.compaction.max： 待合并⽂文件数据必须⼩小于等于该值，默认 10 hbase.hstore.compaction.min.size： ⽂件⼤⼩小于该值的 store file 一定会加入到 minor compaction的store file中 hbase.hstore.compaction.max.size：⽂件⼤⼩⼤于该值的 store file 一定会被 minor compaction 排除，默认值为 LONG.MAX_VALU hbase.server.thread.wakefrequency： 周期性检查是否需要 compaction 操作，，默认值是 10000 millseconds hbase.hregion.majorcompaction： 默认值为 7 天进⾏一次⼤合并 Region 拆分机制 拆分策略 (1) ConstantSizeRegionSplitPolicy 0.94- 默认切分 (2) IncreasingToUpperBoundRegionSplitPolity 1regioncount ^3 * 128M * 2 (3) SteppingSplitPolity 2.0 默认的策略。和待分裂 region 所属表在当前 regionserver 上的 region 个数有关 (4) KeyPrefixRegionSpliPolicy (5) DelimitedKeyPrefixRegionSPlitPolicy (6) DisabledRegionSplitPolicy 不启用切分。 指定策略 (1) 全局指定 (2) 通过 Java API 指定策略 (3) 通过 Hbase SHell 指定策略 hbase.regionserver.region.split.policy： 指定拆分策略，如org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy Region 合并 仅用于维护的目的。 冷合并：不需要启动 HBase 集群。对 RowKey 进行合并。 热合并： 在线进行合并。 Hbase 原理HBase 表的预分区(Region)表被创建时候，默认分配一个 Region 给 Table。此时读写都到同一个 RegionServer 的同一个 Region，无法 Balance。 预分区的好处： 增加数据读写效率 负载均衡，防⽌止数据倾斜 ⽅便集群容灾调度 region 协处理器 Observer 协处理器，将业务运算代码封装到 Coprocessor 中，并在 RegionServer 上运行，即在数据实际存储位置执行，最后将运算结果返回给 Client。 默认访问 HBase 使用 scan / get 获取数据，对数据进行业务运算，数据量大的情况下，会出现性能问题。 类似数据库中的触发器和存储过程、Hadoop 中的 MapReduce、Spring 中的 AOP。 Observer Coprocessor： 类似触发器，可以在一些事件(Get, Scan) 发生前后执行指定的代码。 Endpoint Coprocessor： 类似存储过程，在 RegionServer 上直接存储的数据计算。 (1) OBSERVER 与触发器类似 RegionObserver：⽤户可以⽤这种的处理器处理数据修改事件，它们与表的 region 联系紧密。 MasterObserver：可以被⽤作管理或 DDL 类型的操作，这些是集群级事件。 WALObserver：提供控制 WAL 的钩⼦函数 HBase 的一种二级索引通过此种方式实现。 (2) Endpoint 类似存储过程，在 RegionServer 中执行代码 常见用途： 聚合操作。普通操作为全表扫描。 Endpoint Coprocessor 借助 phoenix 框架容易实现。针对 HBase 数据集进行聚合运算直接使用 SQL 语句。 初始化表 12create &#39;t1&#39;,&#39;info&#39;create &#39;t2&#39;,&#39;info&#39; 安装处理器 1234567hdfs dfs -mkdir -p /processorhdfs dfs -put processor.jar /processor# HBasedescribe &#x27;t1&#x27;alter &#x27;t1&#x27;,METHOD =&gt; &#x27;table_att&#x27;,&#x27;Coprocessor&#x27;=&gt;&#x27;hdfs://linux121:9000/processor/processor.jar|com.janhen.bigdata.hbase.processor.MyProcessor|1001|&#x27;describe &#x27;t1&#x27; 验证处理器 1put &#x27;t1&#x27;,&#x27;rk1&#x27;,&#x27;info:name&#x27;,&#x27;lisi&#x27; 卸载处理器 123disable &#x27;t1&#x27;alter &#x27;t1&#x27;,METHOD=&gt;&#x27;table_att_unset&#x27;,NAME=&gt;&#x27;coprocessor$1&#x27;enable &#x27;t2&#x27; RowKey 设计RowKey 的设计会直接影响 HBase 的使用效率 HBase 是三维有序存储的，通过 rowkey, column key(column family 和 qualifier) 和 timestamp 这三个维度对 HBase 中的数据进行快速定位。 HBase 的查询方式 get 方式，指定 rowkey 获取唯一一条数据 scan 方式，设置 startRow 和 stopRow 参数进行范围匹配 全表扫描: 直接扫描整个表中的所有记录 RowKey 是一个二进制码流(byte[])，最大长度 64Kb，实际一般为 10 - 100 bytes，一般设置为定长的。 设计良好的数据访问模式可以使集群被充分、均衡的利用。 基本原则 (1) 长度原则： 建议越短越好，不要超过 16 字节。 HBase 持久化文件 HFile 中按照 KeyValue 存储，RowKey 过长，占用空间内大，影响 HFile 的存储效率。 MemStore 将缓存部分数据到内存，若 rowkey 字段过长，内存的有效利用率降低，系统不可缓存更多的数据，降低检索效率。 当前 OS 都为 64 位，内存 8 字节对齐，控制在 16 字节，利用了 OS 的最佳特性。 (2) 散列原则： 如果 RowKey 按照时间戳方式递增，不要将时间戳放到二进制码前面，将 RowKey 的高位作为散列字段，由程序随机生成，低位放时间字段，提高数据负载均衡分布到每个 RegionServer，实现负载均衡的机率。 若没有散列字段，首字段直接是时间信息，所有数据集中在一个 RegionServer 上，在数据检索时负载会集中在个别的 RegionServer 上，造成热点问题，降低查询效率。 (3) 唯一原则： 设计时保证唯一性，RowKey 是按照字典顺序排序存储的，设计的时候可利用这个排序的特点，将经常读取的数据存储到一块、最近可能被访问的数据放到一块实现的 HBase 中快速的读写 表热点 大量的 Client 直接访问集群中的一个或少数几个节点，造成少数region server 的读/写请求过多、负载过大，⽽其他 region server 负载却很小。 造成热点 Region 所在的单个机器超出自身承受能力，引起性能下降甚至 Region 不可用，会影响同一个 RegionServer 上其他的 Region，由于主机无法服务其他 Region 的请求。 糟糕的 RowKey 是热点的源头 处理表的热点方案： (1) 预分区：避免前期读写都在一个 RegionServer (2) 加盐：非密码学上的加盐，而是在 RowKey 前增加一个随机数。操作起来较为复杂。 (3) 哈希：较为简单。使同⼀一⾏行行永远⽤用⼀一个前缀加盐。 (4) 反转：反转固定长度或者数字的 RowKey，牺牲了 RowKey 的有序性。一般用字符串进行反转，如手机号的反转、时间戳的反转 HBase 的二级索引本质就是建⽴ hbase 表中列与行键之间的映射关系。 借助其他的开源 常见二级索引 Phoenix / solr / es… 布隆过滤器HBase 底层中使用，判断某个元素是否存在。 RefApache HBase ™ Reference Guide HBase 文档","tags":[{"name":"HBase","slug":"HBase","permalink":"http://example.com/tags/HBase/"}]},{"title":"Guava使用","date":"2021-03-08T17:24:34.000Z","path":"2021/03/09/Guava使用/","text":"Guava工具Joiner 字符拼接 1、Joiner | JDK8 Stream 字符拼接常见操作： 不带空元素的拼接 对空元素使用默认值拼接 写入文件 12345678910111213141516String result=Joiner.on(&quot;#&quot;).join(stringList);String result =Joiner.on(&quot;#&quot;).userorNull(&quot;DEAULT&quot;).join(stringListwithNull);final StringBuilder builder=new StringBuilder();StringBuilder resultBuilder=Joiner.on(&quot;#&quot;) .useForNull(&quot;DEFAULT&quot;) .appendTo(builder, stringListwithNullval);// Writertry (Filewriter writer = new Filewriter(new File(targetFileName))) &#123; Joiner.on(&quot;#&quot;) .useForNull(&quot;DEFAULT&quot;) .appendTo(writer, stringListwithNullValue);&#125; catch (IOException e) &#123; fail(&quot;append to the writer occur fetal error.&quot;);&#125; JDK8 中的 Join 中的 Stream：跳过 Null 值 12345678910String result=stringListwithNullValue.stream() .filter(item-&gt;item!=null &amp;&amp; !item.isEmpty()).collect(joining(&quot;#&quot;));// null to defalutString result=stringListwithNullValue.stream() .map(item-&gt;item==null || item.isEmpty()?&quot;DEFAULT&quot;:item).collect(joining(&quot;#&quot;));// function inferpublic void testJoiningByStreamwithDefaultyalue()&#123;string result =stringListwithNullValue.stream() .map(this::defaultValue) .collect(joining(&quot;#&quot;)); 2、Map 拼接 将 Map 的键值按照特定分隔符进行拼接 默认 key 和 val 通过 = 进行拼接 12345678910// join mapprivate final Map&lt;String,string&gt; stringMap = of(&quot;Hello&quot;,&quot;Guaga&quot;,&quot;Java&quot;,&quot;Scala&quot;);assertThat(Joiner.on(&#x27;#&quot;).withKeyValueSeparator(&quot;=&quot;).join(stringMap), equalTo(&quot;Hello=Guava#Java=Scala&quot;));try(FileWriter writer=new Filewriter(new File(targetFileNameToMap)))&#123; Joiner.on(&quot;#&quot;).withKeyValueSeparator(&quot;=&quot;).appendTo(writer, stringMap); assertThat(Files.isFile().test(new File(targetFileNameToMap)), equalTo(true));&#125; catch (IOException e)&#123; fail(&quot;append to the writer occur fetal error.&quot;);&#125; Splitter 字符分隔 剔除一些无用的 按照特定分隔符分隔并转化成 List 剔除掉空的字符并转化成 List 指定分隔符 剔除 Blank 字符 剔除剔除 Empty 字符 收集结果成为 List 1、分隔与忽略 12345678// 1.1 普通分隔List&lt;String&gt;result=Splitter.on(&quot;|&quot;).splitToList(&quot;hellolworld&quot;);// 1.2 忽略空串List&lt;string&gt;result=Splitter.on(&quot;|&quot;).omitEmptystrings().splitroList(&quot;hellolworldlll&quot;);// 1.3 忽略空格并忽略空串result=Splitter.on(&quot;I&quot;).trimResults().omitEmptystrings().splitroList(&quot;hel1o I worldlll&quot;); 2、正则分隔 使用 onPattern 替代 on, 参数即为正则 12345// 3.1 传入正则表达式进行分隔List&lt;String&gt;result=Splitter.onPattern(&quot;\\\\\\\\\\\\\\\\l&quot;).trimResults().omitEmptystrings().splitTorist(&quot;hello | worldl&quot;);// 3.2 传入正则PatternList&lt;string&gt;result =Splitter.on(Pattern.compile(&quot;\\\\\\\\l&quot;)).trimResults(). omitEmptystrings().splitToList(&quot;&quot;); 3、结果分隔 通过字符长度截取字符串 截取结果中固定的个数，最后的保存剩余所有的 1234567891011121314// 2.1 分隔固定长度，报文的固定长度截取 aaabbbcccList&lt;string&gt; result = Splitter.fixedtength(4).splitToList(&quot;aaaabbbbccccdddd&quot;);// 2.2 限制返回的结果数，多余的放在最后一个结果中List&lt;String&gt;result=Splitter.on(&quot;#&quot;) .limit(3) .splitToList(&quot;hello# world# java# google# scala&quot;);// 3.3 传入正则Pattern, 返回 MapMap&lt;String, String&gt; result = Splitter.on(Pattern.compile(&quot;\\\\\\\\\\\\\\\\l&quot;)) .trimResults() .omitEmptystrings() .withKeyValueSeparator(&quot;=&quot;) .split(&quot;hello=HELLOl|world=WORLD|||&quot;); String 其他工具 1、Strings 填充字符串 获取公共前缀|后缀 重复指定次数的字符串: 和 python 进行重复类似 123456789101112131415assertlhat(strings.emptytovull(&quot;&quot;) rnullvalue())assertThat(Strings.nullToEmpty(null), equalro(&quot;&quot;)); assertThat(strings.nullToEmpty(&quot;hello&quot;), equalTo(&quot;hello&quot;));assertThat(Strings.commonPrefix(&quot;Hello&quot;,&quot;Hit&quot;), equalTo(&quot;H&quot;));assertThat(Strings.commonPrefix(&quot;Hello&quot;,&quot;Xit&quot;), equalro(&quot;&quot;));assertThat(Strings.commonSuffix(&quot;Hello&quot;,&quot;Echo&quot;), equalTo(&quot;o&quot;));assertThat(Strings.repeat(&quot;Alex&quot;,3), equalTo(&quot;AlexAlexAlex&quot;));assertlhat(btrings.1sNullormpty(nulL), equallo(true))assertThat(Strings.isNullOrEmpty(&quot;&quot;), equalTo(true));assertThat(Strings.padstart(&quot;Alex&quot;,3,&#x27;H&#x27;), equalTo(&quot;Alex&quot;));assertThat(strings.padstart(&quot;Alex&quot;,5,&#x27;H&#x27;), equalTo(&quot;HAlex&quot;));assertThat(Strings.padEnd(&quot;Alex&quot;,5,&#x27;H&#x27;), equalTo(&quot;AlexH&quot;)); 2、CharSet 3、CharMatcher 123456assertThat(CharMatcher. javaDigit(). matches(&#x27;A&#x27;), equalTo(true));assertThat(CharMatcher. javaDigit(). matches(&#x27;x&#x27;), equalTo(false)); assertrhat(CharMatcher. is(&#x27;A&#x27;).countIn(&quot;Alex sharing the Google Guava to Us&quot;), equalTo(1));assertThat(CharMatcher. breakingwhitespace().collapseFrom(&quot;hello Guava &quot;,&#x27;*&quot;), equalro(&quot;* hello Guaval&quot;);assertThat(CharMatcher. javaDigit(). or(CharMatcher. whitespace()). removeFrom(&quot;hello 234 world&quot;), equalTo(&quot;helloworld&quot;);asserThat(CharMatcher. javaDigit(). or(CharMatcher. whitespace()). retainFrom(&quot;hello 234 world&quot;), equalro(&quot;234&quot;)); 类通用工具 1、MoreObjects 1、实战 toString 辅助编写： 支持忽略空值 123456public string tostring()&#123; return Moreobjects.tostringHelper(this).omitNullValues() .add(&quot;manufacturer&quot;, this.manufacturer) .add(&quot;version&quot;, this.version) .add(&quot;releaseDate&quot;, this.releaseDate).tostring();&#125; 2、源码 链表结构 12345678910111213private ToStringHelper(String className) &#123; this.holderHead = new MoreObjects.ToStringHelper.ValueHolder(); this.holderTail = this.holderHead; this.omitNullValues = false; // check and get this.className = (String)Preconditions.checkNotNull(className);&#125;private MoreObjects.ToStringHelper.ValueHolder addHolder() &#123; MoreObjects.ToStringHelper.ValueHolder valueHolder = new MoreObjects.ToStringHelper.ValueHolder(); this.holderTail = this.holderTail.next = valueHolder; return valueHolder;&#125; 2、Objects 深度比较 deepEquals hash: compare(a,b, cmp); requireNonNull template = String.valueOf(template); // null -&gt; “null” 在 JDK7+ 使用 JDK 提供的 Objects 方法替代 3、ComparisonChain 链式的比较规则 JDK8 添加类似的实现在 Comparator 上 1234567@Overridepublic int compareTo(Guava this, Guava o) &#123; return Comparisonchain.start() .compare(this.manufacturer,o.manufacturer) .compare(this.version,o.version) .compare(this.releasepate,o.releaseDate).result();&#125; 2、源码 本身为抽象类 类中持有一个实例化的对象 提供该抽象类的一个继承实现类 针对 float,double,int,long,Object 提供比较，针对 Object 提供 classis 和基于 Comparator 的比较 StopWatch 工厂方式获取，createStarted() 省去创建之后开启的步骤 可控制返回的时间单位 对象可以来回复用记录 1234LOGGER.info(&quot;start process the order [&#123;&#125;]&quot;, orderNo);Stopwatch stopwatch = Stopwatch.createStarted();TimeUnit.MILLISECONDS.sleep(100);LOGGER.info(&quot;The orderNo [&#123;&#125;] process successful and elapsed [&#123;&#125;] min.&quot;, orderNo, stopwatch.stop().elapsed(TimeUnit.MINUTES)); PreConditons 运行时空判断 判断并可给出 message，默认通过 String 自带的格式化字符串实现 通过方法名空值语义： checkState: 判断状态 checkElementIndex: 判断容器的索引 1234Preconditions.checkNotNull(list,&quot;The list should not be null and the size must be %s&quot;,2);Preconditions.checkArgument(type.equals(&quot;B&quot;));Preconditions.checkState(state.equals(&quot;B&quot;), &quot;The state is illegal.&quot;);Preconditions.checkElementIndex(10, list.size()); CollectionsFluentIterable 类似网络中的数据流处理 1、将其当做链表操作： 链表的头结点、尾节点获取 两条链表的合并 链表中是否含有满足特定条件的值 链表中是否全部满足特定条件 链表中满足特定条件的第一个值 将链表按照特定的长度进行分割 从数组中构建链表 从迭代器中构建链表 链表的循环读取，先连接成环，之后扫描限定个数的节点 链表元素的转换(transform)，通过 Function 接口实现 链表元素进行转换，单个元素转化后是一个 迭代器，重新进行连接 1234FluentIterable&lt;String&gt; fit = build();boolean result = fit.allMatch(e -&gt; e != null &amp;&amp; e.length() &gt;= 4);result = fit.anyMatch(e -&gt; e != null &amp;&amp; e.length() == 5);Optional&lt;String&gt; optional = fit.firstMatch(e -&gt; e != null &amp;&amp; e.length() == 5); Lists | Sets List 构造 通过工厂提供方便的构造方式, 同时语义明确 主要提供三种 List 的构造： ArrayList 原始方式 提供根据迭代器构造 提供根据可变数组构造 提供容量构造，包含限定长度、给出期望长度 LinkedList 构造仅提供两种方式： 底层链表无法指定大小 原始方式 迭代器构造 CopyOnWriteArrayList： 构造方式同 LinkedList Set 构造 HashSet: 方式和 ArrayList 方式类似 LinkedHashSet： 方式与 LinkedList 类似 增加限定容量，底层基于数组。。。 TreeSet： 普通构建，(), (comparator) 传入迭代器 EnumSet： Set 集合性质(&amp;) 笛卡尔积 组合选取 两个集合不同的部分， difference(set1, set2) 返回在 set1 中的元素而不再 set2 中的元组 交集、并集 MultiSet 记录重复元素的个数 Maps | BiMap | MultiMap 构造方式 转换成不可变Map 根据 Set 转变成可变 Map 根据 Map 转换对应的值 BiMap 严格的一对一映射，通过接口声明通用的操作 Table | Range ArrayTable TreeBaseTable HashBaseTable ImmutableTable 表名、列明、列值 Range 提供实现了自然排序类的范围： &gt;= &lt;=, [) 范围映射，Key 为一个给定的范围，放入的为泛型 K 的一个 Range，支持按照某个具体的值获取到对应范围端的一个值 Range | RangeMap | Ording Sorted 判断是否已经按照自然排序完成 给出相反的比较。。。 Getting Started with Google Guava.pdf 不可变设计： 提供工厂方式， of 提供构建器方式 CacheBuilder.newBuilder().build() 提供原型方式构建 copyOf Range 存放在其中的元素必须是 Comparable 的 提供多种符合语义的工厂构造： 根据枚举值确定两端的开闭 RangeMap 另一种存在有序性的 Map Ording 支持对 null 的特殊处理，将其放在第一位或者最后一位 与当前 JDK8 中 Comparator 中的 nullFirst 一致，配合 thenXXX 进行控制比较规则 Guava 缓存原始缓存实现 1、LinkedHashMap 通过 JDK 自带的实现 2、LinkedList 借助 LinkedList 实现 3、通过 SoftReference 实现 容量： 初始容量、最大容量 过期策略： 设置的大小、按照权重、访问、写入、更新、GC 并发等级： 并发使用 KV收集机制： 软引用、弱引用进行 GC 过期策略(5) 1、大小 限定缓存的个数，内存大小 2、LRU 限定大小的情况下，模式通过 LRU 算法进行淘汰 3、Time(W,R) 在访问命中某个元素多长时间后过期； 在更新某个元素多长时间后过期； 4、引用生命周期控制(GC) .softValues() .softKeys() 借助 GC 控制何时回收，何时保留 5、权重 通过为每个缓存的数据设定一定的权重进行控制是否过期 通过实体，给出计算规则： 入参为 key, values，返回的值为对应的权重 其他特性(4) 1、不存在的默认值(LoadingCache) 在获取不到的情况下给出的值 类似缓存雪崩情况下的熔断 通过 CacheLoader 抽象类实现： 给类提供根据 Function，Supplier 进行创建的工厂 是 LoadingCache 实现必须要指定的参数 12CacheLoader&lt;K, V&gt; from(Function&lt;K, V&gt; function)CacheLoader&lt;Object, V&gt; from(Supplier&lt;V&gt; supplier) 2、统计功能 .recordStats() 记录缓存的命中率，执行情况 3、监听缓存移除事件 可设置缓存移除监听器，监听到删除事件进行处理 封装成通知实体控制移除策略 1void onRemoval(RemovalNotification&lt;K, V&gt; notification); 4、灵活的构造方式 支持根据特定的字符串构造 CacheBuilder： 类似读取配置文件实现缓存 根据函数式接口构造 CacheLoader 1234String spec = &quot;maximumSize=5,recordStats&quot;;CacheBuilderSpec builderSpec = CacheBuilderSpec.parse(spec);CacheLoader&lt;String, String&gt; loader = CacheLoader.from(String::toUpperCase);LoadingCache&lt;String, String&gt; cache = CacheBuilder.from(builderSpec).build(loader); 源码分析 通过 ConcurrentHashMap 实现 Entry 中各种引用的获得： 有限的几个 每个实现都不同 有一定的规律(存在是否) ⇒ 定义Enum, 将所有Enum放入数组中，定义工厂获取方法，按照将boolean作为参数进行掩码控制获得 EventBus消息的 pull 和 push 进程级别，内部 Listener 对其进行 subscribe 只能有一个参数： 实战 1、多种类型 同种类型的多个订阅方法会被调用 2、Listener 继承特性 都会被调用 3、Event继承 都会被调用 4、异步总线 设计 自己实现一个消息总线，实现与 Guava 中 EventBus 类似的功能 并发Monitor LockCondition 的一个封装 可替换 synchronized 的实现，语义更加明确，同时更加方便编程 1、阻塞队列设计 (1) synchronized 方式设计 (2) Reentrant + Condition 方式设计 (3) Monitor 方式设计 限流 rateLimiter Semaphore 通过 Semaphore 进行获取并释放令牌的方式进行控制访问的速率 令牌桶 Refhttps://github.com/google/guava/wiki Github Guava Wiki","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"Guava","slug":"Guava","permalink":"http://example.com/tags/Guava/"}]},{"title":"Java 并发学习笔记","date":"2020-12-17T14:53:30.000Z","path":"2020/12/17/Java-并发学习笔记/","text":"线程 Java 代码首先会编译成Java字节码，字节码被类加载器加载到JVM里，JVM执行字节码，最终需要转化为汇编指令在CPU上进行执行。 Java中所使用的并发机制依赖于 JVM 的实现和 CPU 的指令。 创建方式(1) extends Thread 当调用 start() 方法启动一个线程时，虚拟机会将该线程放入就绪队列中等待被调度，当一个线程被调度时会执行该线程的 run() 方法。 12MyThread t1 = new MyThread();t1.start(); (2) Runnable (3) Callable 与 Runnable 相比，Callable 可以有返回值，且可以跑出异常，返回值通过 FutureTask 进行封装。 (4) ThreadPool 继承与实现接口的比较 优先实现接口 ① Thread 只能够通过单继承来实现； ② Thread 创建开销大，Runnable 创建的开销小； ③ Runnable 实现解耦； 可以配合线程池使用 生命周期 12345678public enum State &#123; NEW, RUNNABLE, BLOCKED, WAITING, TIMED_WAITING, TERMINATED;&#125; (1) 新建(NEW) 创建后尚未启动。 当程序使用 new 关键字 创建了一个线程之后，该线程就处于新建状态，此时仅由JVM为其分配内存，并初始化其成员变量的值。 (2) 可运行(Runnable)可能正在运行，也可能正在等待 CPU 时间片。 包含了操作系统线程状态中的 Running 和 Ready。 当线程对象调用了start()方法之后，该线程处于就绪状态。Java虚拟机会为其创建方法调用栈和程序计数器，等待调度运行。 (3) 阻塞(Blocked)等待获取一个排它锁，如果其线程释放了锁就会结束此状态。 主要分为三种阻塞方式： ① 同步阻塞： 等待获取锁，获取同步锁时该同步锁被别的线程占用，JVM 将线程放入到锁池 (lock pool) 中。 ② 等待阻塞 执行 o.wait() ，JVM会把该线程放入等待队列(waitting queue)中。 ③ 其他阻塞 I/O 阻塞： 等待 I/O 操作完成； 执行 Thread.sleep() ； 执行 t.join() 方法； (4) 无限期等待(Waiting)等待其它线程显式地唤醒，否则不会被分配 CPU 时间片； 都是交互性质的方法； Object.wait()、Thread.join()、LockSupport.part() 进入方法 退出方法 没有设置 Timeout 参数的 Object.wait() 方法 Object.notify() / Object.notifyAll() 没有设置 Timeout 参数的 Thread.join() 方法 被调用的线程执行完毕 LockSupport.park() 方法 LockSupport.unpark(Thread) (5) 限期等待(Timed Waiting) 无需等待其它线程显式地唤醒，在一定时间之后会被系统自动唤醒。 调用 Thread.sleep() 方法使线程进入限期等待状态时，常常用“使一个线程睡眠”进行描述。 调用 Object.wait() 方法使线程进入限期等待或者无限期等待时，常常用“挂起一个线程”进行描述。 睡眠和挂起是用来描述行为，而阻塞和等待用来描述状态。 阻塞和等待的区别在于，阻塞是被动的，它是在等待获取一个排它锁。而等待是主动的，通过调用 Thread.sleep() 和 Object.wait() 等方法进入。 进入方法 退出方法 Thread.sleep() 方法 时间结束 设置了 Timeout 参数的 Object.wait() 方法 时间结束 / Object.notify() / Object.notifyAll() 设置了 Timeout 参数的 Thread.join() 方法 时间结束 / 被调用的线程执行完毕 LockSupport.parkNanos() 方法 LockSupport.unpark(Thread) LockSupport.parkUntil() 方法 LockSupport.unpark(Thread) (6) 死亡(Terminated) 可以是线程结束任务之后自己结束，或者产生了异常而结束。 线程的终止(1) 正常终止 运行结束，正常终止； (2) 退出标志 定义了一个退出标志exit，当exit为true时，while循环退出，exit的默认值为false.在定义exit时，使用了一个Java关键字volatile，这个关键字的目的是使exit同步，也就是说在同一时刻只能由一个线程来修改exit的值。 volatile 无锁同步的应用场景之一； 123456class MyThread extends Thread &#123; public volatile boolean exit = false; // volatile public void run() &#123; // ... &#125;&#125; (3) Interrupt 方法结束 ① 阻塞下的结束 在线程处于阻塞状态下，调用 interrupt() 会抛出 InterrupteException，一定要先捕获InterruptedException异常之后通过break来跳出循环，才能正常结束run方法。 死循环中的退出，只有在捕获后进行显示的 break 才能实现； ② 未阻塞下的结束 使用 isInterrupted() 判断线程的中断标志来退出循环。当使用 interrupt() 方法时，中断标志就会置 true，和使用自定义的标志来控制循环是一样的道理。 1234567891011class MyThread implements Runnable &#123; public void run() &#123; while (!isInterrupted()) &#123; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; break; // NOTE: after catch exception must break to skip loop &#125; &#125; &#125;&#125; (4) stop 方法终止 程序中可以直接使用thread.stop()来强行终止线程，但是stop方法是很危险的，就象突然关闭计算机电源，而不是按正常程序关机一样，可能会产生不可预料的结果，不安全主要是：thread.stop()调用之后，创建子线程的线程就会抛出ThreadDeatherror的错误，并且会释放子线程所持有的所有锁。一般任何进行加锁的代码块，都是为了保护数据的一致性，如果在调用thread.stop()后 导致了该线程所持有的所有锁的突然释放(不可控制) ，那么被保护数据就有可能呈现不一致性，其他线程在使用这些被破坏的数据时，有可能导致一些很奇怪的应用程序错误。因此，并不推荐使用stop方法来终止线程。 (5) Callable 通过 Future.camcel 来进行终止 Interrupt()一个线程执行完毕之后会自动结束，如果在运行过程中发生异常也会提前结束。 (1) InterruptedException 该线程处于阻塞、限期等待或者无限期等待状态，那么就会抛出 InterruptedException，从而提前结束该线程。但是不能中断 I/O 阻塞和 synchronized 锁阻塞。 Thread.sleep()； synchronized； join()； (2) interrupted() 无限循环不跳出，只有在该循环中执行 sleep() 等会抛出 InterruptedException 操作， 可通过其返回值来防止无线循环，作为一种退出标志； 调用 interrupt() 方法会设置线程的中断标记，此时调用 interrupted() 方法会返回 true。因此可以在循环体中使用 interrupted() 方法来判断线程是否处于中断状态，从而提前结束线程。 (3) Executor 的中断操作 ① 关闭池子操作 shutdown() 方法会等待线程都执行完毕之后再关闭，但是如果调用的是 shutdownNow() 方法，则相当于调用每个线程的 interrupt() 方法。 ② 关闭指定的线程(Future) 只想中断 Executor 中的一个线程，可以通过使用 submit() 方法来提交一个线程，它会返回一个 Future&lt;?&gt; 对象，通过调用该对象的 cancel(true) 方法就可以中断线程。 方法(1) join 进行线程之间的流程控制，进行线程通信的一种方式； (2) yield() 让出当前 CPU，之后重新进行竞争； (3) sleep() 不释放锁，在等待一定时间后自动唤醒； sleep() 与 wait() 的区别 想到对应在阻塞队列中、以及延时双删策略中的场景； ① 设计|定义： sleep() 是 Thread 的静态方法，wait() 是 Object 的成员方法； ② 锁的占用： sleep() 导致程序暂停执行指定的时间，它的监控状态依然保持着，不释放锁， 而 wait() 释放对象锁，进入等待此对象的等待池中； ③ 使用范围： sleep() 可以用在任何地方， wait() 只能够用在同步控制方法或同步控制块中使用； ④ 唤醒方式： sleep() 给定时间内自动唤醒，wait() 需要调用 notify 显视唤醒； start() 与 run() 的区别 start() 方法来启动线程，真正实现了多线程运行。这时无需等待 run 方法体代码执行完毕，可以直接继续执行下面的代码。 通过调用 Thread 类的 start() 方法来启动一个线程， 这时此线程是处于就绪状态， 并没有运行。 run ⽅法只是 thread 的⼀个普通 ⽅法调⽤，直接运行。 方法 run() 称为线程体，它包含了要执行的这个线程的内容，线程就进入了运行状态，开始运行 run 函数当中的代码。 Run方法运行结束， 此线程终止。然后CPU再调度其它线程。 其他性质 (1) 进程与线程的比较 进程是 OS 资源分配的单位，有自己独立的寻址空间； 线程是 OS 独立运行的单元，其共享同一个进程内的所有数据； 线程相较于进程更加轻量； ⼀个进程中可以有多个线程，多个线程共享进程的堆和⽅法区 (JDK1.8 之后的元空间)资源，但是每个线程有⾃⼰的程序计数器、虚拟机栈 和 本地⽅法栈。 (2) 实现多线程的方式 Java 中通过将每个线程映射为一个进程实现的； 线程的实现3种模型:内核线程；用户线程；两者结合。 (3) 守护线程 是个服务线程，准确地来说就是服务其他的线程，这是它的作用——而其他的线程只有一种，那就是用户线程。所以java里线程分2种。 ① 停止执行情况 专门用于服务其他的线程，如果其他的线程(即用户自定义线程)都执行完毕，连main线程也执行完毕，那么jvm就会退出(即停止运行)——此时，连jvm都停止运行了，守护线程当然也就停止执行了。 ② 优先级 优先级较低 ③ 设置 通过 setDaemon(true) 在 Thread 未 start() 之前显视设置 Daemon 线程产生的新线程也是 Daemon 的 ④ 性质 为 JVM 级别的线程，即使你停止了Web应用，这个线程依旧是活跃的。 (4) 线程派生的联系 继承对应的优先级、daemon等属性； (5) 并发和并行 并发： 同⼀时间段，多个任务都在执⾏ (单位时间内不⼀定同时执⾏)； 并⾏： 单位时间内，多个任务同时执⾏。 线程间通信while 循环监测 线程B是一直执行着while(true) 循环的，直到长度为5才终止执行，显然这种方式是很消耗资源的。所以，就需要一种机制能避免上述的操作又能实现多个线程之间的通信，这就是接下来需要学习的“wait/notify线程间通信”。 通信方式(1) 进程间的通信方式 ① 管道(pipe)、有名管道(named pipe) ② 信号量(semophore) ③ 消息队列(message queue) ④ 信号(signal) ⑤ 共享内存(shared memory) ⑥ 套接字(socket) (2) 线程间的通信方式 1、锁机制： 1.1 互斥锁：提供了以排它方式阻止数据结构被并发修改的方法。 1.2 读写锁：允许多个线程同时读共享数据，而对写操作互斥。 1.3 条件变量：可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。2、信号量机制：包括无名线程信号量与有名线程信号量3、信号机制：类似于进程间的信号处理。线程间通信的主要目的是用于线程同步，所以线程没有像进程通信中用于数据交换的通信机制。 等待/通知机制 Object.wait, notify 机制，需要配合 synchronized 一起使用 1、wait()/notify 方法 (1)wait() 和 notify() 方法要在同步块或同步方法中(synchronized 关键字) 调用，即在调用前，线程也必须获得该对象的对象级别锁。(2)wait方法是释放锁，notify方法是不释放锁的；(3)notify 每次唤醒 wait 等待状态的线程都是随机的，且每次只唤醒一个；(4)notifAll 每次唤醒 wait 等待状态的线程使之重新竞争获取对象锁，优先级最高的那个线程会最先执行；(5)当线程处于 wait() 状态时，调用线程对象的 interrupt() 方法会出现 InterruptedException 异常； 通过 等待通知模式实现阻塞队列 (1) 结构 1234Queue&lt;Object&gt; queue = new LinkedList&lt;&gt;();AtomicInteger count = new AtomicInteger();int capacity = 5;Object lock = new Object(); // use for thread communication (2) 入队 1234567891011121314void put(Object task) &#123; synchronzied(lock) &#123; while (count.get() == capacity) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; log.error(&quot;&quot;, e); &#125; &#125; queue.offer(task); count.getAndIncrement(); lock.notify(); // not empty conditon &#125;&#125; (2) 出队 12345678910111213141516Object take() &#123; Obejct oldFront = null; synchronized(lock) &#123; while (count.get() == 0) &#123; try &#123; lock.wait()； &#125; catch (Exception e) &#123; log.error(&quot;Error&quot;, e); &#125; &#125; oldFront = queue.poll(); count.getAndDecrment(); lock.notify(); // not full condition &#125; return oldFront;&#125; 应用 (1) MyBatis 中 在数据库连接这个地方使用到的：org.apache.ibatis.datasource.pooled.PooledDataSource 类中，所以不用花太多的时间去深究。1、获取连接的时候，如果数据库连接池没有空闲的连接，那么当前线程就会进入等待，直到被通知，这个地方就是popConnection()方法 *ThreadLocal 实现每⼀个线程都有⾃⼰的专属本地变量。 如果你创建了⼀个 ThreadLocal 变量，那么访问这个变量的每个线程都会有这个变量的本地副本，这也是 ThreadLocal 变量名的由来。 如果使用 ThreadLocal 管理变量，则每一个使用该变量的线程都获得该变量的副本， 副本之间相互独立，这样每一个线程都可以随意修改自己的变量副本，而不会对其他线程产生影响。 (1)概述 原理： 为每个使用该变量的线程都提供独立的变量副本，从而不会影响到其他线程所对应的副本。 是一种多线程间并发访问变量的解决方案，不使用锁来保证并发访问，本质是以空间换时间的方式，为每个线程提供变量的独立副本，以保证线程的安全。 (2) 作用 ThreadLocal 的作用是提供线程内的局部变量 ，这种变量在线程的生命周期内起作用，减少同一个线程内多个函数或者组件之间一些公共变量的传递的复杂度。 底层结构 一个Thread中只有一个ThreadLocalMap， 一个ThreadLocalMap中可以有多个ThreadLocal对象， 其中一个ThreadLocal对象对应一个ThreadLocalMap中一个的Entry实体 (也就是说：一个Thread可以依附有多个ThreadLocal对象)。 (1) Thread 中持有的结构 线程局部变量 ，那么理所当然就应该存储在自己的线程对象中 线程局部变量存储在 Thread 对象的 threadLocals 属性中 12345public class Thread implements Runnable &#123; ThreadLocal.ThreadLocalMap threadLocals = null; ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; ...&#125; (2) ThreadLocal.ThreadLocalMap 是实现 ThreadLocal 的原理，用于存储每一个线程的变量副本，Map中元素的键为线程对象，而值对应线程的变量副本。 KEY: 线程对象； VALUE: 对应线程的变量副本； 12ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; ......&#125; 一些操作 (1) ThreadLocal 4 大 public 方法 根据当前线程获取到对应的 ThreadLocalMap，借助该 Map 操作实现； get()、 set()、 remove()、 withInitial()。 最终的变量是放在了当前线程的 ThreadLocalMap 中，并不是存在 ThreadLocal 上， ThreadLocal 可以理解为只是 ThreadLocalMap 的封装，传递了变量值。 1234567891011121314151617181920212223242526272829public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125;public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings(&quot;unchecked&quot;) T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125;public void remove() &#123; ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this); &#125;public static &lt;S&gt; ThreadLocal&lt;S&gt; withInitial(Supplier&lt;? extends S&gt; supplier) &#123; return new SuppliedThreadLocal&lt;&gt;(supplier);&#125; 与同步机制的比较 a.ThreadLocal与同步机制都是为了 &lt;u&gt;解决多线程中相同变量的访问冲突问题&lt;/u&gt;。 b.前者采用以&quot;空间换时间&quot;的方法，后者采用以&quot;时间换空间&quot;的方式 对于多线程资源共享的问题，同步机制采用了“以时间换空间”的方式，而ThreadLocal采用了“以空间换时间”的方式。前者仅提供一份变量，让不同的线程排队访问，而后者为每一个线程都提供了一份变量，因此可以同时访问而互不影响。 Thread 造成的内存溢出问题 (1) 与线程池协作引发的内存溢出问题 ThreadLocal变量是维护在Thread内部的，线程不退出，对象的引用就会一直存在。 当我们使用线程池的时候，就意味着当前线程未必会退出(比如固定大小的线程池，线程总是存在的)。如果这样的话，将一些很大的对象设置到ThreadLocal中(这个很大的对象实际保存在Thread的threadLocals属性中)，这样的话就可能会出现内存溢出的情况。 一种场景就是说如果使用了线程池并且设置了固定的线程，处理一次业务的时候存放到ThreadLocalMap中一个大对象，处理另一个业务的时候，又一个线程存放到ThreadLocalMap中一个大对象，但是这个线程由于是线程池创建的他会一直存在，不会被销毁，这样的话，以前执行业务的时候存放到ThreadLocalMap中的对象可能不会被再次使用，但是由于线程不会被关闭，因此无法释放Thread 中的ThreadLocalMap对象，造成内存溢出。 也就是说，ThreadLocal在没有线程池使用的情况下，正常情况下不会存在内存泄露，但是如果使用了线程池的话，就依赖于线程池的实现，如果线程池不销毁线程的话，那么就会存在内存泄露。所以我们在使用线程池的时候，使用ThreadLocal要格外小心！ (2) 原因 ThreadLocal内存泄漏的根源是：由于 ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏 ，而不是因为弱引用。 弱引用与内存泄漏 弱引用被回收了只是回收了Entry的key引用，但是Entry应该还是存在的吧？ ThreadLocal的get(),set(),remove() 的时候都会清除线程ThreadLocalMap里所有key为null的value。 1234567static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125; 这里我们就需要重新认识一下，什么是：当垃圾收集器工作时，无论当前内存是否足够，都会回收掉只被弱引用关联的对象，这里的重点是：只被弱引用关联的对象 上述过程尽管 GC 执行了垃圾收集，但是弱引用还是可以访问到结果的，也就是没有被回收，这是因为除了一个弱引用 userWeakReference 指向了User实例对象，还有 user 指向 User 的实例对象，只有当user和User实例对象的引用断了的时候，弱引用的对象才会被真正的回收 并不是所有弱引用的对象都会在第二次GC回收的时候被回收，而是 回收掉只被弱引用关联的对象 。因此，使用弱引用的时候要注意到！希望以后在面试的时候，不要上来张口就说，弱引用在第二次执行GC之后就会被回收！ 应用场景 (1) 解决数据库连接 避免方法中总是出现 Connection 参数，每个线程每次使用的都是用一个 Connection； (2) MyBatis 中用于 Session 管理 123456789101112private static final ThreadLocal threadSession = new ThreadLocal(); public static Session getSession() throws InfrastructureException &#123; Session s = (Session) threadSession.get(); try &#123; if (s == null) &#123; s = getSessionFactory().openSession(); threadSession.set(s); &#125; &#125; catch (HibernateException ex) &#123; throw new InfrastructureException(ex); &#125; return s; &#125; *线程池(1) new Thread 弊端： 每次new Thread新建对象，Thread 为大对象，性能差 ； 线程缺乏统一管理，可能无限制的新建线程，相互竞争，有可能占用过多系统资源导致死机或 OOM； 缺少更多功能，如更多执行、定期执行、线程中断； (2) 线程池好处 重用性： 重用存在的线程，减少对象创建、消亡的开销，性能佳 ； 可控性： 可有效控制最大井发线程数，提高系统资源利用率，同时可以避免过多资源竞争，避免阻塞 ； 功能性： 提供定时执行、定期执行、单线程、井发数控制等功能； 线程池参数 corePoolSize: &lt;= x maximumPoolSize: 最大线程数 workQueue: 工作队列，为BlockingQueue threadFactory: 默认非守护，同优先级，名称 rejectHandler: BlockingQueue 满，无空闲的线程池，拒绝cel，默认直接抛出 exception keepAliveTime，unit: corePoolSize –&gt;&gt; maximumPoolSize 123456789101112131415161718192021222324public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 四种拒绝策略： ThreadPoolExecutor 类中提供 AbortPolicythrows exception DiscardPolicy： 直接丢弃 CallerRunPolicy： 使用调用者 thread 执行 DiscardOldestPolicy： 丢弃 BlockingQueue 中最靠前的 task，执行当前 task 方法 (1) 主要方法 生命周期及 ExecutorService： - execute() - submit(): execute + Future - shutdown() : handle BlockingQueue in - shutdownNow() : BlockingQueue not handler (2) 监控方法： getTaskCount()：线程池已执行和未执行的任务总数 getCompIetedTaskCount()：已完成的任务数量 getPoolSize()：线程池当前的线程数量 getActiveCount()：当前线程池中正在执行任务的线程数量 Executors① Executors.newCachedThreadPool 创建一个可根据需要创建新线程的线程池，但是在以前构造的线程可用时将重用它们。对于执行很多短期异步任务的程序而言，这些线程池通常可提高程序性能。调用 execute 将重用以前构造的线程(如果线程可用)。如果现有线程没有可用的，则创建一个新线程并添加到池中。终止并从缓存中移除那些已有 60 秒钟未被使用的线程。因此，长时间保持空闲的线程池不会使用任何资源。 ② Executors.newFixedThreadPool 创建一个可重用固定线程数的线程池，以共享的无界队列方式来运行这些线程。在任意点，在大多数 nThreads 线程会处于处理任务的活动状态。如果在所有线程处于活动状态时提交附加任务，则在有可用线程之前，附加任务将在队列中等待。如果在关闭前的执行期间由于失败而导致任何线程终止，那么一个新线程将代替它执行后续的任务(如果需要)。在某个线程被显式地关闭之前，池中的线程将一直存在。 ③ Executors.newSingleThreadExecutor Executors.newSingleThreadExecutor()返回一个线程池(这个线程池只有一个线程),这个线程池可以在线程死后(或发生异常时)重新启动一个线程来替代原来的线程继续执行下去！ ④ Executors.newScheduIedThreadPool 创建一个线程池，它可安排在给定延迟后运行命令或者定期地执行。 1ScheduledExecutorService scheduledThreadPool= Executors.newScheduledThreadPool(3); scheduledThreadPool.schedule(newRunnable()&#123; @Override public void run() &#123; System.out.println(&quot;延迟三秒&quot;); &#125; &#125;, 3, TimeUnit.SECONDS); scheduledThreadPool.scheduleAtFixedRate(newRunnable()&#123; @Override public void run() &#123; System.out.println(&quot;延迟1秒后每三秒执行一次&quot;); &#125; &#125;,1,3,TimeUnit.SECONDS); ⑤ 线程池配置 CPU 密集型任务，就需要尽量压榨 CPU，参考值可以设为 NCPU + 1IO 密集型任务，参考值可以设置为 2 * NCPU； 选用基础： - 小型应用不适合 - 线程调度时间开销大 @@计算密集型与IO密集型 密集型： CPU 核 + 1IO 密集： CPU 核数 / (1-阻塞系数) 一般0。8~0.9 @@如何正确的使用线程池 设置线程池，比设置界限； hook 机制嵌入行为，由 beforeMethod, afterMethod 记录线程执行前和后做日志，异常结果； 优雅的关闭，hook 机制，推荐使用 JavaBean 创建线程池，在 destoryMethod 里面在销毁时调用 shutdown； JMM(1) 概述 Java 并发采用的是共享内存模型，线程之间的通信总是隐式执行。 定义： Java 线程之间的通信由 JMM 控制， JMM 决定一个线程对共享变量的写入何时对另一个线程可见。 Java内存模型(JMM)解决了可见性和有序性的问题，而锁解决了原子性的问题，理想情况下我们希望做到“同步”和“互斥” 主内存与工作内存 主内存副本拷贝，非对整个obj拷贝。 Java借助共享内存实现线程间的通信 内存间的交互操作 8中操作, 主内存，保证原子性； 对于long和double的特殊规则(了) 64位 对于volatile变量的特殊规则 语义： 可见性, 实现对于其修改立即写回主内存中, 非保证原子性; 有序性, 禁止指令重排序, 是一种同步机制, 轻量, 与DCL实现安全的单例. 原子性、可见性与有序性volatile：finale: this 引用逃逸(读初始化一般的data)；synchronized: “万能”, 重量, 阻塞 硬件的效率与一致性:缓存一致性协议 关键字*volatile 能够在线程之间保持可见性，能够被 多线程同时读，并且保证不会读到过期的值，但 只能被单线程写。基于 happens-before 原则，对 volatile 字段的写入操作先于读操作，即使两个线程同时修改和获取 volatile 变量。 volatile是轻量级的synchronized，他的意思是：当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。如果volatile变量修饰符使用恰当的话，它比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度 (1) 特性 a.volatile关键字为域变量的访问提供了一种免锁机制， b.使用volatile修饰域相当于告诉虚拟机该域可能会被其他线程更新， c.因此每次使用该域就要重新计算，而不是使用寄存器中的值 d.volatile不会提供任何原子操作，它也不能用来修饰final类型的变量 volatile 保证可见性 有volatile变量修饰的共享变量进行写操作的时候会引发了两件事情：(1)将当前处理器缓存行的数据写回到系统内存；(2)这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效； 底层实现细节(了)： ① 发生 volatile W， JVM 向处理器发送 Lock 前缀的汇编命令，将该变量缓存行写到系统内存； ② 为了保证写回到的数据被其他线程立即可见，借助 缓存一致性协议 实现，每个处理器 嗅探总线 上传播的数据检查自己是否过期，过期强制从系统内存中把数据读到处理器缓存中。 volatile 如何禁止指令重排序 借助内存屏障和禁止指令重排实现 对 volatile 变量写操作时，会在写操作之后加上一条 store 屏障指令，将本地内存中的共享变量刷新到主内存； 对 volatile 变量的读操作，会在读操作之前加上一条 load 屏障指令，从主内存中读取共享变量。 使用volatile关键字修饰共享变量可以禁止重排序。若用volatile修饰共享变量，在编译时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序volatile禁止指令重排序的一些规则： 1.当第二个操作是voaltile写时，无论第一个操作是什么，都不能进行重排序 2.当地一个操作是volatile读时，不管第二个操作是什么，都不能进行重排序 3.当第一个操作是volatile写时，第二个操作是volatile读时，不能进行重排序 涉及到内存屏障(Memory Barrier)，它是让一个CPU处理单元中的内存状态对其它处理单元可见的一项技术。 一些应用 无锁读取数据： ConcurrencyHashMap 的 get 操作，通过 volatile 替换锁，AQS 中 state 变量； 作为终止标识，exit 来进行终止，类似 interrupt 终止； 那么在禁止重排序时是一个较好的使用场景，否则我们不需要再使用它，如 DCL 中通过 volatile 修饰； 热部署的变量： 通过线程修改之后立即被其他线程可见； 适用场景： (1) 对变量的写操作不依赖于当前值(比如 i++)，或者说是单纯的变量赋值(boolean flag = true)。 (2)该变量没有包含在具有其他变量的不变式中，也就是说，不同的 volatile 变量之间，不能互相依赖。只有在状态真正独立于程序内其他内容时才能使用 volatile。 synchronized字解决的是多个线程之间访问资源的同步性。 (1) 一些性质： 提供原子性，实现同步功能； 是 JVM 提供的同步工具，使用 lock 和 unlock 字节码指令，保证被它修饰 的⽅法或者代码块在任意时刻只能有⼀个线程执⾏； 与 CAS 比较： 相比于 CAS 可以保证 一块 而非一个变量的原子性； 与 Lock 比较： 相比于 JDK Lock 可以保存 程序运行信息 ，便于解决死锁和异常； (2) 使用的位置： code bloker： 显视锁住当前调用对象 synchronized(this)，通过监视器锁实现； method: 锁住调用对象，通过访问标识位实现； static method: 锁住该类的所有对象； class: 显视锁住类对象 synchronized(xxx.class)； 适用同一个对象调用锁住 obj 的可以实现同步，使其中的一个线程阻塞等待另一个线程执行完毕；不同对象调用时不适用； synchronized 保证可见性原理 通过 javap -v xxx.class 获取字节码指令分析 关键字synchronized可以修饰方法或者以同步块的形式来进行使用，它主要确保多个线程在同一个时刻，只能有一个线程处于方法或者同步块中，它保证了线程对变量访问的可见性和排他性。 JMM 对 synchronized 的规定： 线程解锁前，必须把共享变量的最新值刷新到主内存； 线程加锁时，将清空工作内存 中共享变量的值，从而使用共享变量时需要从主内存中重新读取最新的值(注意，加锁与解锁是同一把锁)； 本质是对一个对象的监视器(monitor) 进行获取，而这个获取过程是排他的，也就是说同一时刻只有一个线程获取到由 synchronized 所保护对象的监视器。 (1) 对代码块同步 monitorenter 和 monitorexit 指令 Synchronized 每个对象有一个 内置的监视器锁(monitor) 。当 monitor 被占用时就会处于锁定状态，线程执行monitorenter 指令时尝试获取monitor的所有权，过程如下：1、如果 monitor 的进入数为0，则该线程进入 monitor，然后将进入数设置为1，该线程即为 monitor 的所有者。2、如果线程己经占有该 monitor，只是重新进入，则进入monitor 的进入数加1．3．如果其他线程巳经占用了 monitor ,则该线程进入阻塞状态，直到 monitor 的进入数为0，再重新尝试获取monitor的所有权。 (2) 同步方法 调用指令将会检查方法的 ACC_SYNCHRONIZED 访问标志是否被设置，如果设置了， 执行线程将先获取 monitor，获取成功之后才能执行方法体 ，方法执行完后再释放 monitor Synchronize和lock都属于同步阻塞。 synchronized 其他性质 (1) 作为锁 见下部分锁相关 (2) 原子性 @Q: CAS 机制与 synchronized 保证原子性的区别？ ① CAS 仅能够保证一个变量的原子性，而 synchronized 可用于方法、类、字段等多处； ② CAS 通过不断循环重试实现，存在不必要的开销，但是是一种无锁的实现； ③ CAS 存在 ABA 问题； Q: JDK1.6+ 的 synchronized 关键字做了哪些优化? 对锁的实现引⼊了⼤量的优化，如偏向锁、轻量级锁、⾃旋锁、适应性⾃旋锁、锁消除、锁粗 化等技术来减少锁操作的开销。 锁主要存在四种状态，依次是：⽆锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈⽽逐渐升级。锁可以升级不可降级，提⾼获得锁和释放锁的效率。 三大特性原子性 提供了互斥访问，同一时刻只能有一个线程对它进行操作。 (1) 实现方式： 锁的同步机制： synchronized | Lock CAS 机制： 包括 AtomicInteger 等原子类 可见性 一个线程对主内存中共享变量的修改，能够及时地被其他线程观察到。 (1) 不可见的原因： 线程交叉执行 重排序结合线程交叉执行 共享变量更新后的值没有在工作内存与主存间及时更新 (2) 实现方式 volatile 关键字可以保证共享变量的可⻅性。 有序性： 代码在执⾏的过程中的先后顺序。 Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性，导致代码的执⾏顺 序未必就是编写代码时候的顺序。 volatile、synchronized、Lock *happen-before 原则 单一线程原则 Single Thread Rule 一个线程内，程序前面的操作先于后面的操作。也叫程序次序原则。 管程锁定原则 Monitor Lock Rule 一个 unlock option 先于后面同一个锁的 lock option。 volatile 变量规则 Volatile Variable Rule 对一个 volatile 变量的写操作先于后面对这个变量的读操作。 传递性原则 Transitivity A –&gt; B, B –&gt; C ==&gt;&gt; A –&gt; C 线程启动规则 Thread Start Rule丶。 入 Thread 对象的 start() 先于此线程的每一个动作 线程中断规则 Thread Interruption Rule 对线程 interrupt() 的调用先于被中断线程的代码检测到中断事件的发生，即 isInterrupt(). 线程加入规则 Thread Join Rule Thread 对象的结束先于 join() 方法返回 对象终结规则 Finalizer Rule 一个对象的初始化完成(构造函数结束)先于它的 finalize() 方法的开始 线程同步的实现说明： 需要使用线程同步的根本原因在于对普通变量的操作不是原子的。 1、 互斥同步 (1) 同步方法 、同步代码块 (2) 使用重入锁实现线程同步 (3) 使用阻塞队列实现线程同步 2、 非阻塞同步 主要是 CAS 不断尝试实现 (1) 使用原子变量实现线程同步 3、 无同步方案 (1) 使用局部变量实现线程同步 如果使用ThreadLocal管理变量，则每一个使用该变量的线程都获得该变量的副本， 副本之间相互独立，这样每一个线程都可以随意修改自己的变量副本，而不会对其他线程产生影响。 (2) 使用特殊域变量(volatile)实现线程同步 注：多线程中的非同步问题主要出现在对域的读写上，如果让域自身避免这个问题，则就不需要修改操作该域的方法。 锁与锁优化线程安全 Java语言中的线程安全绝对线程安全相对线程安全线程兼容线程对立 2. 线程安全的实现方法1)同步互斥Synchronized：存在挂起、恢复，是阻塞 实现的，且java线程直接映射到OS原生线程上的，存在用户态到内核态的_转换_，因而性能较差。 Lock：可重用锁 2)非阻塞基于CAS+Loop实现 3)无同步 可重入代码 TLC，线程本地， 是消息队列架构模式 锁乐观锁 乐观锁是一种乐观思想，即认为读多写少，遇到并发写的可能性低，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是 在更新的时候会判断一下在此期间别人有没有去更新这个数据，采取在写时先读出当前版本号，然后加锁操作 (比较跟上一次的版本号，如果一样则更新)，如果失败则要重复读-比较-写的操作。 java中的乐观锁基本都是通过CAS操作实现的，CAS是一种更新的原子操作， 比较当前值跟传入值是否一样，一样则更新，否则失败。 悲观锁 悲观锁是就是悲观思想，即认为写多，遇到并发写的可能性高，每次去拿数据的时候都认为别人会修改，所以每次在读写数据的时候都会上锁，这样别人想读写这个数据就会block直到拿到锁。java中的悲观锁就是 Synchronized,AQS框架下的锁则是先尝试cas乐观锁去获取锁，获取不到，才会转换为悲观锁，如RetreenLock。 JVM 锁优化锁有四种状态，无锁 -&gt; 偏向锁 -&gt; 轻量级锁 -&gt; 重量级锁 通过 对象头实现 (1) 原理： 基于对象头的Mark Word， 23位表示偏向的线程ID 偏向锁 偏向锁的目的是在某个线程获得锁之后，消除这个线程锁重入(CAS)的开销，看起来让这个线程得到了偏护 而偏向锁只需要在置换 ThreadID 的时候依赖一次CAS原子指令 如果一个线程获得了锁，那么锁就进入了偏向模式。当这个线程再次请求锁时，无需再做任何同步操作 (1) 设计原因 为什么会出现这种设计的方式那？这是因为根据HotSpot的作者研究，他发现 锁不仅不存在多线程竞争，而且总是由同一线程多次获得 ，为了让线程获得锁的代价更低而引入了的偏向锁这个概念。 (2) 锁的升级 在锁竞争比较激烈的场景，最有可能的情况是每次不同的线程来请求相同的锁，这样的话偏向锁就会失效，倒不如不开启这种模式，幸运的是Java虚拟机提供了参数可以让我们有选择的设置是否开启偏向锁。如果偏向锁失败，虚拟机并不会立即挂起线程，而是使用轻量级锁进行操作。 (3) 性质： 线程获取到锁之后，消除这个线程的重入开销； 1-XX:+UseBiasedLocking 轻量级锁 轻量级锁是为了在线程交替执行同步块时提高性能，而偏向锁则是在只有一个线程执行同步块时进一步提高性能。 如果偏向锁失败，虚拟机并不会立即挂起线程，而是使用轻量级锁进行操作。轻量级锁他只是简单的将对象头部作为指针，指向持有锁的线程堆栈的内部，来判断一个线程是否持有对象锁。 如果线程获得轻量级锁成功，则可以顺利进入临界区。如果轻量级锁加锁失败，则表示其他线程抢先夺到锁，那么当前线程的轻量级锁就会膨胀为重量级锁。 轻量级锁所适应的场景是 线程交替执行同步块 的情况，如果存在同一时间访问同一锁的情况，就会导致轻量级锁膨胀为重量级锁。 (1) 说明： 嵌入在线程栈中的对象使用 Displaced Mark Word 复制对象头到堆栈中，借助CAS实现同步。还是需要进行 CAS , 出现竞争时，会尝试自旋 (2) 原理： 绝大部分锁在整个同步周期内都是不存在竞争的 自旋锁(无锁) (1) 原理 自旋锁原理非常简单，如果持有锁的线程能在很短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞挂起状态，它们只需要等一等(自旋)，等持有锁的线程释放锁后即可立即获取锁，这样就避免用户线程和内核的切换的消耗。 (2) 优缺点 (3) 时间阈值 在1.6引入了适应性自旋锁，适应性自旋锁意味着自旋的时间不在是固定的了，而是 由前一次在同一个锁上的自旋时间以及锁的拥有者的状态来决定，基本认为一个线程上下文切换的时间是最佳的一个时间。 (4) 一些实现 在通过一定的自旋失败后，通常转化为加悲观锁实现，如 ConcurrentHashMap 中对于 put 在尝试 3 次失败后进行转换成对链表头进行加锁； 1-XX:+UseSpinning 自适应锁原来默认是10，现在可以实现自适应自旋 自适应，由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。 如ConcurrentHashMap的tryLock() -XX:+UseSpinning 1.7默认开启-XX:PreBlockSpin 默认为10，代表 锁消除不存在共享数据竞争，需要对其进行逃逸分析，从而减少不必要的锁。 锁粗化防止在循环中加锁，进行资源的浪费 锁的对比 程序锁优化1.减少锁粒度 将大对象(这个对象可能会被很多线程访问)，拆成小对象，大大增加并行度，降低锁竞争。降低了锁的竞争，偏向锁，轻量级锁成功率才会提高。 () 应用 ① 最最典型的减小锁粒度的案例就是ConcurrentHashMap。进而提高并发程度如将 HashMap –&gt; ConcurrentHashMap使用Segment(16)增加并行度。 2. 减少锁持有时间 只用在有线程安全要求的程序上加锁 只在有必要的时候进行同步，这样就明显减少了线程持有锁的时间，从而提高系统的性能。 12345public synchronized void syncMethod()&#123; method1(); // cost much time mutextMethod(); // 实际需要进行同步的方法 method2();&#125; 3. 锁分离如根据功能进行锁分离(1) 应用 ① ReadWriteLock，即保证了线程安全，又提高了性能。 在读多写少的情况下，通过 ReentrantReadWriteLock 替换 ReentrantLock，实现对于 Read 的不加锁实现； ② 读写分离思想可以延伸， 只要操作互不影响，锁就可以分离 。比如LinkedBlockingQueue 从头部取出，从尾部放数据。 如果使用独占锁的话，则要求两个操作在进行时首先要获取当前队列的锁，那么take和put就不是先真正的并发了，因此，在JDK中正是实现了两种不同的锁，一个是takeLock一个是putLock。 4. 锁粗化不在循环中加锁，来回加和释放的开销大 12345678910111213public void syncMethod() &#123; synchronized (lock) &#123; //第一次加锁 method1(); &#125; method3(); synchronized (lock) &#123; //第二次加锁 mutextMethod(); &#125; method4(); synchronized (lock) &#123; //第三次加锁 method2(); &#125;&#125; 如果第一次和第二次加锁和线程上下文切换的时间超过了method1()、method2()method3()、method4() 的时间. 改进后的代码的执行时间可能小于上述分别加锁的时间，这就是锁粗化，也是一种锁优化的方式，但是要根据具体的场景； 5. 锁消除 锁消除是在 编译器级别的事情。在即时编译器时，如果发现不可能被共享的对象，则可以消除这些对象的锁操作。 引发原因： ① 多数是因为程序员编码不规范引起。 ② 有时这些锁并不是程序员所写的，有的是JDK实现中就有锁的，比如Vector和StringBuffer 这样的类，它们中的很多方法都是有锁的。当我们在一些不会有线程安全的情况下使用这些类的方法时，达到某些条件时，编译器会将锁消除来提高性能。 **6. JVM 锁优化(volatile, synchronized) ** 见上部分 *synchronized(1) 作用范围 (2) 核心组件 Wait Set：哪些调用wait方法被阻塞的线程被放置在这里； Contention List：竞争队列，所有请求锁的线程首先被放在这个竞争队列中； Entry List：Contention List中那些有资格成为候选资源的线程被移动到Entry List中； OnDeck：任意时刻，最多只有一个线程正在竞争锁资源，该线程被成为OnDeck； Owner：当前已经获取到所资源的线程被称为Owner； !Owner：当前释放锁的线程。 () 底层实现 是非公平锁，等待的线程会先尝试自旋获取锁，如果获取不到就进入 ContentionList； 每个对象后有个 monitor 对象， 加锁就是在竞争 monitor 对象 ， 代码块加锁是在前后分别加上monitorenter和monitorexit指令来实现的，方法加锁是通过一个标记位来判断的。 与 ReentrantLock 对比 相同点： 都是可重入锁。 ① 底层实现：ReentrantLock 是 API 级别的，synchronized 是 JVM 级别的，为关键字，能够在出现异常时打印出对应的错误堆栈用于分析问题，同时 JVM 对 synchronized 提供了锁升级的优化； ② 锁的实现方式： ReentrantLock 是同步非阻塞，采用的是乐观并发策略，而 synchronized 是同步阻塞，使用的是悲观并发策略。 ③ 锁的使用的安全性： ReentrantLock 需要显视加锁解锁，可能因为忘记解锁而陷入死锁，而 synchronized 为隐式加锁，不会因为忘记解锁而陷入死锁。 ④ 功能灵活性： ReentrantLock 可尝试获取锁； RentrantLock 可中断获取锁，提供了⼀种能够中断等待锁的线程的机制，lock.lockInterruptibly() ； RentrantLock 能够支持公平锁, synchronized 只能实现非公平锁； RentrantLock 可实现选择性通知： synchronizedf 使用 notify / notifyAll 进行通知时，通知的线程由 JVM 选择，ReentrantLock 更加灵活的绑定多个 Condition, 进行选择性通知。 1 ReentrantLock显示的获得、释放锁，synchronized隐式获得释放锁2 ReentrantLock可响应中断、可轮回，synchronized是不可以响应中断的，为处理锁的不可用性提供了更高的灵活性3 ReentrantLock是API级别的，synchronized是JVM级别的4 ReentrantLock可以实现公平锁5 ReentrantLock通过Condition可以绑定多个条件6 底层实现不一样， synchronized是同步阻塞，使用的是悲观并发策略，lock是同步非阻塞，采用的是乐观并发策略7 Lock是一个接口，而synchronized是Java中的关键字，synchronized是内置的语言实现。8 synchronized在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而Lock在发生异常时，如果没有主动通过unLock()去释放锁，则很可能造成死锁现象，因此使用Lock时需要在finally块中释放锁。9 Lock可以让等待锁的线程响应中断，而synchronized却不行，使用synchronized时，等待的线程会一直等待下去，不能够响应中断。10 通过Lock可以知道有没有成功获取锁，而synchronized却无法办到。11 Lock可以提高多个线程进行读操作的效率，既就是实现读写锁等。 synchronized 与volatile 的比较 ① 实现与性能：volatile关键字是线程同步的 轻量级实现，所以 volatile性能肯定⽐synchronized关键字要好。 但是 volatile关键字只能⽤于变量⽽synchronized关键字可以修饰⽅法以及代码块。 synchronized关键字在JavaSE1.6之后进⾏了主要包括为了减少获得锁和释放锁带来的性能消耗 ⽽引⼊的偏向锁和轻量级锁以及其它各种优化之后执⾏效率有了显著提升，实际开发中使⽤ synchronized 关键字的场景还是更多⼀些。 ② 阻塞： 多线程访问volatile关键字不会发⽣阻塞，⽽synchronized关键字可能会发⽣阻塞 ③ 三特性的： volatile关键字能保证数据的可⻅性，但不能保证数据的原⼦性。synchronized关键字两者都能 保证。 ④ 使用场景： volatile关键字主要⽤于解决变量在多个线程之间的可⻅性，⽽ synchronized关键字解决的是 多个线程之间访问资源的同步性。 死锁两个进程都在等待对方执行完毕才能继续往下执行的时候就发生了死锁。结果就是两个进程都陷入了无限的等待中。 (1) 死锁的四个必要条件： 互斥条件：该资源任意⼀个时刻只由⼀个线程占⽤。 持有和等待条件： ⼀个进程因请求资源⽽阻塞时，对已获得的资源保持不放。 不可剥夺条件：线程已获得的资源在末使⽤完之前不能被其他线程强⾏剥夺，只有⾃⼰使⽤完毕后 才释放资源。 循环等待条件：:若⼲进程之间形成⼀种头尾相接的循环等待资源关系。 (2) 避免线程死锁 ① 破坏互斥条件 ：这个条件我们没有办法破坏，因为我们⽤锁本来就是想让他们互斥的（临界资源需要互斥访问）。 ② 破坏请求与保持条件 ：⼀次性申请所有的资源。 ③ 破坏不剥夺条件 ：占⽤部分资源的线程进⼀步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 ④ 破坏循环等待条件 ：靠按序申请资源来预防。按某⼀顺序申请资源，释放资源则反序释放。 相关问题 (1) 如何确保N个线程可以访问N个资源同时又不导致死锁？ ① 指定获取锁的顺序，并强制线程按照指定的顺序获取锁。因此，如果所有的线程都是以同样的顺序加锁和释放锁，就不会出现死锁了； ② 使用带有超时时间的锁； ③ 通过死锁的检测和恢复机制进行规避； (2) 写一个发生死锁的程序 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 class DeadLock1 implements Runnable &#123; private static Object resource1 = new Object(); private static Object resource2 = new Object(); private int flag = 0; public DeadLock1(int flag) &#123; this.flag = flag; &#125; @Override public void run() &#123; if (flag == 1) &#123; synchronized (resource1) &#123; try &#123; Thread.sleep(500); System.out.println(&quot;flag1 one level&quot;); synchronized (resource2) &#123; System.out.println(&quot;flag 1&quot;); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; if (flag == 2) &#123; synchronized (resource2) &#123; try &#123; Thread.sleep(500); System.out.println(&quot;flag2 one level&quot;); synchronized (resource1) &#123; System.out.println(&quot;flag 2&quot;); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; Thread t1 = new Thread(new DeadLock1(1)); Thread t2 = new Thread(new DeadLock1(2)); t1.start(); t2.start(); &#125;&#125; 其他锁无锁 CAS算法的过程是这样：它包含三个参数CAS(V,E,N): V表示要更新的变量，E表示预期值，N表示新值。仅当V值等于E值时，才会将V的值设为N，如果V值和E值不同，则说明已经有其他线程做了更新，则当前线程什么都不做。最后，CAS返回当前V的真实值。 可重入锁(递归锁) 可重入锁，也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。 偏向锁对于偏向的线程没有重入的开销。 公平锁和非公平锁 公平锁(Fair) 加锁前检查是否有排队等待的线程，优先排队等待的线程，先来先得。 非公平锁(Nonfair) 加锁时不考虑排队等待问题，直接尝试获取锁，获取不到自动到队尾等待。 非公平锁性能比公平锁高5~10倍，因为公平锁需要在多核的情况下维护一个队列 Java中的synchronized是非公平锁，ReentrantLock 默认的lock()方法采用的是非公平锁。 读写锁 读读不互斥，读写互斥，写写互斥 为了提高性能，Java提供了读写锁，在读的地方使用读锁，在写的地方使用写锁，灵活控制，如果没有写锁的情况下，读是无阻塞的,在一定程度上提高了程序的执行效率。读写锁分为读锁和写锁，多个读锁不互斥，读锁与写锁互斥，这是由jvm自己控制的，你只要上好相应的锁即可。 (1) 读锁 如果你的代码只读数据，可以很多人同时读，但不能同时写，那就上读锁 (2) 写锁 如果你的代码修改数据，只能有一个人在写，且不能同时读取，那就上写锁。总之，读的时候上读锁，写的时候上写锁！ Java中读写锁有个接口java.util.concurrent.locks.ReadWriteLock，也有具体的实现ReentrantReadWriteLock。 共享锁和独占锁 java并发包提供的加锁模式分为独占锁和共享锁。 (1) 独占锁 独占锁模式下，每次只能有一个线程能持有锁，ReentrantLock就是以独占方式实现的互斥锁。独占锁是一种悲观保守的加锁策略，它避免了读/读冲突，如果某个只读线程获取锁，则其他读线程都只能等待，这种情况下就限制了不必要的并发性，因为读操作并不会影响数据的一致性。 (2) 共享锁 共享锁则允许多个线程同时获取锁，并发访问 共享资源，如：ReadWriteLock。共享锁则是一种乐观锁，它放宽了加锁策略，允许多个执行读操作的线程同时访问共享资源。 AQS的内部类Node定义了两个常量 SHARED 和 EXCLUSIVE ，他们分别标识 AQS队列中等待线程的锁获取模式。 java的并发包中提供了ReadWriteLock，读-写锁。它允许一个资源可以被多个读操作访问，或者被一个 写操作访问，但两者不能同时进行。 重量级锁(Mutex Lock) Synchronized是通过对象内部的一个叫做监视器锁(monitor)来实现的。但是监视器锁本质又是依赖于底层的操作系统的Mutex Lock来实现的。而操作系统实现线程之间的切换这就需要从用户态转换到核心态，这个成本非常高，状态之间的转换需要相对比较长的时间，这就是为什么Synchronized效率低的原因。 因此，这种 依赖于操作系统Mutex Lock所实现的锁我们称之为“重量级锁”。JDK中对Synchronized做的种种优化，其核心都是为了减少这种重量级锁的使用。JDK1.6以后，为了减少获得锁和释放锁所带来的性能消耗，提高性能，引入了“轻量级锁”和“偏向锁”。 分段锁 是一种思想ConcurrentHashMap是学习分段锁的最好实践 活锁 因为活跃性而引入的问题 并发中的设计模式单例模式保证全局唯一，并发情况下使用安全 见设计模式单例 7 种单例模式： 饿汉式； 双重监测懒汉式； 线程安全懒汉式； 静态内部类持有懒汉式； 枚举式； 变种的饿汉式； 变种的懒汉式； Future 模式Future模式的核心思想：异步调用 不仅可以在子线程完成后收集其结果，还可以设定子线程的超时时间，避免主任务一直等待。 () 性质Future模式不会立即返回你需要的数据，但是，他会返回一个契约 ，以后在使用到数据的时候就可以通过这个契约获取到需要的数据。 在广义的Future模式中，虽然获取数据是一个耗时的操作，但是服务程序不等数据完成就立即返回客户端一个伪造的数据(就是上述说的“契约”)，实现了Future模式的客户端并不急于对其进行处理，而是先去处理其他业务，充分利用了等待的时间，这也是Future模式的核心所在，在完成了其他数据无关的任务之后，最后在使用返回比较慢的Future数据。这样在整个调用的过程中就不会出现长时间的等待，充分利用时间，从而提高系统效率。 () JDK 中的 Future 模式 FutureTask实现了 Callable，Future接口，RunnableFuture接口继承了Future和Runnable接口。因为RunnableFuture实现了Runnable接口，因此FutureTask可以提交给Executor进行执行，FutureTask有两个构造方法，如下： Runnable 与 Callable 的区别 (1) Callable规定的方法是call()，Runnable规定的方法是run()；(2) Callable的任务执行后可返回值，而Runnable的任务是不能返回值得；(3) call()方法可以抛出异常，run()方法不可以；(4) 运行Callable任务可以拿到一个Future对象，Future 表示异步计算的结果 () 异常 Future 的 get() 可能会阻塞当前线程的执行，会抛出 InterruptedExcpeiton、ExecutionException，若线程已经取消，抛出 CancellationException，取消由cancel 方法来执行。isDone确定任务是正常完成还是被取消了。 () 可取消性 一旦计算完成，就不能再取消计算。如果为了可取消性而使用Future 但又不提供可用的结果，则可以声明Future&lt;?&gt; 形式类型、并返回 null 作为底层任务的结果。 生产者消费者使用生产者消费者模式实现的一个例子；模仿分布式爬虫； 单生产者单消费者 单生产者多消费者 多生产者单消费者 多生产者多消费者 Refs 《Java 并发编程的艺术》 《深入理解Java虚拟机(第二版)》","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"并发","slug":"并发","permalink":"http://example.com/tags/%E5%B9%B6%E5%8F%91/"}]},{"title":"Docker 学习笔记","date":"2020-12-17T14:41:19.000Z","path":"2020/12/17/Docker-学习笔记/","text":"Docker 基础 类似精简的 Linux 环境，含 root 权限、进程空间、用户空间和网络空间，以及运行在其中的应用程序 Client： 客户端通过 CLI 命令与 Docker 交互Docker daemon： 宿主机的守护进程，通过 RESTful 接口处理 Client 的命令，连接 Registry 进行镜像的拉取的推送，具体配置见 [Daemon配置](#Daemon 配置)Registry： 保存 image 的地方，实现 image 的维护、复用Image： 静态的镜像，可根据 Image 运行 containerContainer： 依据 Image 生成的具体的容器，实际运行的程序 Docker 底层实现原理： Namespaces：做隔离pid，net，ipc，mnt，uts Control groups(cgroups)：做资源限制 Union file systems: Container和image的分层，分层文件系统 镜像 一个特殊的文件系统，提供容器运行时所需的程序，同时包含一些为运行时准备的配置参数，无法更改 镜像的获取 根据 Dockerfile 构建镜像，配合 sh 脚本实现一些定制的初始化和参数判断逻辑，可重建 根据容器构建镜像，在只读镜像上操作可写容器重新打包成镜像，Docker 无状态，volume 不会打包进镜像，较少使用 从远程 Registry 拉取镜像 123456# 从远程 registry 拉取docker commit wonderful_mendeleev janhen/centos-vim-gcc:1.0.0# 从 Dockerfile 构建docker build -t janhen/myimage:1.0 .# 从容器创建docker pull &lt;registry_host&gt;/&lt;username OR project_name&gt;/&lt;image_name&gt;:&lt;image_tag&gt; 镜像 tag 1234# image 的查、交互docker imagesdocker history &lt;image_id&gt;docker tag &lt;image_old_name&gt; &lt;image_new_name&gt; 镜像清理 处理同一个版本多次覆盖，默认查找顺序为 Local -&gt; Registry 的问题 1234567# 删除指定的 imagedocker rmi &lt;image_id OR image_name&gt;# 强制删除指定|全部 imagedocker rmi -f $(docker images)# 删除 &lt;none&gt; 的镜像(#)docker rmi $(docker images -f &quot;dangling=true&quot; -q)docker images | grep none | awk &#x27;&#123;print $3&#125;&#x27; | xargs docker rmi 容器 是镜像运行时的实体，构建在镜像上，可对容器进行写操作 Container 的启动并运行 单机上使用最多，控制部署时候的各种参数，包含网络、存储、密码、变量… 常用的启动指定： 指定网络，根据需要选择端口转发、单机桥接网络、多机网络、主机网络 指定文件映射，将程序中的配置文件、数据文件隔离出来，避免应容器销毁而丢失 指定命令，内部运行的程序自带的命令，如 Redis 中的命令控制持久化方式… 指定变量，通过命令方式、环境变量方式指定，让运行容器更加定制化 1234567891011121314151617181920212223242526# 容器的运行# --name: 按照特定名称启动，作为容器标识# -d: 后台运行# -i: 交互式运行容器，打开STDIN，用于控制台交互 # -t: 终端方式交互, 通过 bash、shell... 进行命令式交互# -p: 映射宿主机与容器的端口号# --network=&lt;value&gt;: 指定网络连接类(#)# -v: 进行宿主机文件与容器文件的映射(#)# --&lt;param&gt;=&lt;value&gt;: 进行特定参数指定，传入中的参数，在容器中的文件处可引用# -e: 指定环境变量, 对应镜像提供，与 Dockerfile 中指定的 ENV 等同，可进入容器使用 env 查看(#)# --privileged=true: 给容器扩展的权限# --rm: 在容器终止运行后自动删除容器文件，避免磁盘浪费，常用于测试# --restart=&lt;strategy&gt;: 重启策略，与 --rm 参数冲突，提供多种策略# --entrypoint: 覆盖默认镜像的 ENTRYPOINT# --link: 添加链接到另一个 container, 不建议使用# -w: 指定工作目录，等价于 Dockerfile 中的 WORKDIR# 启动过后执行一段 Shell 脚本, 用于测试环境类镜像使用docker run ubuntu:18.04 /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;# 以命令行方式进入容器，查看镜像具体情况docker run -it --entrypoint bash openjdk:7-jre # Dockerfile 中环境变量配合运行指定 JVM 运行参数、运行端口，参数名仿照 spring-boot maven 插件docker run -d -p 7070:7070 -e JVM_OPTS=&quot;-Xms1024m -Xmx2048m&quot; -e PROGRAM_ARGS=&quot;--server.port=7070&quot; com.blinkfox/web-demo:1.0.0docker run -e &quot;JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,address=5005,server=y,suspend=n&quot; -p 8080:8080 -p 5005:5005 -t springio/gs-spring-boot-dockerdocker run -d --name test1 \\ -e MYENV=AAAA \\ busybox /bin/sh -c &quot;while true;do sleep 3600;done&quot; 容器的信息查看 1234567891011121314151617181920# 容器整体信息查询docker infodocker info | grep &quot;Docker Root Dir&quot;docker ps [(-a)|(-aq)]?# 配置信息docker inspect &lt;container&gt;docker inspect -f &#123;&#123;xx.yy&#125;&#125; &lt;container&gt;# 交互，调试# 日志， -f ： follow log output，持续实时显示日志， -t:......# 命令交互# 在容器中执行特定命令# 日志查看docker logs &lt;contain_id OR container_name&gt;docker logs -f &lt;container_id OR container_name&gt;# 容器内部执行docker exec -it &lt;container_id&gt; bashdocker exec &lt;container_id&gt; ip adocker exec -it &lt;container_id OR container_name&gt; env# 运行信息docker stat &lt;container&gt; 容器基础命令 123456789# 容器的启、停docker container start|stop|restart &lt;container_id OR contaienr_name&gt;# 导入导出# 导出容器成指定的 tar 包# 容器快照文件导入为*镜像*# URL/目录导入docker export 7691a814370e &gt; ubuntu.tarcat ubuntu.tar | docker import - test/ubuntu:v1.0docker import http://example.com/exampleimage.tgz example/imagerepo 容器的清理 12345678# 删除|强制删除指定的容器docker container rm &lt;container_id OR container_name&gt;docker contaienr rm -f &lt;container_id OR container_name&gt;# 删除所有容器docker rm $(docker ps -aq)docker rm -f $(docker ps -aq)# 删除已停止运行的容器(#)docker rm $(docker ps -f &quot;status=exited&quot; -q) Container 交互 容器内部可执行的命令，特定目录存储的配置内容，可以通过 CLI 的监控命令 支持更改 /etc/hosts， /etc/hostname，/etc/resolv.conf ，只针对运行时，临时的更改 几种交互方式： 运行时直接进入交互、运行时直接执行命令交互，包含对文件的操作、内部命令执行 运行后按特定终端进入交互、运行后按特定命令交互，同上 日志交互，logs，支持最后几行、最近的时间点、实时显示 基本情况，inspect，返回运行情况 JSON 字符串，可通过 Go Templete 获取特定情况 运行的资源情况，stats，实时显示 CPU、内存、网络、磁盘情况 1234567891011121314151617181920212223242526272829303132# 进入容器内部docker exec -it -u root jenkins sh# 执行特定命令# 创建之后执行# 在已运行的容器中执行命令docker run -it --rm ubuntu:18.04 ip adocker run -it --rm ubuntu:18.04 --hostname=test.com --dns=172.16.3.3 ip adocker run -it --rm ubuntu:18.04 cat /etc/resolv.confdocker exec -it gitlab cat /etc/resolv.confdocker exec -it gitlab cat /etc/hostnamedocker exec -it gitlab cat /etc/hosts# 容器内部执行# 查看挂载情况# 查看定义的环境变量# 查看 dns 情况, 与在宿主机上的 /etc/docker/daemon.json 上配置 dns 类似?# 查看容器IP地址配置# 查看路由情况mountenvcat /etc/resolv.confip addr show eth0ip route# logs 查看# 特定时间偏移, 特定时间段docker logs -f -t --since=40m --tail=10 jenkinsdocker logs -t --since=&quot;2019-08-01T13:23:37&quot; --until &quot;2018-08-31T12:23:37&quot; jenkins# inpect 查看docker inspect -f &#x27;&#123;&#123;.State.Pid&#125;&#125;&#x27; 1f1f4c1f931a# stat 查看docker stats &lt;container_id OR container_name&gt;# 拷贝文件，作为 Dockerfile 中 COPY 的...docker cp &lt;host_machine file OR dir&gt; &lt;container_name&gt;:&lt;container_dir&gt; Registry Docker 的私有仓库，实现容器的复用共享 发布镜像到 Registry 的方式： 发布镜像到仓库 直接将本地已经构建好的镜像发布到仓库中 根据指定 Dockerfile 由 Docker hub 进行构建形成镜像 自动在 git 发生变化的时候拉取数据进行构建重新发布到仓库上，自动构建发布，CICD 保证镜像的可再生性 私有 Registry 搭建： 官方提供的 registry Vmware 开源的 harbor，见 工具与环境 123456789# 登录 docker hub 账号和密码# 推送镜像到 docker hub# docker hub 关联 github or bitbucketdocker login 172.17.11.29:5111 -u admin -p Harbor12345# 重命名镜像的名称(tag)docker push 172.17.11.29:5111/centos-vim-gcc:1.0.0docker tag janhen/centos-vim-gcc:1.0.0 172.17.11.29:5111/study-docker/centos-vim-gcc:1.0.0docker push 172.17.11.29:5111/study-docker/centos-vim-gcc:1.0.0 Docker 网络 进行容器之间的访问，包含单机上的访问，多台机器之间的访问； 含端口映射、容器互联 关联文档: 使用网络 | 高级网络配置 Linux 上网络访问 Linux 网络命名空间，进行网络的隔离 Veth pair： 进行网络命名空间的连接，实现两个 net namspce 连接通信 12345678910# 网络命名空间# ip link# 给命名空间分配 ip 地址, 默认情况下只有 mac 地址# 启动接口# 连接双方使其网络互通ip netns listip netns delete test1ip netns exec test2 ip linkip netns exec testl ip link set dev veth-testl upip netns exec testz ip link set dev veth-test2 up Docker 网络访问 通过link 方式实现容器之间的访问，直接通过名称而非 IP，适用于单台机器 一个容器对应一个网络空间 类似局域网连接，通过中间的交换机实现两个容器之间的通信， docker0 的内网指定默认为 172.17.0/16，自定义为 172.17.18.0/16… 访问外部网络，需要经过 NAT 转换 12345678910# 查看容器网络， bridge 网络docker network lssudo docker network inspect &lt;network_id OR network_name&gt;ip ayum install bridge-utils# 展示系统当前桥接brctl showip a# 创建指定类型的网络docker network create -d bridge net-my Docker link 网络连接 通过命名 Docker 进行相连，类似网络命名空间中的 Veth pair，目前不推荐使用 命令格式： –link : 替代方案： docker-compose.yml 中使用 depends_on，使用 overlay 网络 123456789101112# 类似给 net-test2 添加 DNS 记录# link 方向性; 使用少docker run -d --name net-test2 \\ --link net-test1 busybox \\ /bin/sh -c &quot;while true; do sleep 3600; done&quot;docker exec -it net-test2 /bin/sh ip a ping net-test1# -d 指定网络类型， bridge|overlaydocker network create -d bridge net-mydocker run -it --rm --name busybox1 --network my-net busybox shdocker run -it --rm --name busybox2 --network my-net busybox sh 12345678910111213# --link &lt;name&gt; 支持通过名称访问容器 docker run -d --name flask-redis \\ -p 5000:5000 \\ --link redis \\ -e REDIS_HOST=redis \\ janhen/flask-redis docker run -d --name test1 \\ -e PENG=testt1 \\ busybox docker run -d --name test2 \\ -e PENG=testest \\ busybox \\ /bin/sh -c &quot;while true; do sleep 3600; done&quot; 自定义网络连接 避免使用 –link 进行容器之间网络的连接 1dockernetwork create -d bridge net-demo Docker 单机网络Docker bridge 网络 可以创建自己的桥接网络，进行区分，docker-compose 默认管理的容器共享同一个 bridge 网络 12345678# 创建自己的桥接网络docker network create -d bridge my-bridgedocker network lsbrctl showdocker run -d \\ -- name net-test3 \\ --network my-bridge busybox /bin/sh -c &quot;while true; do sleep 3600; done&quot;docker network connect mybridge net-test2 bridge 性能一般，对性能要求较高，可使用个 SR-IOV 网卡嵌入容器内。 Docker host 和 none网络 none 网络： 不会有网络信息，孤立的网络，用来做私有的工具，如保存密码??，使用场景少 host 网络：无网络信息，与主机共享网络命名空间，存在端口冲突问题 1docker run -d -p 80:80 nginx Docker 多机网络 实现多个不同机器之间的容器进行通信 Overlay 网络 依赖一个分布式存储，保存对应的 IP，防止网络(172.18.0.0/16)、容器名称等的冲突 实现 Docker 的多机网络，见 [Internel 访问](#Internel 访问) 两台机器之间可以相互通信，为了实现不同容器之间的通信需要借助第三方的分布式存储 使用etcd 建立的 cluster 中容器名称不允许重复、Ip 地址不允许重复 1234567891011121314151617# 创建 overlay 网络，实现多态主机之间的同步创建 overlay 网络docker network lsdocker netword create -d overlay net-overlay-demo# 查看网络情况，子网范围，容器情况docker network inspect net-overlay-demo# 启动容器指定到 overlay 网络docker run -d --name node1-test1 \\ --net net-overlay-demo \\ busybox sh -c &quot;while true; do sleep 3600; done&quot;docker run -d --name node2-test1 \\ --net net-overlay-demo \\ busybox sh -c &quot;while true; do sleep 3600; done&quot;# 查看节点上容器的地址# 查看 cluster 中网络的情况docker exec node1-test1 ip adocker exec node2-test1 ip adocker network inspect net-overlay-demo Etcd 分布式存储 存储分布式系统中的 key-val，开源免费，保证 overlay 网络中分配的容器与容器对应的IP地址在整个网络中唯一 关联： GitHub 1234567891011121314151617# 在对应的两台机器上安装 etcd，容器安装/binary 安装# 通过命令指定好集群启动# 验证 cluster 的运行情况# 进入 etcd 文件夹执行健康检查，两台机器同时执行./etcdctl cluster-health# 关闭 Docker 服务# 使用 etcd 作为分布式存储启动 docker， 手动启动 dockerd 守护进程# 验证systemctl stop dockersudo /usr/bin/dockerd -H tcp://0.0.0.0:2375 \\ -H unix://var/run/docker.sock \\ --cluster-store=etcd://192.168.xx.xx:2379 \\ --cluster-advertise=192.168.xx.xx:2375 &amp; docker version Docker 持久化 将容器与数据存储隔离开，如 Mysql 运行程序与数据保存位置 两种持久化的方式： 本地 FS 的 Volumn 基于 plugin 的 Volume， 如 NAS 本机上三种持久化实现, -mount 选项选择数据卷： bind :挂载在 Linux FS 中任意位置 volume：统一挂载在 daemon 设置的 docker 目录下，默认为 /var/lib/docker/volumes/&lt;unique_str_id OR volume_name&gt; tmpfs： 只挂载在内存中，易丢失 使用命令: 12345678docker volume create -d local testdocker volume inspect &lt;contaienr&gt;# 清理docker volume prune &lt;&gt;docker volume rm &lt;&gt;docker run -d --mount type=bind, source=/data, destination=/redis/data xxxx# 指定 :ro 容器无法对挂载数据卷内的数据进行修改docker run -d -v /webapp:/opt/webapp:ro 数据卷容器 实现多个容器操作数据，任意容器修改都可被其他容器看到 –volumes-from 参数所挂载的数据卷容器无需处在运行状态 1docker run -d --volumes-from dbdata xxx Volume 通过 Dockerfile 中的 Volumn 控制，在宿主机上 docker 文件下建立目录存放文件 建议 -v 参数指定在 docker 目录下 volume 的名称，默认为 /var/lib/docker/volumes/&lt;-v_name OR long_str&gt; 针对官方镜像，到 Docker Hub 上查看对应的 volume 挂载目录位置 1234567891011121314# 创建 volume，查看所有|指定|删除volumedocker volume create volume1docker volume lsdocker volume inspect volume1docker volume rm volume2# 运行-&gt;删除-&gt;验证docker run -d -v mysql1:/var/lib/mysql \\ --name mysql1 \\ -e MYSQL_ALLOW_EMPTY_PASSWORD=true mysql docker rm -f mysql1 mysql2docker run -d -v mysql1:/var/lib/mysql --name mysql1 -e MYSQL_ALLOW_EMPTY_PASSWORD=true mysql docker exec -it mysql2 /bin/bash mysql -u root show databases; Bind Mouting 指定容器目录与宿主机目录绑定，宿主机文件更改影响到容器中的运行 可以实现本台电脑 –&gt; 虚拟机 –&gt; 容器三者的目录映射 1234# -v: &lt;宿主机目录&gt;:&lt;容器目录&gt; 进行一一映射docker run -d -v $(pwd):/usr/share/nginx/html -p 80:80 --name web janhen/my-nginx# 使用 Docker 作为本地开发环境docker run -d -p 80:5000 --name flask janhen/flask-skeleton Dockerfile 编写 用于生成 Docker Image 的文件，一般只用于 docker build -t janhen/xx:99 . 命令执行使用 关联： Dockerfile 指令 语法Dockerfile 的基本语法 FROM,WORKDIR,ENV,COPY,ADD RUN,CMD,ENTRYPOINT VOLUME,EXPOSE FROM： 根据特定的镜像制作，从头制作、 根据指定环境制作、某个镜像作为构建阶段使用 RUN ： 运行命令脚本, 可以通过此安装一些环境并对环境进行配置，如安装 Node 环境，每运行一个命令增加一层 ==&gt; 建议将多个命令合并成一个命令使用 WORKDIR： 设定当前工作目录, 类似 cd 改变目录, 没有目录自动创建(#) 直接通过绝对路径定位 通过绝对路径+相对路径定位目录 ADD and COPY： 将本地文件添加到 docker image 中,常 配合 WORKDIR 使用 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748FROM python:3.7# LABEL 镜像的 metadata，帮助信息LABEL maintainer=&quot;janhen &lt;ipaam414@gmail.com&gt;&quot;RUN yum update &amp;&amp; yum install -y vim \\ python-devRUN apt-get update &amp;&amp; apt-get install -y perl \\ pwgen --no-install-recommends &amp;&amp; rm -rf \\ /var/lib/apt/lists/*RUN /bin/bash -c &#x27;source $HOME/.bashrc;echo $HOME&#x27;WORKDIR /rootWORKDIR /testWORKDIR demoRUN pwdADD hello /ADD test.tar.gz /WORKDIR /rootADD hello test/COPY hello test/# ENV 设定环境变量, 建议使用ENV MYSQL_VERSION 5.6RUN apt-get install -y mysql-server = &quot;$&#123;MYSQL_VERSION&#125;&quot; \\ &amp;&amp; rm -rf /var/lib/apt/lists/*# CMD# 设置容器启动后默认执行的命令和参数# docker run 指定其他命令, CMD 被忽略# 定义多个 CMD，只运行最后一个docker run [image] # CMD 会被执行docker run -it [image] /bin/bash # CMD 不会执行CMD [&quot;mongod&quot;]# ENTRYPOINT# 设置容器启动时运行的命令# 容器以应用程序/服务的形式运行# 不会被忽略, 一定会执行# 最佳实践: 通过 shell 脚本作为 entrypoint COPY docker-entrypoint.sh /usr/local/bin/ # 添加到容器中ENTRYPOINT [&quot;docker-entrypoint.sh&quot;] # 指定入口脚本EXPOSE 27017ENTRYPOINT [&quot;scripts/dev.sh&quot;]# 进行宿主机与容器中文件的映射# 映射容器中的 /tmp 到宿主机上，默认在 /var/lib/docker/volumes/&lt;long_id OR name&gt; 下建立对应的映射VOLUME /tmp 命令格式 不同的命令执行写法，以及对应的区别 Shell 格式, 默认通过 shell 执行 Exec 格式, ENTRYPOINT [“/bin/bash”, “-c”, “echo”, “hello $name”] 针对 Exec 无法映射变量问题的处理： 通过命令方式编写语句 12ENV name DockerENTRYPOINT [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo&quot;, &quot;hello $name&quot;] 命令区别 1、RUN、CMD和ENTRYPOINT命令区别 RUN 运行在 image 的构建阶段执行，执行结果会被打包进 image 文件 CMD 在容器启动后执行，可用于在容器内启动某个服务、进程，只可使用一次，与 run 中年执行命令冲突 ENTRYPOINT 在容器启动后执行，出现多行不会忽略，一定执行，通常配合 COPY 到容器中的 sh 脚本使用 2、COPY 与 ADD 命令区别 ADD 可以获取网络资源，可以直接解压缩 注意事项 1、CMD 的最后一次有效性 官方镜像中大多最后运行 CMD，方便覆盖实现定制化的参数的启动 2、目录 COPY . /app 与 COPY . /app/ 映射不同 Docker Compose 多容器管理，通过 yml 配置管理容器之间的依赖关系，底层 python 编写，前身为开源的 Fig 项目。主要用于本地开发使用。 关联文档：Compose 模板文件 | Doc 管理 docker-compose 的启动、停止、交互 123456789101112131415161718# compose 后台启动# 启动并查看日志docker-compose updocker-compose up -ddocker-compose -f &lt;compose_name&gt; up -d # 停止服务# 停止并删除 容器、网络、volumesdocker-compose stop &lt;service&gt;docker-compose down &lt;service&gt;docker-compose build# compose 查看运行情况，状态、端口情况# 查看 compose 中定义容器使用的 imagesdocker-compose psdocker-compose images# 进入 compose 中的 servicedocker-compose exec mysql bash# 扩展docker-compose scale &lt;service_name&gt;=&lt;count&gt; service 的扩展 实现水平扩展，负载均衡，在不存在端口冲突的情况下通过 haproxy 进行负载均衡，在 Docker Swarm 运行时可直接通过 deploy 中的参数指定复制扩展的个数 通过 docker-compose scale 命令进行扩展 处理 scale 中端口映射重复问题 在 docker-compose 中增加 dockercloud/haproxy 进行负载均衡 12345678docker-compose up -d# 启动时指定扩展docker-compose up --scale web=3 -d# 运行后进行扩展docker-compose scale web=4# 验证扩展情况docker-compose psfor i in `seq 10`; do curl 127.0.0.1:8080; done 语法 对应 docker-compose.yml 文件的语法 三大实体： service: 服务 networks: 网络指定，指定网络类型，一般为 bridge、overlay，根据需要指定多个网络，进行一定的隔离 volumes: 进行数据卷的映射 image 获取方式： 通过 image 获取本地的或是拉取远程的，或者通过 build 进行构建，传入 Dockerfile 的目录以及对应的 Dockerfile 名称 ports: 进行宿主端口与容器端口的映射 depends_on: 解决容器的依赖，启动先后问题 links: 服务之间的依赖关系，在容器内部可以直接使用依赖服务名称对应的 IP 地址，不建议使用 deploy: 进行部署，控制集群中的各种情况，用于 Docker swarm，version 3 支持 12345678910111213141516# 特定片段参考# 设置网络, 可多个# frontend, backend 前后端设置networks: - frontend - backend# 端口设置# 直接引号设置# 宿主机与容器端口映射ports: - &quot;6379&quot;ports: - 5000:80# 依赖depends_on: - mysql Docker Swarm Docker 自带的服务编排框架，大多数都由其中的 Manager 做管理，较难定制，不适合太多节点的部署 Docker Swarm 特点： 符合传统IT的管理模式 平台本身集成性好，可当成云管平台使用 内置太多不易进行定制化，不好 Debug，不易干预 不提供存储选项：Docker Swarm不提供将容器连接到存储的无障碍方式，其数据量需要在主机和手动配置上进行大量即兴创作 监控不良：Docker Swarm提供有关容器的基本信息，如果您正在寻找基本的监控解决方案，那么Stats命令就足够了。如果您正在寻找高级监控，那么Docker Swarm永远不是一个选择。虽然有像CAdvisor这样的第三方工具可以提供更多监控，但使用Docker本身实时收集有关容器的更多数据是不可行的。 Swarm 架构 Raft consensus group： 进行控制分布式场景下的协商: 内置的分布式的存储数据库，通过 Raft 协议进行同步，包含 Leader 选举、Log 复制 Internel distributed state store： 分布式存储数据库，功能如保证分布式场景下 Ip 等唯一，类似 etcd Manager: 可以保存 Raft 关联的文件，用于 Secret 实现 Worker: 主要运行容器，通过 Gossip network 进行通信，保证分布式下的一致性 Gossip network： 各个 Worker 之间同步实现的协议 扩展： Service: 通过 swam manager 进行控制，具体 service 部署到哪个 node 上 Replicas： 一个 Service 对应多个 Replicas，用于扩展 集群搭建管理 让几台服务器搭建成一个 Swarm Cluster 1234567891011# 配置 Manager Nodedocker swarm init --advertise-addr=192.169.xx.xx# 配置 Worker Node 加入到特定的 Manager Nodedocker swarm join --token xxxfsdfsdf &lt;ip&gt;:&lt;port&gt;# 查看当前 Node 情况# 节点查看# 节点降级docker node ls docker node inspect &lt;node_name&gt;docker node demote &lt;node_name&gt;docker node ps Swarm管理Swarm Services 管理 单个 Service 的管理，一个 Service 可扩展到多个 cluster node 上的 Container 运行 123456789101112131415161718192021222324# 创建容器，运行位置有 mananger 进行控制运行在哪个节点上# 类似 docker run 命令，在本地创建 container# 查看 service 情况# MODE: replicated# REPLICAS: 1/1 支持水平扩展，类似 docker compose 中的 scale# 查看具体的 service 情况# 运行在哪个节点上# 扩展servie，通过复制的方式(#) docker service create --name demo busybox \\ sh -c &quot;while true; do sleep 3600; done&quot;docker service lsdocker service ps demodocker service scale demo=5docker service ps demo # 本机查看 docker 容器运行# 强制删除某个正在运行中的容器# 集群自动恢复，确保一定数目的 scale 扩展有效，系统稳定运行时# 显示节点中容器运行情况# 删除服务，对应的集群节点容器删除docker psdocker rm -f e64432docker service lsdocker service ps demodocker service rm demo RoutingMesh Swarm 网络通信原理，管理集群服务间的通信，访问集群中任何一个节点特定端口都会被重定向到实际运行服务的节点上 DNS 服务发现，单机情况下可以通过 service 的名称进行相互访问，多机情况下通过 swarm 进行相互访问 VIP： 非真实机器的IP地址，避免多个IP地址变化问题，造成系统运行不稳定，一个 service 对应一个 LVS： 根据虚拟 IP 找出容器中的具体的 IP 地址 两种体现： Internel：容器之间通过 overlay 网络访问 Ingress ：服务绑定接口的情况，此服务通过任意 Swarm 节点对应接口访问 Internel 访问 容器间实现相互访问，通过 overlay 网络实现，实现 service 与 service 之间的通信 whoami 镜像： 提供 web 服务，访问 8000 端口，返回 container 的 hostname 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# 创建 overlay 网络# 创建 whoami 服务# 后台运行# 端口映射# 网络指定# 查看所有 service # 查看 whoami 服务运行位置# 到对应机器上验证docker network create -d overlay net-demodocker service create -d \\ --name whoami \\ -p 8000:8000 \\ --network net-demo jwilder/whoamidocker service lsdocker service ps whoamidocker pscurl 127.0.0.1:8000# 创建 busybox 的容器# 连接到同一个 overlay 网络# 查看所有服务，当前 busybox service 是否启动完成# 查看服务 client 服务具体位置# 进入对应的机器查看对应运行的 container# 进入容器# 10.0.0.7 IP 地址，为虚拟 IP， 将 whoiam 通过 scale 扩展# 通过 scale 进行扩展 whoami # 查看 whoami 位置，并进入# 进入对应 client contaienr 中# 连接 whoami # 查询 dns，只有一个虚拟IP 10.0.0.7# 进入容器 whoami 查看网络地址# 进入容器 whoami(另一) 查看网络地址# 进入容器 client# 查看 task.whoami，返回对应的多个节点，为真实的 IP 地址（#）docker service create -d \\ --name client \\ --network net-demo busybox \\ sh -c &quot;while true; do sleep 3600; done&quot;docker service lsdocker service ps clientdocker psdocerer exec -it &lt;container_id&gt; sh ping whoami docker service scale whoami=2 docker service ps whoamidocker service ps clientdocker exec -it &lt;container_client_id&gt; sh ping whoami nslookup whoamidocker exec 5b79 ip adocker exec df9 ip adocerk exec -it &lt;container_client_id&gt; sh nslookup task.whoami# 扩展 whoami 服务# 查案 client 对应的 task.whoami，显示三个对应的(whoami)IP 地址# ==&gt; 虚拟IP: 不会随 service 的扩展而变化, 包括增加、减少、机器之间的迁移不会变化(#)# 访问多次服务 whoami，相应的对应机器上的容器会因为负载均衡而不同，通过 LVS 实现docker service scale whoami=3docker service ps whoami--- nslookup task.whoami wget whoami:8000 more index.html rm -rf index.html wget whoami:8000 两种体现： Internal: 容器键通过 overlay 网络(VIP)访问 Ingress: 服务绑定接口, 通过任意 swarm 节点的接口访问’ DNS + VIP + iptables + LVS 实现的过程图： // todo 具体 Swarm 网络中数据的流动情况 小结： 容器之间连接到 overlay 网络进行通信，service 之间的通信通过 VIP + LVS 实现 Ingress 负载均衡 绑定端口实现的容器之间的访问，通过 : 直接访问服务 作用体现：集群中的 Node 对应的端口提供相同的服务，即使 Node 本地无服务也支持访问 Ingress Network 的数据包走向图 在 IPTables + IPVS 发往目的网络 12345678910111213141516# 常看网络桥接情况# 查看机器的网络命令空间# 进入 ingress_sbox 网络命名空间iptablesbrctl showsudo ls /var/run/docker/netnssudo nsenter --net=/var/run/docker/netns/ingress_sboxip aiptables -nL -t mangle# 安装 LVS 管理工具# # 查看 LVS 情况，展示可选的服务 IP 地址，展示机器的 weight, yum install ipvsadmsudo nsenter --net=/var/run/docker/netns/ingress_sboxiptables -nL -t mangleipvsadm -l Docker Stack 部署 进行多服务部署，可以使用 docker-compose.yml ，只能用于 swarm cluster，无法用于其他的服务编排框架 docker-compose.yml 文件更改 compose file version 3: 增加 deploy 命令，具体参数如下 1234567891011121314151617181920212223242526# deploy# endpoint_mode: vip 模式(默认), dnsrr 模式 循环访问(少用)# labels: 帮助描述信息# mode: global, replicated， global 全局唯一, 无法通过 scale 横向扩展，一般外部服务使用此种方式，如 mysql,nginx,redis等； replicated 默认，可通过复制来进行扩展# placement: # constraint: # - node.role == manager # 限制部署到 manager 节点上# preferences: 优先喜好# -# replicas: 在 mod 是 replicated 的时候定义初始化时候需要的 replicas# resources: 进行资源的限制# limits:# cpus: &#x27;0.50&#x27; # CPU 使用限制# memeory: 50M # 内存使用限制# reservations: # 优先保留，最小的情况# cpus: &#x27;0.25&#x27;# memory: 20M# restart_policy: # 容器宕机后的处理# conditon: on-failure # 什么情况下重启# delay: 5s # 延迟# max_attempts: 3 # 最大重试次数# window: 120s# update_config: # service 更新的配置# parallelism: 2# delay: 10s# order: stop-first 部署的过程： 更改单机的 docker-compose.yml 为对应 cluster 部署(deploy) 按条件执行命令： 如下 验证： 通过访问任意一个 cluster 中的地址即可访问 12345678910# 整个 application 定义为一个 stack 为 wordpress# 可通过 -c=docker-compose.yml 进行简化# 查看运行情况# mysql: 限制只运行一个，只能运行在 manager 节点# 通过 stack 查看服务情况docker stack deploy wordpress --compose-file=docker-compose.ymldocker stack lsdocker stack ps wordpressdocker stack services wordpressdocker stack rm wordpress Docker Secret 管理 对一些密码进行管理， 处理 docker-compose.yml 中存储密码不安全问题，借助内部分布式存储数据库控制，只作用于 Docker Swarm 关联： Doc-CLI Secret 类型： username password， SSH key, TLS 认证，不想让人看到的数据 生产环境至少要两个 Manager，分布式存储的天然加密环境 Secret 的管理： 将 Secret 存储在 Manager 中的分布式存储中的 Raft Database Secret 给某个 service 指派 Service 基本使用 Secret 的创建方式：文件方式、输入方式。 存放在容器中的 /run/secrets/&lt;secret_file_name&gt; 文件中 123456789101112131415161718192021222324252627282930313233# 按文件方式进行创建# 删除文件，保证安全性# 查看 secret# 借助管道按照输入方式创建 secretvim passworddocker secret create my-file-pw passwordrm -rf passworddocker secret lsecho &quot;mypassword&quot; | docker secret create my-input-pwdocker secret rm my-input-pw# 通过 swarm service 创建过程中指定 secret 进行使用# 进入容器查看指定目录，找到 manager 通过 Raft Database 保存的 secretdocker service create -d --name client \\ --secret my-file-pw busybox \\ sh -c &quot;while true; do sleep 3600; done&quot;docker service ls docker service ps clientdocker psdocker exec -it &lt;client_container_id&gt; sh cd /run/secrets/ ls cat my-file-pw # 原文# 实际使用# 在创建 service 的时候指定好 secret，并在环境变量中指定在容器中的位置docker service create -d --name db \\ --secret my-file-pw \\ -e MYSQL_ROOT_PASSWORD_FILE=/run/secrets/my-file-pw mysqldocker service ps db--docker exec -it &lt;db_container_id&gt; shls /run/secretscat /run/secrets/my-file-pwmysql -u root -p 在 Stack 中的使用 在服务配置下增加 secrets，指定对应的 Secret 对应的密码参数使用指定的 secrets 在容器中的位置 可以连通创建 secret 一起使用，不建议 12345# -c 简化 --compose-file # 查看服务是否全部启动完成docker stack deploy wordpress \\ -c=docker-compose.ymldocker stack services wordpress Docker Service 更新 在运行过程中对 service 依赖的镜像进行升级，实现升级过程中不会中断原来的服务 单 Service 更新进行 service 的更新，不会暂停运行的项目 @Q: 存在一段时间有 1.0和2.0并存的情况，如何处理?? 12345678910111213141516171819# 创建 overlay 网络，启动服务# 等待服务启动完毕docker network create -d overlay net-demodocker network lsdocker service create -d --name web \\ --publish 8080:5000 \\ --network net-demo janhen/python-flask-demo:1.0.0docker service ps web# 扩展服务# 检查服务运行情况# 编写测试脚本方便验证docker service scale web=2docker service ps webcurl 127.0.0.1:8080sh -c &quot;while true; do curl 127.0.0.1:8080&amp;&amp;sleep 1; done&quot;# 更新镜像，一般通过 Dockerfile 进行构建，指定对应更新的版本，发布到私有 registry# 运行环境拉取镜像，执行更新命令docker service update --image janehn/python-plask-demo:2.0.0 web 12345# 更新镜像并设置参数, 覆盖 docker-compose.ymldocker service update --image westos.org/game2048 \\ --update-parallelism 10 \\ --update-delay 10s \\ nginx 端口更新 对 service 与宿主机的端口映射进行更改 删除掉原来的端口映射，无法做到更新时业务不中断，通过 VIP + 端口实现原理导致的 12docker service update --publish-rm 8080:500 \\ --publish-add 8088:5000 web Stack 更新 更改 Swarm Cluster 中多个容器中对于镜像、网络、部署配置的更新，关联 .deploy.update_config 下的配置 可更改 docker-compose.yml 中 deploy 下的 update_config 控制更新时的细节，允许几个 scale 进行更新，延迟信息。。。 第一次通过 deploy 进行启动进行了多 service 的部署 第二次通过 deploy 部署时，自动检测到 docker-compose.yml 的变化，进行更新 1docker stack deploy wordpress --compose-file docker-compose.yml Docker Swarm 监控 实现对 Docker Swarm Cluster 中运行节点上容器的监控 CAdvisor+InfluxDB+Grafana docker swarm集群的监控方案，开源免费 cAdvisor：数据收集模块，需要部署在集群中的每一个节点上，当然前提条件是节点接受task。 InfluxDB：数据存储模块 Grafana：数据展示模块 Docker Universal Control Plane(UCP) docker原厂的可视化集群管理GUI，企业级的，只支持docker EE portainer 在集群中部署portainer的service，只能被调度给manager角色的节点 关联： Web 其他Daemon 配置 对容器的 dockerd 守护线程进行配置 关联： Configure the daemon dameon.json 配置文件编写： 源镜像地址配置 私有源非 Https 配置 Debug 模式开启 /etc/docker/daemon.json ip: 永久绑定到某个固定的 IP 地址 bridge： 将 Docker 默认桥接到创建的网桥上 1234567891011121314151617181920&#123; &quot;registry-mirrors&quot;: [ &quot;https://dockerhub.azk8s.cn&quot;, &quot;https://reg-mirror.qiniu.com&quot; ], &quot;insecure-registries&quot;: [ &quot;172.17.11.29:80&quot;, &quot;172.17.11.29:5111&quot;, &quot;192.168.205.23:80&quot;, &quot;192.168.205.23:5111&quot;, &quot;172.17.10.150:80&quot; ], &quot;debug&quot;: false, &quot;dns&quot; : [ &quot;114.114.114.114&quot;, &quot;8.8.8.8&quot; ], &quot;ip&quot;: &quot;0.0.0.0&quot;, &quot;bridge&quot;: &quot;bridge-my&quot;&#125; 12345# 更改后使其生效sudo systemctl daemon-reloadsudo systemctl restart docker.servicesudo systemctl status docker -lsudo docker info 设置运行时目录，存储驱动 设置 Http/Https 代理 加快拉取国外访问、处理国内制作镜像无法访问国外资源问题 123456789101112131415161718# 添加配置mkdir -p /etc/systemd/system/docker.service.dvim /etc/systemd/system/docker.service.d/http-proxy.conf[Service] Environment=&quot;HTTP_PROXY=https://172.17.10.18:5720/&quot; &quot;NO_PROXY=localhost,127.0.0.1&quot;# 配置生效systemctl daemon-reloadsystemctl restart dockersystemctl show --property=Environment docker# 重置rm -f /etc/systemd/system/docker.service.d/http-proxy.confsystemctl daemon-reloadsystemctl restart dockersystemctl show --property=Environment docker Docker systemd http-proxy 监控与管理 人工进行容器的管理、监控、资源调整、故障排除，包括日志查看、容器实时运行情况、资源重分配，在无法使用或没有监控方案情况下使用 dockerd 支持 在发生故障后，通过设置 Docker 守护线程的一些参数方便调试 123456# 开启守护线程的 debug 模式，给出更多的信息提示dockerd --debug \\ --tls=true \\ --tlscert=/var/docker/server.pem \\ --tlskey=/var/docker/serverkey.pem \\ --host tcp://192.169.9.2:2376 容器的运行日志查看 使用 Go 模板尽心格式化日志输出 可使用日志驱动程序插件，企业版支持统一格式查看远程的日志，默认双重日志 Format command and log output 1234567docker container inspect --format &#x27;&#123;&#123; .NetworkSettings.IPAddress &#125;&#125;&#x27; $&#123;CID&#125;# 获取某个镜像对应的全部容器docker container ls | grep &lt;image&gt; | awk &#x27;&#123;print $1&#125;docker inspect -f &#x27;&#123;&#123;.HostConfig.LogConfig.Type&#125;&#125;&#x27; &lt;CONTAINER&gt;docker logs -f &lt;container_id&gt;# 通过 Go 的模板语法进行格式化展示数据docker image ls --format &quot;&#123;&#123;.ID&#125;&#125;: &#123;&#123;.Repository&#125;&#125;&quot; 容器日志 在 daemon 中的日志配置，根据实际需要进行优化，可选择日志插件 指定容器日志最大大小 20M 最大的文件个数 5 压缩， 开 容器的运行情况 通过 stats 实时查看容器的资源信息 123# 实时查看容器统计信息，CPU、内存、网络、磁盘docker stats 13b9203f9f0b d42877298134 44fb90cd2f2cdocker container stats --format &quot;table &#123;&#123;.Name&#125;&#125;\\t&#123;&#123;.CPUPerc&#125;&#125;\\t&#123;&#123;.MemUsage&#125;&#125;&quot; 容器的资源分配 容器使用多少宿主机的资源，可以通过 docker-compose.yml 中设置 123# 通过参数限定容器访问内存、CPUdocker run --help | grep cpu docker run --help | grep memory 容器可用性 容器支持重启，可以通过 --restart 指定重启策略，保证可用性 容器网络 1234# 查看所有网络docker network ls# 查看容器网络映射docker port nostalgic_morse 5000 .dockerignore 针对非 SpringBoot 项目，如前端项目需要忽略一些文件。 123.gitnode_modulesnpm-debug.log 默认的重要文件： /var/run/docker.sock /var/lib/docker/volumes/ Docker 卸载1234567891011121314151617181920212223# 停止并删除容器docker rm -f `docker ps -aq`# 删除安装yum list installed|grep docker yum -y remove docker-ce.x86_64 yum -y remove docker-ce-cli.x86_64 yum -y remove containerd.io.x86_64 # 所有镜像、Volume删除 # 删除 docker-compose rm -rf /var/lib/docker rm -rf /hdapp rm -rf /etc/docker rm -f /usr/local/bin/docker-compose # 测试卸载情况 yum list installed|grep docker # 删除 docker0 网卡 yum install bridge-utils ip link set dev docker0 down brctl delbr docker0 RefsDocker-guide: 中文的 GitBook 官方镜像示例： Docker Hub 中一些镜像的开源示例 play-with-docker ： 方便环境搭建， 保存 docker 4h，在网站上创建多个网络，进行互通访问 10分钟看懂Docker和K8S： 快速入门 docker学习笔记： 他人博客笔记 Docker – 从入门到实践 如何调试 Docker： 总结调试方法 关于对docker run –link的理解： Docker 桥接网络理解 工具和示例： Docker 相关工具 Dockerfile 最佳实践： 编写 Dockerfile 的一些建议 CentOS7.4—构建LVS+Keepalived高可用群集： LVS docker镜像操作： 容器制作、本地导入、镜像导出","tags":[{"name":"Docker","slug":"Docker","permalink":"http://example.com/tags/Docker/"}]},{"title":"Arthas","date":"2020-12-17T13:10:57.000Z","path":"2020/12/17/Arthas/","text":"概述 Arthas 是Alibaba开源的Java诊断工具。 Github 当你遇到以下类似问题而束手无策时，Arthas可以帮助你解决： 这个类从哪个 jar 包加载的？为什么会报各种类相关的 Exception？ 我改的代码为什么没有执行到？难道是我没 commit？分支搞错了？ 遇到问题无法在线上 debug，难道只能通过加日志再重新发布吗？ 线上遇到某个用户的数据处理有问题，但线上同样无法 debug，线下无法重现！ 是否有一个全局视角来查看系统的运行状况？ 有什么办法可以监控到JVM的实时运行状态？ 怎么快速定位应用的热点，生成火焰图？ JVM 信息JVM 相关命令的 1.dashboard -&gt; thread -&gt; 3.jvm -&gt; 4.sysprop -&gt; 查看和修改JVM的系统属性 5.getstatic -&gt; 查看类的静态属性 dashboard 当前系统的实时数据面板 线程、内存、运行信息 thread 当前JVM 线程堆栈信息 -b: 找出持有锁的 -n: 根据 cpu 使用排序 -i: 指定时间间隔 –state : 过滤线程状态 使用最繁忙的 5个线程 1thread -i 2000 -n 5 查看指定线程堆栈 1thread &lt;id&gt; 查看特定状态的线程: RUNNABLE, TIMED_WAITING, WAITING, BLOCKED.. 12thread -i 2000 --state TIMED_WAITINGthread -i 2000 --state RUNNABLE jvm 查看当前JVM 的信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182 RUNTIME ------------------------------------------------------------------------------------------------------------------------------------ MACHINE-NAME 8@e57d77f3ad01 JVM-START-TIME 2020-12-10 19:38:01 MANAGEMENT-SPEC-VERSION 1.2 SPEC-NAME Java Virtual Machine Specification SPEC-VENDOR Oracle Corporation SPEC-VERSION 1.8 VM-NAME Java HotSpot(TM) 64-Bit Server VM VM-VENDOR Oracle Corporation VM-VERSION 25.11-b03 INPUT-ARGUMENTS -Xmx4096M -Xms4096M -Xmn1536M ... CLASS-PATH ... BOOT-CLASS-PATH ... ------------------------------------------------------------------------------------------------------------------------------------ CLASS-LOADING ------------------------------------------------------------------------------------------------------------------------------------ LOADED-CLASS-COUNT 34980 TOTAL-LOADED-CLASS-COUNT 46897 UNLOADED-CLASS-COUNT 11917 IS-VERBOSE false ------------------------------------------------------------------------------------------------------------------------------------ COMPILATION ------------------------------------------------------------------------------------------------------------------------------------ NAME HotSpot 64-Bit Tiered Compilers TOTAL-COMPILE-TIME 549939(ms) ------------------------------------------------------------------------------------------------------------------------------------ GARBAGE-COLLECTORS ------------------------------------------------------------------------------------------------------------------------------------ ParNew 5889/191768(ms) [count/time] ConcurrentMarkSweep 18/12942(ms) [count/time] ------------------------------------------------------------------------------------------------------------------------------------ MEMORY-MANAGERS ------------------------------------------------------------------------------------------------------------------------------------ CodeCacheManager Code Cache ... ------------------------------------------------------------------------------------------------------------------------------------ MEMORY ------------------------------------------------------------------------------------------------------------------------------------ HEAP-MEMORY-USAGE 4133945344(3.85 GiB)/4294967296(4.00 GiB)/4133945344(3.85 GiB)/289171584(275.78 MiB) [committed/init/max/used] NO-HEAP-MEMORY-USAGE 398196736(379.75 MiB)/2555904(2.44 MiB)/1862270976(1.73 GiB)/340835576(325.05 MiB) [committed/init/max/used] PENDING-FINALIZE-COUNT 0 ------------------------------------------------------------------------------------------------------------------------------------ OPERATING-SYSTEM ------------------------------------------------------------------------------------------------------------------------------------ OS Linux ARCH amd64 PROCESSORS-COUNT 6 LOAD-AVERAGE 0.35 VERSION 3.10.0-957.el7.x86_64 ------------------------------------------------------------------------------------------------------------------------------------ THREAD ------------------------------------------------------------------------------------------------------------------------------------ COUNT 99 DAEMON-COUNT 75 PEAK-COUNT 99 STARTED-COUNT 70889 DEADLOCK-COUNT 0 ------------------------------------------------------------------------------------------------------------------------------------ FILE-DESCRIPTOR ------------------------------------------------------------------------------------------------------------------------------------ MAX-FILE-DESCRIPTOR-COUNT 1048576 OPEN-FILE-DESCRIPTOR-COUNT 298 Affect(row-cnt:0) cost in 19 ms. sysprop 查看当前JVM的系统属性(System Property) 查看日志匹配，方便进行日志的收集查看日志文件位置，方便日志的查看查看执行的参数，SpringBoot 之后拼接的参数，方便定位自定义参数的实际指定情况查看 JVM 运行的时区，方便处理日志的时间问题查看运行时的版本，方便查看已知的 JDK bug 修复情况 123456sysprop FILE_LOG_PATTERNsysprop CONSOLE_LOG_PATTERsysprop LOG_FILEsysprop sun.java.commandsysprop user.timezonesysprop java.runtime.version getstatic 查看类的静态属性 , 推荐直接使用 ognl 命令 -c: 类加载器的 hash id -E: 开启正则表达式匹配，默认通配符匹配 &lt;class-pattern&gt; Class name pattern, use either ‘.’ or ‘/‘ as separator &lt;field-pattern&gt; Field name pattern &lt;express&gt; the content you want to watch, written by ognl 123456789getstatic com.janhen.SapConstants JOB_RUNNING_INTERVALfield: JOB_RUNNING_INTERVAL@Integer[2]Affect(row-cnt:1) cost in 331 ms.[arthas@8]$ getstatic com.janhen.SapConstants ALCNTC_INTERFACE_NOfield: ALCNTC_INTERFACE_NO@String[MD038]Affect(row-cnt:1) cost in 33 ms. 查看私有的静态变量 1getstatic org.springframework.amqp.rabbit.connection.CachingConnectionFactory DEFAULT_CHANNEL_CACHE_SIZE 查看集合信息 1getstatic org.springframework.amqp.rabbit.connection.CachingConnectionFactory txStarts 指定的 classloader 加载的类查看 12getstatic -c 73ad2d6 io.netty.channel.nio.NioEventLoop logger &#x27;getClass().getName()&#x27;field: logger 类加载sc 查看JVM已加载的类信息 各个字段的类型，以及访问标识符 父类信息、接口信息、类加载器信息、加载的来源文件 search-class -d, –details: Display the details of class -f, –field: Display all the member variables Class name pattern, use either ‘.’ or ‘/‘ as separato 1sc -df org.springframework.amqp.rabbit.connection.CachingConnectionFactory* sm 查看已加载类的方法信息 声明的类、 构造器、注解、参数、异常、类加载器 -c, –classloader The hash code of the special class’s classLoader -d, –details Display the details of method&lt;class-pattern&gt; Class name pattern, use either ‘.’ or ‘/‘ as separator&lt;method-pattern&gt; Method name pattern 1sm -d org.springframework.amqp.rabbit.connection.CachingConnectionFactory 查看具体的方法信息 1sm -d org.springframework.amqp.rabbit.connection.CachingConnectionFactory toString classloader 查看classloader的继承树，urls，类加载信息 12345678910111213 classloader name numberOfInstances loadedCountTotal org.springframework.boot.loader.LaunchedURLClassLoader 1 15617 java.net.URLClassLoader 1060 5560 BootstrapClassLoader 1 4535 java.net.FactoryURLClassLoader 2 1666 sun.reflect.DelegatingClassLoader 1406 1406 com.taobao.arthas.agent.ArthasClassloader 1 1116 sun.misc.Launcher$AppClassLoader 1 47 com.alibaba.fastjson.util.ASMClassLoader 1 13 sun.misc.Launcher$ExtClassLoader 1 10 org.apache.cxf.common.util.ASMHelper$TypeHelperClassLoader 1 6 Affect(row-cnt:10) cost in 330 ms. redefine 载入外部 .class，直接修改线上的代码，不能恢复 -c, –classloader classLoader hashcode .class file paths *ognl 执行ognl表达式， ognl命令实际上包含了getstatic的功能 查看Spring的配置 -c, –classLoader The hash code of the special class’s classLoader, default classLoader is SystemClassLoader. 查看 Spring 中运行时指定属性的值，先找到持有 ApplicationContext 的类对应的类加载器 12sc -df com.janhen.SpringContextUtil | grep classLoaderHashognl -c b1bc7ed &#x27;#spCtx=@com.janhen.SpringContextUtil@context,#spCtx.getEnvironment().getProperty(&quot;spring.redis.sentinel.nodes&quot;)&#x27; 设置静态属性值 一般由配置中心修改，防止 setstatic 不知道什么时候因为什么修改的应该无法更改 final 的静态变量 1ognl &#x27;#field=@demo.MathGame@class.getDeclaredField(&quot;random&quot;), #field.setAccessible(true), #field.set(null,null)&#x27; 替换方式实现： 可以写一个新的类，里面设置 static field的值。然后用 classloader 命令把这个新的类 load到JVM里再执行。 上传class到服务器上 redefine https://github.com/WangJi92/arthas-idea-plugin/issues/1 tt 记录下指定方法每次调用的入参和返回信息，并能对这些不同的时间下调用进行观测 -n &lt;count&gt;: 指定次数, 通过 -n 参数指定需要记录的次数，当达到记录次数时 Arthas 会主动中断tt命令的记录过程，避免人工操作无法停止的情况 1tt -t org.springframework.amqp.rabbit.connection.CachingConnectionFactory toString 监控与执行获取运行时的方法信息、返回信息、执行过程的耗时通过字节码增强技术实现，使用完成之后需要执行shutdown 或者 将增强过的类执行reset 命令。 monitor -c, –cycle The monitor interval (in seconds), 60 seconds by default Path and classname of Pattern Matching Method of Pattern Matching watch 查看方法参数 -n, –limits Threshold of execution times -b, –before Watch before invocation -x, –expand Expand level of object (1 by default) 查看入参对象(Object)以及返回对象(Set) 123456watch com.janhen.ConsumingRtnProcessor getBinAlreadyExistContainerBarcodes &quot;&#123;params, returnObj&#125;&quot; -b -x 2watch com.janhen.ConsumingRtnProcessor verifyContainer &quot;&#123;params, returnObj&#125;&quot; -bwatch -E .*ConsumingRtnProcessor verify|verifyContainer &quot;&#123;params, returnObj&#125;&quot; -b -x 3watch -E .*ConsumingRtnProcessor verify|verifyBin|verifyContainer &quot;&#123;params,returnObj&#125;&quot; -b -x 3watch -E .*ReceiveBillDao update &quot;&#123;params,returnObj&#125;&quot; -b -x 3watch com.janhen.StockServiceImpl query &quot;&#123;params,target&#125;&quot; -x 3 trace 跟踪方法耗时 跟踪方法的耗时情况，包含各个阶段的 1trace com.janhen.OrderBillServiceImpl getByBillNumber|query 匹配特定类的所有方法 1trace -E .*OrderServiceImpl .* 1trace -E com.janhen.OrderServiceImpl query RefConfigure logging drivershttps://docs.docker.com/config/containers/logging/configure/ Java线上诊断神器Arthas-2https://kamzhuyuqing.github.io/2018/12/20/Java%E7%BA%BF%E4%B8%8A%E8%AF%8A%E6%96%AD%E7%A5%9E%E5%99%A8Arthas-2/ 技术征文 | 那些年，我用 Arthas 排查过的问题https://mp.weixin.qq.com/s/gJ4ZVvFBuiXbirjTxjwGeQ 是否可以考虑支持setstatichttps://github.com/alibaba/arthas/issues/641","tags":[{"name":"工具","slug":"工具","permalink":"http://example.com/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"Percona-Toolkit-数据库工具","date":"2020-12-17T11:20:54.000Z","path":"2020/12/17/Percona-Toolkit/","text":"概述 perl 语言编写, 不同的 linux 发行版不同的安装 DSN 语法 h: host P: 端口 u: 用户 p: 密码 S: mysql_socket D: 数据库 A: charset t: table F: mysql_read_default_file 1h&#x3D;172.17.10.233,P&#x3D;3306,u&#x3D;root,p&#x3D;root,D&#x3D;testdb3,t&#x3D;testtable1 分类 这些工具主要包括开发、性能、配置、监控、复制、系统、实用六大类 PT 工具分类 工具类别 工具命令 工具作用 备注 开发类 pt-duplicate-key-checker 列出并删除重复的索引和外键 优化使用，实用 pt-online-schema-change 在线修改表结构 pt-query-advisor 分析查询语句，并给出建议，有bug 已废弃 pt-show-grants 规范化和打印权限 分析使用 pt-upgrade 在多个服务器上执行查询，并比较不同 性能类 pt-index-usage 分析日志中索引使用情况，并出报告 索引查看，实用 pt-pmp 为查询结果跟踪，并汇总跟踪结果 pt-visual-explain 格式化执行计划 pt-table-usage 分析日志中查询并分析表使用情况 pt 2.2新增命令 配置类 pt-config-diff 比较配置文件和参数 pt-mysql-summary 对mysql配置和status进行汇总 整体信息，实用 pt-variable-advisor 分析参数，并提出建议 监控类 pt-deadlock-logger 提取和记录mysql死锁信息 死锁信息，实用 pt-fk-error-logger 提取和记录外键信息 pt-deadlock-logger pt-mext 并行查看status样本信息 pt-query-digest 分析查询日志，并产生报告 常用命令 pt-trend 按照时间段读取slow日志信息 已废弃 复制类 pt-heartbeat 监控mysql复制延迟 pt-slave-delay 设定从落后主的时间 pt-slave-find 查找和打印所有mysql复制层级关系 pt-slave-restart 监控salve错误，并尝试重启salve pt-table-checksum 校验主从复制一致性 实用 pt-table-sync 高效同步表数据 实用 系统类 pt-diskstats 查看系统磁盘状态 pt-fifo-split 模拟切割文件并输出 pt-summary 收集和显示系统概况 pt-stalk 出现问题时，收集诊断数据 pt-sift 浏览由pt-stalk创建的文件 pt 2.2新增命令 pt-ioprofile 查询进程IO并打印一个IO活动表 pt 2.2新增命令 实用类 pt-archiver 将表数据归档到另一个表或文件中 pt-find 查找表并执行命令 pt-kill Kill掉符合条件的sql 常用命令 pt-align 对齐其他工具的输出 pt 2.2新增命令 pt-fingerprint 将查询转成密文 pt 2.2新增命令 信息查看pt-summary 可查看挂载情况、网络情况、进程情况 –sleep: 通过 vmstat 收集的 sleep 时间, 默认 5 –summarize-mounts: 挂载的文件系统、磁盘使用, 默认 TRUE –summarize-network: 网络收集、配置, 默认 TRUE –summarize-processes: top process vmstat 输出, 默认 TRUE 1pt-summary pt-mysql-summary 精细地对 mysql 的配置和 sataus 信息进行汇总, 优先执行各种类型字段在对应数据库中的数量btree 在对应 db 中的数量连接的 host 当前连接情况, Process list主从连接情况 –all-databases: 默认 false –databases: 查看指定的数据库 12345678910# 查看所有数据库pt-mysql-summary \\ --host=127.0.0.1 --port=3306 \\ -u root -p root \\ --all-databases# 查看指令数据库pt-mysql-summary \\ --host=127.0.0.1 --port=3306 \\ -u root -p root \\ --databases testdb 1234567891011121314151617181920212223状态统计:Aborted_clients: 取消的连接Com_change_db: 更改 db 命令?Com_commit: 事务提交的个数Com_insert: 插入语句Com_select: 查询语句Com_show_engine_status: 查看存储引擎情况Com_show_table_status: 查看表状态Com_show_variables: 查看变量Connections: 连接数Created_tmp_disk_tables: 创建的临时表Handler_commitHandler_rollbackHandler_updateHandler_writeInnodb_buffer_pool_bytes_data: 缓存池数据Innodb_row_lock_time: 行锁的时间Innodb_row_lock_waits: 锁等待Innodb_rows_deletedInnodb_rows_insertedSelect_full_join: 全表连接Select_full_range_join: 范围连接Sort_rows: 排序的行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107pt-mysql-summary \\ --host=127.0.0.1 --port=3306 \\ -u root -p root \\ --databases testdb1,testdb2# Note Processlist, Schema, InnoDB, Binary Logging Server_id: 315 Master_id: 391 Slave_UUID: 7f80892a-650a-11ea-bb0f-0242ac110002 Version | 5.6.47-log MySQL Community Server (GPL) Started | 2020-07-01 19:35 (up 43+14:06:36) Databases | 9 Datadir | /var/lib/mysql/ Processes | 60 connected, 4 running Replication | Is not a slave, has 1 slaves connected# Processlist ################################################ Command COUNT(*) Working SUM(Time) MAX(Time) ------------------------------ -------- ------- --------- --------- Binlog Dump 1 1 8000 8000 Daemon 1 1 20000 20000 Query 1 1 0 0 Sleep 60 0 22500 3500 User COUNT(*) Working SUM(Time) MAX(Time) ------------------------------ -------- ------- --------- --------- event_scheduler 1 1 20000 20000 user1 45 1 0 0 user2 1 1 8000 8000 user3 15 0 0 0 Host COUNT(*) Working SUM(Time) MAX(Time) ------------------------------ -------- ------- --------- --------- 127.0.0.1 1 1 0 0 172.17.0.1 2 0 0 0 192.168.199.116 1 1 8000 8000 ... localhost 1 1 20000 20000 db COUNT(*) Working SUM(Time) MAX(Time) ------------------------------ -------- ------- --------- --------- NULL 3 3 30000 20000 testdb1 6 0 0 0 testdb2 10 0 0 0 testdb3 15 0 0 0 testdb4 30 0 0 0 testdb5 1 0 0 0 State COUNT(*) Working SUM(Time) MAX(Time) ------------------------------ -------- ------- --------- --------- 60 0 0 0 Master has sent all binlog to 1 1 8000 8000 Waiting for next activation 1 1 20000 20000 init 1 1 0 0# Schema ##################################################### Database Tables Views SPs Trigs Funcs FKs Partn testdb1 35 4 testdb3 223 1 4 Database InnoDB testdb1 35 testdb3 223 Database BTREE testdb1 112 testdb3 653 v d b s t i c d d t a a i m e n h e a i r t g a x t a c t n c e i l t r i e y h t n l m i a i t i a n r m n l t e t Database === === === === === === === === === === testdb1 209 34 13 3 4 17 5 testdb3 3482 283 88 1 2 132 37 433 73 4# InnoDB ##################################################### Version | 5.6.47 Buffer Pool Size | 18.0G Buffer Pool Fill | 100% Buffer Pool Dirty | 0% File Per Table | ON Page Size | 16k Log File Size | 2 * 500.0M = 1000.0M Log Buffer Size | 8M Flush Method | Flush Log At Commit | 2 XA Support | ON Checksums | ON Doublewrite | ON...# Binary Logging ############################################# Binlogs | 15 Zero-Sized | 0 Total Size | 14.2G binlog_format | ROW expire_logs_days | 7 sync_binlog | 0 server_id | 391 binlog_do_db | binlog_ignore_db | pt-deadlock-logger 死锁检测, 收集和保存 mysql 上最近的死锁信息，可以直接打印死锁信息和存储死锁信息到数据库中，死锁信息包括发生死锁的服务器、最近发生死锁的时间、死锁线程 id、死锁的事务 id、发生死锁时事务执行了多长时间等信息。 –columns=A: 控制输出的列 –daemonize: 后台运行 –log=s: 后台运行将输出到处到指定文件 –tab: 使用 tab 进行分割 1234pt-deadlock-logger \\ --h 127.0.0.1 \\ -u root -p root \\ --tab pt-duplicate-key-checker 找出重复的索引和外键，并生成删除重复索引的 SQL 语句 1234pt-duplicate-key-checker \\ -h 127.0.0.1 \\ -u root -p root \\ -d testdb3,testdb1 1234567891011121314151617181920212223# ######################################################################### testdb3.testtable1 # ######################################################################### idx_testtable1_06 is a left-prefix of id_testtable1_unique_01# Key definitions:# KEY &#96;idx_testtable1_06&#96; (&#96;dcUuid&#96;)# UNIQUE KEY &#96;id_testtable1_unique_01&#96; (&#96;dcUuid&#96;,&#96;billNumber&#96;),# Column types:# &#96;dcuuid&#96; varchar(38) not null comment &#39;配送中心uuid&#39;# &#96;billnumber&#96; varchar(30) not null comment &#39;单号&#39;# To remove this duplicate index, execute:ALTER TABLE &#96;testdb3&#96;.&#96;testtable1&#96; DROP INDEX &#96;idx_testtable1_06&#96;;...# ######################################################################### Summary of indexes # ######################################################################### Size Duplicate Indexes 30832348# Total Duplicate Indexes 23# Total Indexes 707 pt-show-grants 查看授权情况 –flush: 刷新权限 1234# Show all grantspt-show-grants \\ -h 127.0.0.1 -P3306 \\ -u root -p $MYSQL_ROOT_PASSWORD 1234# Show database grantspt-show-grants \\ -u root -p root \\ -D testdb4 pt-variable-advisor 分析 mysql 的参数变量，并对可能存在的问题提出建议 12345678pt-variable-advisor --h 172.17.10.233 --u root --p root pt-variable-advisor --h localhost --u root --p root \\ --source-of-variables /etc/mysql/mysql.conf.d/mysqld.cnf pt-variable-advisor \\ h=172.17.10.233,P=3306,u=root,p=root, \\ S=/var/run/mysqld/mysqld.sock \\ --source-of-variables=mysql pt-table-checksum 校验 MySQL 主从复制的完整性，存在锁表问题。 参数： –databases=：指定需要被检查的数据库，多个则用逗号隔开 –tables=：指定需要被检查的表，多个用逗号隔开 -h=127.0.0.1 ：Master的地址 -u=xiaoml：用户名 -p=123456：密码 -P=3306：端口 –tables-regex=s： 表正则匹配 –ignore-tables-regex=s： 忽略的表 –replicate=s: 将校验结果保存到表中 percona.checksums –replicate-database=s: 指定数据库复制校验 123pt-table-checksum -u root -p root \\ --databases testdb5 \\ --tables=QRTZ_TRIGGERS 使用依赖： 需要一个既能登录主库，也能登录从库，而且还能同步数据库的账号 生产环境使用 pt-table-checksum 检查MySQL数据一致性https://segmentfault.com/a/1190000004309169 pt-diskstats 为 GUN/LINUX 打印磁盘 io 统计信息,可以分析从远程机器收集的数据 分析pt-index-usage 从 log 文件中读取查询语句，并用 explain 分析他们是如何利用索引。 完成分析之后会生成一份关于索引没有被查询使用过的报告。 1234pt-index-usage /var/lib/mysql/slow.log \\ --h localhost --u root --p 123456 \\ -d testdb3 \\ --no-report --create-save-results-database *pt-query-digest 分析查询执行日志，并产生一个查询报告，为 MySQL、PostgreSQL、 memcached 过滤、重放或者转换语句。pt-query-digest --database=s: 连接的数据库, 非分析的数据库 --limit=A: 限制输出的百分比/数量， (default 95%:20) --report-all： 所有的查询输出 --ignore-attributes=a: 忽略收集的属性 --timeline: 展示时间线的事件 --review type: DSN 保存查询结果供之后 review, 默认数据库和表为 percona_schema.query_review --report-histogram=s: 属性的直方图，默认为 Query_time --type tcpdump: 类型, 分析多种不同类型的日志 binlog: 分析 binlog genlog: slowlog: 分析慢查询 tcpdump: --order-by: 默认根据查询时间排序， Query_time:sum ，attribute:aggregate 参数的语法 sum Sum/total attribute value min Minimum attribute value max Maximum attribute value cnt Frequency/count of the query 12# 按照总耗时排序--order-by Query_time:sum --since=s: 过滤开始时间 --until=s 过滤结束时间 --limit=A: 限制输出的百分比/数量， (default 95%:20) --filter: 过滤指定的事件, 不同的扫描条件 12# 过滤语句 select：--filter &#x27;$event-&gt;&#123;arg&#125; =~ m/^select/i&#x27;， 12# 过滤指定用户：--filter &#x27;($event-&gt;&#123;user&#125; || &quot;&quot;) =~ m/^dba/i&#x27; ， 12# 过滤全表扫描：--filter &#x27;(($event-&gt;&#123;Full_scan&#125; || &quot;&quot;) eq &quot;yes&quot;) ||(($event-&gt;&#123;Full_join&#125; || &quot;&quot;) eq &quot;yes&quot;)&#x27; 12# 过滤指定数据库--filter &#x27;$event-&gt;&#123;db&#125; &amp;&amp; $event-&gt;&#123;db&#125; =~ /testdb3/ &amp;&amp; $event-&gt;&#123;user&#125; =~ /root/&#x27; 使用案例分析所有慢查询日志 1pt-query-advisor /var/lib/mysql/slow-query.log 指定的查询分析 1pt-query-digest --query &quot;select * from mysql.user&quot; 1234567891011121314151617pt-query-digest \\ -h127.0.0.1 -P3306 \\ -uroot -proot \\ --since &#x27;2020-07-25 00:00:00&#x27; \\ --until &#x27;2020-07-26 00:00:00&#x27; \\ --limit 20 \\ /var/lib/mysql/mysql-slow.log.200725 pt-query-digest \\ -uroot -pimws \\ --since &#x27;2020-07-25 00:00:00&#x27; \\ --until &#x27;2020-07-26 00:00:00&#x27; \\ --order-by Query_time:cnt \\ --limit 20 \\ /var/lib/mysql/mysql-slow.log.200725 \\ &gt; slow-analyse-2.log 更改相关*pt-online-schema-change 在线更改表结构，适用于大表结构的更改 --host: 连接mysql的地址 -P=3306: 连接mysql的端口号 --user: 连接mysql的用户名 --password: 连接mysql的密码 --database=s / D: 连接mysql的库名 t: 连接mysql的表名 --alter: 修改表结构的语句 --charset=utf8: 使用utf8编码，避免中文乱码 --no-version-check: 不检查版本，在阿里云服务器中一般加入此参数，否则会报错 --execute: 执行修改表结构 --new-table-name=s: 新创建的表，默认为 &lt;old-table-name&gt;_new --dry-run: 常见并更改表, 不会创建触发器、复制数据.. --print: 打印 SQL 执行语句 --statistics： 打印内部计数器的统计信息 123456alert_sql=&quot;ADD COLUMN addColumn varchar(30) DEFAULT &#x27;QTY&#x27; COMMENT &#x27;增加列备注&#x27;,ADD COLUMN addColumn2 varchar(30) DEFAULT 0 COMMENT &#x27;增加列2备注&#x27;,CHANGE COLUMN modColumn varchar(100)&quot;pt-online-schema-change \\ --user=root --password=root --host=127.0.0.1 \\ --alter &quot;$alert_sql&quot; \\ D=testdb3,t=testtable1 \\ --print --dry-run -pt-heartbeat 监控 mysql 复制延迟，测量复制落后主 mysql 或者主 PostgreSQL 多少时间，可以使用这个脚本去更新主或者监控复制 通过 show slave status\\G 命令中的 Seconds_Behind_Master 值来判断主从延迟并不靠谱。 原理：pt-heartbeat 通过真实的复制数据来确认 mysql 和 postgresql 复制延迟，避免了对复制机制的依赖，能得出准确的落后复制时间。 包含两部分： 第一部分在主上 pt-heartbeat 的 --update 线程会在指定的时间间隔更新一个时间戳， 第二部分是 pt-heartbeat 的 --monitor 线程或者 --check 线程连接到从上检查复制的心跳记录（前面更新的时间戳），并和当前系统时间进行比较，得出时间的差异。 可以手工创建 heartbeat 表或者添加 –create-table 参数。 -D / –database=s: 指定数据库, 必须的参数 –update, –monitor, –check: 互斥参数 –daemonize, –check: 互斥参数 –config: 指定配置文件的位置， key 必须为全称 –create-table： 创建heartbeat表如果该表不存在，该表由–database和–table参数来确认 –file： 将最新的–monitor信息输出到文件中，新的信息会覆盖旧的信息，通常和–daemonize参数一起使用 –frames： 统计的时间窗口，默认为1m,5m,15m -master-server-id： 指定master的server_id，在检测从的延迟时，必须指定该参数 1234567891011121314151617181920212223# 新建 heartbeat 表, 保存主从执行情况master_server_id=$( mysql -uroot -proot \\ -e &quot;SHOW VARIABLES LIKE &#x27;server_id&#x27;\\G&quot; \\ | grep Value \\ | sed -n -e &#x27;s/^.*: //p&#x27;)slave_server=192.168.199.116pt-heartbeat \\ -u root -proot \\ -D testdb3 \\ --master-server-id=$master_server_id \\ --create-table \\ --update mysql -uroot -proot -e &quot;SELECT * FROM testdb3.heartbeat &quot;;pt-heartbeat \\ -u root -proot \\ -D testdb3 \\ --master-server-id=$master_server_id \\ --update &amp; 查看主从延迟 12345678910111213pt-heartbeat \\ -h $slave_server \\ --monitor \\ -u root -proot \\ -D testdb3 \\ --master-server-id=$master_server_id \\ --print-master-server-idpt-heartbeat \\ -u root -proot \\ -D testdb3 \\ --master-server-id=$master_server_id \\ --check 123#实时延迟，1分钟延迟，5分钟延迟，15分钟延迟0.00s [ 0.00s, 0.00s, 0.00s ] 3910.00s [ 0.00s, 0.00s, 0.00s ] 391 守护线程方式执行， 2s 执行一次 1234567pt-heartbeat \\ -u root -proot \\ -D testdb3 \\ --master-server-id=$master_server_id \\ --update --daemonize --interval=2pt-heartbeat --stop 监控从库并输出日志 1234567pt-heartbeat \\ -u root -proot \\ -D testdb3 \\ --master-server-id=$master_server_id \\ --monitor --print-master-server-id \\ --daemonize --interval=2 \\ --log=/var/lib/mysql/slave-lag.log 使用pt-heartbeat监控主从复制延迟https://cloud.tencent.com/developer/article/1183713pt-heartbeathttps://www.cnblogs.com/ivictor/p/5901853.html -pt-table-sync 主从过程中不同步的表进行同步, 解决主从数据不一致的问题。 注意事项：使用 --dry-run 和 --print 选项总是先测试同步。 无法同步表结构，和索引等对象，只能同步数据 使用该工具来解决主从数据不一致的问题，也可以用来对两个不在一个主从拓扑实例，进行数据 sync –[no]check-slave: 检查目标服务器是否是从数据库，默认为 Yes –sync-to-master and/or –replicate: 只有当需要sync的表都有唯一键(主键或唯一索引)，才能使用–sync-to-master and/or –replicate。(没有唯一键，则只能在desitination上直接修改，而指定–sync-to-master and/or –replicate时只能在主库上修改)，如果sync主从时没有指定–replicate或者–sync-to-master则所有修改都在从库上执行(不论表上是否有唯一键) 123456789# 查看数据不一致pt-table-sync --print \\ h=127.0.0.1,P=3306,u=root,p=root h=127.0.0.1,P=3307 \\ --database=testdb5 --tables=testtable1 # 修复pt-table-sync --execute \\ h=127.0.0.1,P=3306,u=root,p=root h=127.0.0.1,P=3307 \\ --database=testdb5 --tables=testtable2 pt-archiver 将 mysql 数据库中表的记录归档到另外一个表或者文件，也可以直接进行记录的删除操作 只是归档旧的数据，不会对线上数据的 OLTP 查询造成太大影响，可以将数据插入另外一台服务器的其他表中，也可以写入到一个文件中，方便使用 load data infile 命令导入数据。还可以用来执行 delete 操作, 默认的会删除源中的数据。使用的时候请注意 12345678910# create tablemysql -uroot -proot \\ -e &quot;CREATE TABLE IF NOT EXISTS testdb4.bak_mis_wm_testtable1 LIKE testdb4.mis_wm_testtable1&quot;# archive data to bak table and filept-archiver \\ --source h=172.17.10.233,u=root,p=root,D=testdb4,t=mis_wm_testtable1 \\ --dest h=172.17.10.233,u=root,p=root,D=testdb4,t=bak_mis_wm_testtable1 \\ --file &#x27;/var/lib/mysql/%Y-%m-%d-%D.%t&#x27; \\ --where &quot;dispatchState = &#x27;FINISHED&#x27;&quot; \\ --limit 1000 --commit-each Refspercona-toolkit工具的使用Percona-Toolkit 示例说明https://blog.csdn.net/kk185800961/article/details/85016523 pt-query-digest（percona toolkit）小解https://www.cnblogs.com/shengdimaya/p/7063204.html","tags":[{"name":"工具","slug":"工具","permalink":"http://example.com/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"","date":"2020-12-17T01:14:13.711Z","path":"2020/12/17/Blog/","text":"","tags":[]},{"title":"Linux 常用命令","date":"2020-11-10T14:27:31.000Z","path":"2020/11/10/Linux-常用命令/","text":"概述命令设计Postel 原则： 宽进严出尽可能自由宽松接受输入格式，并输出格式良好的严谨输出格式。宽进减少了过滤器在面对非预期输入时出错的可能性，以及特定情况下崩溃的可能性。严出提高过滤器被其他程序用作输入的可能性。 命令的风格 POSIX GNU MS-DOS 123# POSIX 集中短参数tar -c -f xx.tar xx.txt yy.txttar -cfxx.tar xx.txt yy.txt 命令的参数: 可用选项 选项参数: key=val 子命令: 分割子命令显示不同的信息 密码参数: 防止根据 history 获取出用户密码 位置参数: 多个值按序解析 支持多种类型的参数: 如发布的日志，传入文件名称自动解析文件内容作为参数，传入字符串 支持自定义参数的解析: 可提供自定义的转换器，将参数映射成一个枚举项，指定的类对象 参数类型: File, Date, URL, Pattern.. 12# Show interval and count vmstat 1 10 常见参数 –usage: 给定短的使用消息 -verbose: 显示 Debug 日志 -v / -vvv: 的个数越多显示的日志越详细, 如 ansible 执行时通过 -vvv 显示运行时候的详细信息 -V/--version: 显示版本信息 -a:所有项，与 --all 同名的选项添加（append), 与 tar 中一样 -b缓冲区(buffer) 大小/block 的大小， du、df、tar 中使用批处理（batch）禁用提示/设置通过属性接受文件的输入 -c命令（带参数），如 sh、python检查（check) 不带参数，检查命令/配置是否正确 -d调试（debug)，带或不带参数，设置调试信息级别删除(delete)目录（directory) -D定义(define) 带参数 -e执行(execute) (带参数)编辑(edit)，如 crontab排除(exclude) -f文件(file)(带参数)强制(force), 如 git push -f xxx、ssh -h表头（header)帮助(help) -i初始化交互执行，如 docker exec -it &lt;container-name&gt; bash -I包含 -k: 保留，禁止资源的常规删除 -l:列表加载：登陆： ssh 等要求网络身份 常用的命令缩写:-h: 主机-P: 端口-u: 用户-p: 密码-t: 测试配置文件-c: 指定配置文件地址 常用的子命令start:stop:restart:reload: 命令工具fzf 模糊搜索工具 配置 Ctrl + R 快捷键自动根据 history 中的数据搜索 the_silver_searcher GitHub 替代 grep 的 Linux 工具，类似于ack的代码搜索工具，但速度更快。 可通过 .gitignore、 .hgignore 、.ignore 忽略指定文件 1yum -y install the_silver_searcher navi Github 命令行和应用程序启动器的交互式备忘单工具。 基于 fzf 对命令进行搜索, 可按照特定的标签进行组织命令，支持通过 &lt;name&gt; 指定参数 nohup 允许用户退出帐户/关闭终端之后继续运行相应的进程 /dev/null，代表linux的空设备文件，所有往这个文件里面写入的内容都会丢 失，俗称黑洞 标准输入0，从键盘获得输入 /proc/self/fd/0 标准输出1，输出到屏幕（控制台） /proc/self/fd/1 错误输出2，输出到屏幕（控制台） /proc/self/fd/2 &gt;/dev/null 标准输出1重定向到 /dev/null 中，此时标准输出不存在，没有任 何地方能够找到输出的内容 2&gt;&amp;1 错误输出将会和标准输出输出到同一个地方 &gt;/dev/null 2&gt;&amp;1 不会输出任何信息到控制台，也不会有任何信息输出到文件中 1234nohup flume-ng agent --conf /opt/apps/flume-1.9/conf \\ --conf-file /data/lagoudw/conf/flume-log2hdfs3.conf \\ -name a1 \\ -Dflume.root.logger=INFO,LOGFILE &gt; /dev/null 2&gt;&amp;1 &amp; lrzsz 文件的上传和下载工具，只支持文件，不支持文件夹，配合 tar 命令使用，配合 SSH 连接使用 12345yum -y install lrzsz# 选择本地文件上传到服务器rz # 从服务器下载文件到本地sz &lt;file&gt; ipvsadm LVS 管理工具 12# 查案负载情况ipvsadm -l zsh 辅助命令编写，带有提示 可进行 alias 指定, 主题选择，插件选择 sort 用于排序。 -f ：忽略大小写 -b ：忽略最前面的空格 -M ：以月份的名字来排序，例如 JAN，DEC -n ：使用数字 -r ：反向排序 -u ：相当于 unique，重复的内容只出现一次 -t ：分隔符，默认为 tab -k ：指定排序的区间 1du -sm * | sort -nr | head -n 10 uniq 可以将重复的数据只取一个。 -i ：忽略大小写 -c ：进行计数 当前目录下的磁盘情况 tee输出重定向会将输出内容重定向到文件 1tee [-a] &lt;file&gt; paste 字符转换 12paste [-d] file1 file2-d ：分隔符，默认为 tab rsync 远程同步工具 速度快、避免复制相同内容和支持符号链接。 rsync只对差异文件做更新。scp是把所有文件都复制过去。 -r -v: 显示复制过程 -l: 复制符号链接 1yum install -y rsync 1rsync -rvl /opt/module root@linux123:/opt/ 基础命令cat -n: 显示文本的行号 -T, --show-tabs: 使用 ^I 标示 Tab 字符 -E, --show-ends : 显示 $ 作为每行的末尾Ctrl + D 终止输入 1cat &gt; SampleTextFile.txt File 显示文件字符集 1file -i test_info.sh iconv:文本字符集转换 -f, –from-code=NAME: 来源字符集编码 -t, –to-code=NAME： 目的字符集编码 -o, –output=FILE： 输出文件名 1iconv -f ISO-8859-1 -t UTF-8 in.txt &gt; out.txt 信息查看RedHat 类系统信息查看 1cat /etc/os-release 系统相关 系统服务相关，包括查看状态、停止运行、重新加载、开机自启 主机域名映射，实现 hosts 环境变量控制，实现增加环境变量 1234567891011# 查看系统应用状态# 停止系统应用# 重新加载系统应用# 将系统应用开机自启# 查看失败的系统应用systemctl status redis_6379systemctl status dockersystemctl stop redis_6379systemctl reload docker.servicesystemctl enable docker.servicesystemctl --failed 系统基本信息 123456# 查看系统总信息# 查看系统的位数, 配合下载 URL 使用# 查看系统内核版本uname -auname -muname -r CPU 情况 12345678910# 总核数 = 物理CPU个数 X 每颗物理CPU的核数 # 总逻辑CPU数 = 物理CPU个数 X 每颗物理CPU的核数 X 超线程数# 查看物理CPU个数cat /proc/cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l# 查看每个物理CPU中core的个数(即核数)cat /proc/cpuinfo| grep &quot;cpu cores&quot;| uniq# 查看逻辑CPU的个数cat /proc/cpuinfo| grep &quot;processor&quot;| wc -l# 查看CPU信息（型号）cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c CPU 整体信息 1lscpu 各个 CPU 的情况 1mpstat -P ALL 连续的 CPU 情况 1mpstat -P ALL 1 300 12# 查看僵尸进程ps -al | gawk &#x27;&#123;print $2,$4&#125;&#x27; | grep Z 查看进程实际使用的内存情况 1ps -e -o &#x27;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#x27; | sort -nrk5 运行负载情况 12toptop -p 3306 内存使用情况 1free -m 12# 查看内存使用百分比free | sed -n &#x27;2p&#x27; | gawk &#x27;x = int(( $3 / $2 ) * 100) &#123;print x&#125;&#x27; | sed &#x27;s/$/%/&#x27; 1234# 释放内存echo 1 &gt;/proc/sys/vm/drop_cachesecho 2 &gt;/proc/sys/vm/drop_cachesecho 3 &gt;/proc/sys/vm/drop_caches 磁盘占用情况 可以作为部署时候的参考 12df -hdf -sh * 文件句柄数 1ulimit -n 网络查看端口使用 12ss -nelp | grep 22netstat -lntp | grep 22 查看IP 地址 12# Show all interface ip addrifconfig | grep -Eo &#x27;inet (addr:)?([0-9]*\\.)&#123;3&#125;[0-9]*&#x27; | grep -Eo &#x27;([0-9]*\\.)&#123;3&#125;[0-9]*&#x27; | grep -v &#x27;127.0.0.1&#x27; 123# Specified interfaceifconfig eno1 | grep -Eo &#x27;inet (addr:)?([0-9]*\\.)&#123;3&#125;[0-9]*&#x27; | grep -Eo &#x27;([0-9]*\\.)&#123;3&#125;[0-9]*&#x27;ifconfig eth0 | grep -Eo &#x27;inet (addr:)?([0-9]*\\.)&#123;3&#125;[0-9]*&#x27; | grep -Eo &#x27;([0-9]*\\.)&#123;3&#125;[0-9]*&#x27; 12345678910# 连接指定端口 telnet IP PORT # dns 解析相关nslookup sina.com # 从主机到互联网另一端的主机走的什么路径 traceroute sina.com sar # 网络状况分析跟踪 tcpdump Tcp 端口监听情况 123netstat -lntp | grep &lt;port&gt;ps -ef | grep &lt;port&gt;lsof -i tcp:&lt;port&gt; Tcp 状态连接统计 1netstat -an | awk &#x27;/^tcp/ &#123;++s[$NF]&#125; END &#123;for(a in s) print a,s[a]&#125;&#x27; bridge-utils： yum install bridge-utils 1234567891011# Linux 本地系统的转发支持， 0,1 语义..# 设置开启转发sysctl net.ipv4.ip_forwardsysctl -w net.ipv4.ip_forward=1# iptables 关联命令# 查看 NAT 的端口映射， 显示伪装地址，端口映射情况iptables -t nat -nL# 查看 bridge 情况brctl show# 网络运行情况netstat -anop 12# 查看本机网卡nmcli d 网卡配置 打开文件**/etc/sysconfig/network**来修改主机名和DNS 1234567891011121314vi /etc/sysconfig/network-scripts/ifcfg-ens160ONBOOT=yesBOOTPROTO=staticDNS1=172.17.0.211DNS2=172.17.0.221IPADDR=172.17.10.137 # 更改NETMASK=255.255.240.0GATEWAY=172.17.0.49systemctl restart networkping 8.8.8.8ping 172.17.0.49ping baidu.com 域名主机名配置 1234vi /etc/hostnamehostnamectl hostnamectl set-hostname cnodeecho &quot;172.17.10.153 cnode1&quot; &gt;&gt; /etc/hosts 防火墙相关 线上服务器必须使用，当前为 Centos7 自带防火墙，局域网一般可以关闭 开启、关闭防火墙，基本信息查看 开放或关闭某个|一个数据段的端口， 开放或关闭某个|多个服务 12345678910111213141516171819202122232425262728293031323334353637383940414243# 停止并关闭开机启动防火墙systemctl stop firewalldsystemctl disable firewalld# 查看防火墙版本号# 查看防火墙状态# 启动防火墙# 列出所有zone# 查看特定区域，不指定默认为 publicfirewall-cmd --versionfirewall-cmd --statesystemctl start firewalld.servicefirewall-cmd --get-zonesfirewall-cmd --get-default-zonefirewall-cmd --list-zonefirewall-cmd --list-allfirewall-cmd --zone=pulic# 查看开放的端口# 开启需要的端口# 重新加载端口# 查看开放端口验证# 查看某个端口firewall-cmd --list-portfirewall-cmd --permanent --zone=public --add-port=3306/tcpfirewall-cmd --permanent --zone=public --add-port=22/tcpfirewall-cmd --reloadfirewall-cmd --list-portfirewall-cmd --query-port=22/tcpfirewall-cmd --add-port=22/tcp# 查看开放的端口# 删除某个|某段的端口# 重新加载# 验证开放端口情况# 关闭防火墙firewall-cmd --list-portfirewall-cmd --permanent --remove-port=2000-20002/tcp firewall-cmd --reloadfirewall-cmd --list-portfirewall-cmd --zone=public --list-ports# 开启|关闭某个服务firewall-cmd --add-service ftp --permanentfirewall-cmd --remove-service --permanentfirewall-cmd --zone=public --list-services iptables 相关 12# 对于请求 8082 端口的全部丢弃掉iptables -I INPUT -p tcp --dport 8082 -j DROP net-tools 常用的网络工具 可以使用 Linux 旧有支持的命令，以及扩展的网络工具 route： 如 route -n 命令，等价于现在自带的 ip -r 命令 netstat： 常用的网络工具 ifconfig： 废弃命令，在 Linux 上使用 ip address 代替，简写成 ip a 12345678910# 添加默认网关模板以及对应的实例# 验证设置网关的结果route add default gw &#123;IP-ADDRESS&#125; &#123;INTERFACE-NAME&#125;route add default gw 192.168.2.254 eth0route -n# Linux 自带命令# 配置默认网关# 验证执行情况ip route add default via 192.168.1.254ip r 磁盘查看指定目录下文件的大小，排序并只显示TOP10 1du -sh * | sort -hr | head -n 10 123456sudo find /var/lib/docker/containers -name *.logsudo du -d1 -h /var/lib/docker/containers | sort -hsudo du -d1 -h /var/lib/docker | sort -hdocker inspect --format=&#x27;&#123;&#123;.LogPath&#125;&#125;&#x27; iwms-openapi 磁盘挂载 du -sh *: 查看目录占用的空间情况 df -h：查看磁盘占用情况 df -T：查看所有磁盘的文件系统类型(type) fdisk -l：查看所有被系统识别的磁盘 mount -t type device dir：挂载device到dir 123fdisk /dev/sdb# 格式化分区mkfs -t ext4 /dev/vdb1 mkfs -t ext4 /dev/vdb1: 格式化分区 123456umount /data ##卸载data目录下分区# 分区的UUIDls -l /dev/disk/by-uuid/# 开机自动挂载vim /etc/fstab /dev/sdb /data ext4 defaults 0 0 12# 查看硬盘以及分区情况lsblk 123456789101112131415# 比如要扩充 /var# 在创建好文件系统后 新建临时挂载点 storagemkdir /storage# 将/dev/sdb1挂载到/storage下mount /dev/sdb1 /storage# 拷贝/var下的所有内容到新的硬盘cp -pdr /var /storage# 或在/var 目录下执行：find . -depth -print | cpio - pldvm /temp# 删除当前/var目录下的内容rm -rf /var/*# 重新挂载硬盘到/var目录umount /dev/sdb1mount /dev/sdb1 /var# 过程中若提示磁盘忙，使用fuser找出将正在使用磁盘的程序并结束掉；fuser -m -v /var 123partprobe /dev/sdapartx -a /dev/sdb# 验证情况 软连接删除后，存放的数据不删除 1234ln –snf /var/www/test1 /var/test # 意不要在后面加 ”/”rm –rf &lt;软链接名称&gt;ln –snf &lt;新的源文件或目录&gt; &lt;目标文件或目录&gt; 目录文件 重要的配置文件位置 用户相关 /etc/passwd： 用户列表文件，是否是系统用户，是否可以登录 /etc/group： 用户组列表文件 /etc/sudoers: 可以有 sudo 权限的配置 /etc/profile： 环境变量配置 /etc/hosts： host 文件位置 /etc/hostnames: 主机名 /etc/sysconfig/network-scripts/ifcfg-ensxx: 网卡设置 /etc/yum.repos.d/xxx.repo： 软件安装源 /var/logs： 日志存放位置 执行路径 /usr/local/bin、/usr/bin、/usr/local/sbin、/usr/sbin /home//bin: 可执行路径，常通过 ln -s … 的方式使全局可用 文件操作 基本操作， ls、touch、mkdir、cd、rm、cp、mv、pwd 1234567891011121314# 复制、移动、重命名cp -r /user/newTest /test mv /test/newTest /usr mv aaa bbbrm -rf /user/newTst# 文件的查看cat -n /etc/passwd tail /etc/passwdtail -3 /etc/passwdtail -f nohup.out# 文件压缩与解压tar -cvf &lt;zip_name&gt; &lt;folder_name&gt;# 复制并重命名copy -a nginx_playbooks wordpress_playbooks 文件上传下载 文件下载： wget、curl 文件上传： scp 1scp tar 命令 -z, –gzip, –gunzip, –ungzip 通过 gzip 过滤归档 -c, –create: 创建一个新归档 -v, –verbose: 详细地列出处理的文件 -f, –file=ARCHIVE: 使用归档文件或 ARCHIVE 设备 -C, –directory=DIR: 改变至目录 DIR -x, –extract, –get: 从归档中解出文件 -j, –bzip2 : 通过 bzip2 过滤归档 -t, –list: 列出归档内容 打包 1tar -cvf /mydata/etc.tar /etc 打包并压缩 1tar -zcvf /mydata/etc.tar.gz /etc 用bzip2压缩文件夹/etc到文件/etc.tar.bz2： 1tar -jcvf /mydata/etc.tar.bz2 /etc 解压缩到指定目录 1tar -zxvf etc.tar.gz -C /usr/local/ 找出指定时间范围内的日志文件，打包传输 12345678910find . -name &quot;*04-29*&quot; -exec cp &#123;&#125; 200428_200505 \\;find . -name &quot;*04-28*&quot; -exec cp &#123;&#125; 200428_200505 \\;find . -name &quot;*04-30*&quot; -exec cp &#123;&#125; 200428_200505 \\;find . -name &quot;*04-31*&quot; -exec cp &#123;&#125; 200428_200505 \\;find . -name &quot;*05-01*&quot; -exec cp &#123;&#125; 200428_200505 \\;find . -name &quot;*05-02*&quot; -exec cp &#123;&#125; 200428_200505 \\;find . -name &quot;*05-03*&quot; -exec cp &#123;&#125; 200428_200505 \\;find . -name &quot;*05-04*&quot; -exec cp &#123;&#125; 200428_200505 \\;find . -name &quot;*05-05*&quot; -exec cp &#123;&#125; 200428_200505 \\;tar -zcvf 200428_200505.tar.gz 200428_200505/ 常用工具find -maxdepth levels -atime n File was last accessed n*24 hours ago. -mtime n File’s data was last modified n24 hours ago. -size n[cwbkMG] -type c File is of type c 12# 找出指定目录下文件的个数 find DIR_NAME -type f | wc -l 1234# 找出 /opt 目录下大于 800 M 的文件find /opt -type f -size +800M -print0 | xargs -0 du -h | sort -nr # 找到文件并移到 opt 目录 find / -name &quot;*tower*&quot; -exec rm &#123;&#125; \\; 12# 找出系统中占用容量最大的前 12 个目录 du -hm --max-depth=2 | sort -nr | head -12 12# 当前目录搜索lin开头的文件，然后用其搜索后的结果集，再执行ls -l的命令（这个命令可变，其他命令也可以），其中 -exec 和 &#123;&#125; \\; 都是固定格式 find . -name &quot;lin*&quot; -exec ls -l &#123;&#125; \\; grep-v: 过滤掉 -i: 忽略大小写 -n: 打印行号 -H, --with-filename: 打印文件名 -r: 递归处理 -d, --directories=ACTION: 目录处理策略, read,recurse,skip -c/--count: 打印匹配的数量 -w: 匹配整个词 -x: 整行 查看指定进程并过滤掉 grep 自身 1ps -ef | grep namenode | grep -v grep 正则匹配 123grep &quot;python|PYTHON&quot; filegrep -E &quot;python|PYTHON&quot; filegrep -F &quot;py.*&quot; file 打印匹配行的前后5行 1grep -5 &#x27;parttern&#x27; INPUT_FILE 12grep Full gclogs/fanruan.gc.log.2020-07-26grep -n Full gclogs/fanruan.gc.log.2020-07-26 搜索指定字符，并显示匹配到的行号 1grep -n man /etc/man_db.conf 1233:# This file is used by the man-db package to configure the man and cat paths.4:# It is also used to provide a manpath for those without one by examining5:# their PATH environment variable. For details see the manpath(5) man page. 查看单个/多个文件某字符出现的次数 12345678910grep Full gclogs/* | wc -l# countgrep -c processor /proc/cpuinfo# Show filename:countgrep -c Full gclogs/*gclogs/fanruan.gc.log:1gclogs/fanruan.gc.log.2020-07-17:434gclogs/fanruan.gc.log.2020-07-18:1gclogs/fanruan.gc.log.2020-07-19:3... 查看某个配置文件，排除掉里面以 # 开头的注释内容： 1grep &#x27;^[^#]&#x27; /etc/openvpn/server.conf 查看某个配置文件，排除掉里面以 # 开头和 ; 开头的注释内容： 1grep &#x27;^[^#;]&#x27; /etc/openvpn/server.conf sed 轻量级流编辑器，一般用来处理文本类文件 非交互式的编辑器, 它不会修改文件，除非使用 shell 重定向来保存结果。默认情况下，所有的输出行都被打印到屏幕上 sed -i 会实际写入 p 参数表示打印，一般配合 -n（安静模式）进行使用 c. 替换 s： 搜索并替换 123# 显示网络接口的 Ip 地址 ifconfig eth0 |grep &#x27;inet&#x27; |sed &#x27;s/^.*inet//g&#x27; |sed &#x27;s/netmask.*$//g&#x27; |sed -n &#x27;1p&#x27;192.168.199.183 12# 显示第 7 ~ 10 行内容sed -n &#x27;7,10p&#x27; /opt/log4j2.properties 12# 将文件中每一行以 # 开头的都替换掉空字符并展示sed &#x27;s/^#*//g&#x27; /opt/log4j2.properties 12# 将 1 ~ 4 行内容替换成 GitNavi.comcat -n /opt/log4j2.properties |sed &#x27;1,4c GitNavi.com 日期和日历 date +%Y: 显示当前年份 date +%m: 显示当前⽉份 date +%d: 显示当前是哪⼀天 date &quot;+%Y-%m-%d %H:%M:%S&quot; : 按照指定格式显示 date -d &#39;1 days ago&#39;: 显示前⼀天⽇期 date -d yesterday +&quot;%Y-%m-%d&quot;: 同上 date -d next-day +&quot;%Y-%m-%d&quot;: 显示明天⽇期 date -d &#39;next monday&#39;: 显示下周⼀时间 date -s 字符串时间: 设置系统时间 cal查看日历。cal -3: 查看当前，上个，下个月的日历cal 2020: 查看指令年份的 用户与权限 open files: 可打开的文件限制 file size: 可创建单个文件的最大大小? max memory size: 最大可用的内存大小 stack size: 最大可使用的栈大小 1234567891011121314151617ulimit -acore file size (blocks, -c) 0data seg size (kbytes, -d) unlimitedscheduling priority (-e) 0file size (blocks, -f) unlimitedpending signals (-i) 59459max locked memory (kbytes, -l) 64max memory size (kbytes, -m) unlimitedopen files (-n) 1024pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200real-time priority (-r) 0stack size (kbytes, -s) 8192cpu time (seconds, -t) unlimitedmax user processes (-u) 59459virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited -代表⽂件 d 代表⽬录 c 字符流， s socket p 管道 l 链接⽂档(link ﬁle) b 设备⽂件 12345chmod [&#123;ugoa&#125;&#123;+-=&#125;&#123;rwx&#125;] [⽂件或⽬录] [mode=421 ] [⽂件或⽬录]# 改变⽂件或者⽬录的所有者chown [最终⽤户] [⽂件或⽬录] # 改变⽂件或者⽬录的所属组chgrp [最终⽤户组] [⽂件或⽬录] 12345678910useradd &lt;user-name&gt;passwd &lt;user-name&gt;# 切换⽤户，只能获得⽤户的执⾏权限，不能获得环境变量su &lt;user-name&gt; # 切换到⽤户并获得该⽤户的环境变量及执⾏权限su - &lt;user-name&gt;# 删除⽤户但保存⽤户主⽬录userdel &lt;user-name&gt;# ⽤户和⽤户主⽬录，都删除userdel -r ⽤户名 123# 让用户支持权限命令visudo%vagrant ALL=(ALL) ALL /etc/sudoers Allow root to run any commands anywhere root ALL=(ALL) ALL hadoop ALL=(ALL) ALL cat /etc/passwd 查看创建了哪些⽤户 12345usermod -g &lt;⽤户组&gt; &lt;⽤户名&gt;groupadd &lt;组名&gt; &lt;代表⽬录&gt;groupdel &lt;组名&gt;groupmod -n &lt;新组名&gt; &lt;⽼组名&gt;cat /etc/group Refs 《UNIX 编程艺术》","tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"}]},{"title":"Linux 常用服务","date":"2020-11-10T13:21:25.000Z","path":"2020/11/10/Linux-常用服务/","text":"SSH 默认安装好客户端，对于海外的服务器进行转发有时连接效果较好 配置配置文件存放在 ~/.ssh/config 文件中 1vim ~/.ssh/config 端口更改 123Port=1022service sshd restart 12345# 如果 60s 内没用任何数据,将会自动断开。vim /etc/ssh/ssh_config#添加ServerAliveInterval 60ServerAliveCountMax 3 禁止密码登陆，可避密码泄漏 12345678910# 禁用root账户登录(非必要)PermitRootLogin no# 是否允许用户自行使用成对的密钥系统进行登入行为RSAAuthentication yes# 是否让 sshd 去检查用户home目录的权限数据StrictModes noPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keys# 禁止密码登录PasswordAuthentication no SSH 免密码密钥认证 1234567ssh-keygen -t rsa# 将公钥传递给远程 root ssh-copy-id -i /root/.ssh/id_rsa.pub root@linux121ssh-copy-id -i /root/.ssh/id_rsa.pub root@linux122ssh-copy-id -i /root/.ssh/id_rsa.pub root@linux123# 指定端口传递ssh-copy-id -i /root/.ssh/id_rsa.pub -oPort=6000 root@127.0.0.1 SSH 文件传输 12# 通过 SSH 复制远程服务器指定文件到本地scp root@linux121:/root/frp_0.13.0_linux_amd64.tar.gz frp.tar.gz SSH 连接执行命令 连接上指定的机器，同时执行指定的命令，封装 zookeeper 集群的启动 12345678910echo &quot;start zookeeper server...&quot;if(($#==0));then echo &quot;no params&quot;;exit;fihosts=&quot;linux121 linux122 linux123&quot;for host in $hostsdo ssh $host &quot;source /etc/profile; $1&quot;done crond Linux 自带的定时调度服务 查看查看用户的定时任务 1234# 查看当前用户的定时任务crontab -l# 查看指定用户的定时任务crontab -u &lt;username&gt; -l 按照时间查看定时任务 查看每日的任务 1ls -la /etc/cron.daily/ 1234drwxr-xr-x. 2 root root 4096 Sep 14 15:16 .drwxr-xr-x. 76 root root 4096 Nov 10 21:15 ..-rwx------. 1 root root 219 Apr 1 2020 logrotate-rwxr-xr-x. 1 root root 618 Oct 30 2018 man-db.cron 查看每小时的任务 1ls -la /etc/cron.hourly/ 查看每周的任务 1ls -la /etc/cron.weekly/ 查看每月的任务 1ls -la /etc/cron.monthly/ 查看 /etc/crontab 1less /etc/crontab 查看任务执行日志 1tail -f /var/log/cron 定义任务1234567+---------------- minute (0 - 59)| +------------- hour (0 - 23)| | +---------- day of month (1 - 31)| | | +------- month (1 - 12)| | | | +---- day of week (0 - 6) (Sunday&#x3D;0 or 7)| | | | |* * * * * command to be executed 定时删除日志 每天凌晨2点删除 /usr/apps/logs 目录下7天前包含 log 的日志文件 10 2 * * * &#x2F;usr&#x2F;bin&#x2F;find &#x2F;usr&#x2F;apps&#x2F;logs&#x2F; -type f -mtime +7 -name &quot;*log*&quot; -exec rm -rf &#123;&#125; \\; journal 日志查看服务 Linux 日志分类 内核及系统日志： 日志数据由系统服务 syslog 统一管理 用户日志：这种日志数据用于记录 Linux 系统用户登录及退出系统的相关信息 程序日志：有些应用程序运会选择自己来独立管理一份日志文件, 通过配置日志格式和回滚策略 Linux 中常用日志文件 /var/log/cron： 与定时任务相关的日志信息 /var/log/maillog： 与邮件相关的日志信息 /var/log/secure： 与安全相关的日志信息, 可基于此日志过滤攻击的 IP /var/log/boot.log： 守护进程启动和停止相关的日志消息 /var/log/messages / /var/log/syslog： 存储所有的全局系统活动数据,包括启动、IO错误、网络错误、程序故障等 /varlog/dmesg：记录Linux系统在引导过程中的各种事件信息 /var/log/lastlog：最近几次成功登录事件和最后一次不成功登录事件 /var/log/wtmp：记录每个用户登录、注销及系统启动和停机事件 /var/log/rpmpkgs： RPM软件包日志 /var/log/kern： 存储内核的错误和警告数据，用于排除与定制内核相关的故障 /var/log/btmp： 记录错误的登陆尝试 1journalctl -f -n 1000 logrotate 日志回滚服务 /etc/logrotate.d/ 12345678910tree &#x2F;etc&#x2F;logrotate.d&#x2F;etc&#x2F;logrotate.d├── bootlog├── chrony├── ppp├── subscription-manager├── syslog├── vsftpd├── wpa_supplicant└── yum 使用配合 cron 实现日志的分割压缩处理 123logrotate /etc/logrotate.d/nginxlogrotate -d /etc/logrotate.d/nginxlogrotate -vf /etc/logrotate.d/nginx 123456789101112/var/log/nginx/access_log &#123; rotate 7 size 5k dateext dateformat -%Y-%m-%d missingok compress sharedscripts postrotate test -r /var/run/nginx.pid &amp;&amp; kill -USR1 `cat /var/run/nginx.pid` endscript&#125; How to limit nginx access log file size and compress? firewalld防火墙状态查看 1systemctl status firewalld iptables1234567891011iptables -nvL # 开放指定的端口iptables -I INPUT -p tcp --dport 9000 -j ACCEPT# 暴露docker swarm需要的端口，如果不使用docker swarm不需要打开端口iptables -A INPUT -p tcp --dport 2377 -j ACCEPTiptables -A INPUT -p tcp --dport 7946 -j ACCEPTiptables -A INPUT -p udp --dport 7946 -j ACCEPTiptables -A INPUT -p tcp --dport 4789 -j ACCEPTiptables -A INPUT -p udp --dport 4789 -j ACCEPT ntpd 时间同步工具 123yum install ntpntpdate time.pool.aliyun.comsystemctl enable ntpd 三台机器时钟同步 时间同步的⽅式：在集群中找⼀台机器，作为时间服务器。 通过⽹络连接外⽹进⾏时钟同步, 此台机器需要保证能连上外⽹。 集群中其他机器与这台机器定时的同步时间，每隔一段时间(⼗分钟)同步⼀次时间。 1.时间服务器配置（必须root⽤户） 第⼀步:确定是否安装了ntpd的服务 1yum -y install ntpd 启动ntpd的服务 service ntpd start 设置ntpd的服务开机启动 chkconfig ntpd on 第⼀步:确定是否安装了ntpd的服务 rpm -qa | grep ntpd 第⼆步:编辑/etc/ntp.conf 1234567891011121314编辑第⼀台机器的/etc/ntp.confvim /etc/ntp.conf # 在⽂件中添加如下内容 restrict 192.168.80.0 mask 255.255.255.0 nomodify# 注释⼀下四⾏内容 #server 0.centos.pool.ntp.org #server 1.centos.pool.ntp.org #server 2.centos.pool.ntp.org #server 3.centos.pool.ntp.org # 去掉以下内容的注释，如果没有这两⾏注释，那就⾃⼰添加上 server 127.127.1.0 # local clock fudge 127.127.1.0 stratum 10 配置以下内容，保证BIOS与系统时间同步添加⼀⾏内容 12vim /etc/sysconﬁg/ntpdSYNC_HWLOCK=yes 第三步： 重新启动ntpd 12service ntpd statusservice ntpd start 使NTP服务可以在系统引导的时候⾃动启动 ： 1chkconfig ntpd on 2.其他机器配置（必须root⽤户） 第⼀步：在其他机器配置10分钟与时间服务器同步⼀次 crontab -e 编写脚本 另外两台机器与192.168.80.121进⾏时钟同步 1*/10 * * * * /usr/sbin/ntpdate 192.168.80.121 第⼆步：修改任意机器时间, 进行验证 1date -s &quot;2020-12-31 11:11:11&quot; 第三步：⼗分钟后查看机器是否与时间服务器同步 1date","tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"}]},{"title":"Hive 安装","date":"2020-11-09T17:34:47.000Z","path":"2020/11/10/Hive-安装/","text":"软件安装安装MySQL 数据库 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;# 安装 Docker# &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;curl -fsSL https:&#x2F;&#x2F;get.docker.com | bash -s docker --mirror Aliyuncurl -sSL https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker | shsudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-enginesudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2sudo yum-config-manager \\ --add-repo \\ http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.reposystemctl start dockersystemctl daemon-reloadsystemctl enable docker# &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;# 安装 MySQL、配置 MYSQL# &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;mysql_volume_root&#x3D;&#x2F;opt&#x2F;janhen&#x2F;software&#x2F;mysqlmysql_container_name&#x3D;Mysql_linux121docker rm -f mysql_container_namedocker run -d --name mysql_container_name \\ --privileged \\ --restart&#x3D;always \\ -v $mysql_volume_root&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql \\ -v $mysql_volume_root&#x2F;conf&#x2F;mysqld.cnf:&#x2F;etc&#x2F;mysql&#x2F;mysql.conf.d&#x2F;mysqld.cnf \\ -v $mysql_volume_root&#x2F;logs:&#x2F;var&#x2F;log&#x2F;mysql \\ -p 3306:3306 \\ -e MYSQL_ROOT_PASSWORD&#x3D;hadoop123janhen \\ mysql:5.7docker exec -it $mysql_container_name \\ mysql -uroot -p hadoop123janhen \\ -e &quot;SET GLOBAL validate_password_policy&#x3D;0&quot;CREATE user &#39;hive&#39;@&#39;%&#39; IDENTIFIED BY &#39;youpassword&#39;;GRANT ALL ON *.* TO &#39;hive&#39;@&#39;%&#39;;FLUSH PRIVILEGES;mysql -uhive -p Hive123Hadoop 复制 MySQL 驱动 复制 mysql-connector-java-5.1.46.jar 拷贝到 $HIVE_HOME/lib 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;# 安装 Hive、配置 Hive# &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;# 配置环境变量vi &#x2F;etc&#x2F;profilesource &#x2F;etc&#x2F;profile#.#├── bin#│ ├── beeline#│ ├── hive#│ ├── hive-config.sh#│ ├── hiveserver2#│ └── schematool ...#├── conf#│ ├── hive-default.xml.template#│ ├── hive-env.sh.template#│ ├── hive-site-local.xml ...#├── hcatalog#│ ├── bin#│ ├── etc ...#├── lib36+fd#│ ├── asm-3.1.jar#│ ├── avro-1.7.7.jar#│ ├── derby-10.10.2.0.jar#│ ├── druid-common-0.9.2.jar#│ ├── findbugs-annotations-1.3.9-1.jar#│ ├── hbase-annotations-1.1.1.jar#│ ├── hbase-hadoop2-compat-1.1.1.jar#│ ├── hbase-hadoop2-compat-1.1.1-tests.jar#│ ├── hbase-hadoop-compat-1.1.1.jar#│ ├── hive-beeline-2.3.7.jar#│ ├── hive-exec-2.3.7.jar#│ ├── hive-hcatalog-core-2.3.7.jar#│ ├── hive-hplsql-2.3.7.jar#│ ├── hive-jdbc-2.3.7.jar#│ ├── mysql-metadata-storage-0.9.2.jar#│ ├── netty-3.6.2.Final.jar#│ ├── snappy-java-1.0.5.jar#│ └── zookeeper-3.4.6.jar#│ └── ...#└── scripts# └── ... 配置环境变量 12345vi /etc/profileexport HIVE_HOME=/opt/janhen/servers/hive-2.3.7export PATH=$PATH:$HIVE_HOME/binsource /etc/profile 配置本地元数据管理$HIVE_HOME/conf/hive-site.xml 当启动一个hive 服务时，其内部会启动一个metastore服务。Hive根据 hive.metastore.uris 参数值来判断，如果为空，则为本地模式。 缺点：每启动一次hive服务，都内置启动了一个metastore；在hive-site.xml中暴露 的数据库的连接信息； 优点：配置较简单，本地模式下hive的配置中指定mysql的相关信息即可。 在xml文件中 &amp;amp; 表示 &amp; 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;!-- hive元数据的存储位置 --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://linux121:3306/hivemetadata?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定驱动程序 --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;!-- 连接数据库的用户名 --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;!-- 连接数据库的口令 --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;youpassword&lt;/value&gt; &lt;/property&gt; &lt;!-- 数据默认的存储位置(HDFS) --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;!-- 在命令行中，显示当前操作的数据库 --&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 在命令行中，显示数据的表头 --&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 操作小规模数据时，使用本地模式，提高效率 --&gt; &lt;property&gt; &lt;name&gt;hive.exec.mode.local.auto&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 对数据进行初始化 1schematool -dbType mysql -initSchema 配置远程元数据管理12345678910111213141516171819202122232425262728293031323334&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;!-- 使用 metastore 服务管理元数据 --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://linux121:9083,thrift://linux123:9083&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.client.socket.timeout&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt; &lt;/property&gt; &lt;!-- 数据默认的存储位置(HDFS) --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;!-- 在命令行中，显示当前操作的数据库 --&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 在命令行中，显示数据的表头 --&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 操作小规模数据时，使用本地模式，提高效率 --&gt; &lt;property&gt; &lt;name&gt;hive.exec.mode.local.auto&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置日志Hive的log默认存放在 /tmp/root 目录下 123vi $HIVE_HOME/conf/hive-log4j2.properties# 增加配置property.hive.log.dir = /opt/janhen/hive-2.3.7/logs 配置参数配置方式 用户自定义配置文件(hive-site.xml) 启动hive时指定参数(-hiveconf) hive命令行指定参数(set) 配置的优先级 1set ---&gt; -hiveconf ---&gt; hive-site.xml ---&gt; hive-default.xml 启动时指定参数 12# 启动时指定参数hive -hiveconf hive.exec.mode.local.auto=true 命令执行时更改 123-- 查看全部参数 hive&gt; set;hive&gt; set hive.exec.mode.local.auto; 启动与验证1234567891011121314151617# ================================================# 启动 Hive Metastore# ================================================cd $HIVE_HOME/binnohup hive --service metastore &amp;lsof -i:9083tail -f -n 500 nohup.out# ================================================# 启动 HiveServer2# ================================================nohup hiveserver2 &amp;lsof -i:10000# 浏览器验证http://linux123:10002/ 初始化元数据 - MySql 1schematool -dbType mysql -initSchema 连接 Hive 12hivehive&gt; show functions; 下载网址：http://archive.apache.org/dist/hive/ 文档网址：https://cwiki.apache.org/confluence/display/Hive/LanguageManual","tags":[{"name":"Hive","slug":"Hive","permalink":"http://example.com/tags/Hive/"}]},{"title":"Hadoop 集群环境搭建","date":"2020-11-09T17:23:23.000Z","path":"2020/11/10/Hadoop-集群环境搭建/","text":"环境准备集群规划 框架 linux121 linux122 linux123 HDFS NameNode、DataNode DataNode SecondaryNameNode、DataNode YARN NodeManager NodeManager NodeManager、ResourceManager 安装 JDK 12345678910111213mkdir -p /usr/local/javawget http://download.janhen.com/tools/jdk-8u251-linux-x64.tar.gztar -zxvf jdk-8u251-linux-x64.tar.gz -C /usr/local/javavi /etc/profileexport JAVA_HOME=/usr/local/java/jdk1.8.0_251export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATHsource /etc/profileln -s /usr/local/java/jdk1.8.0_251/bin/java /usr/bin/javajava -version SSH 免密登陆 123456789mkdir /root/.ssh chmod 700 /root/.sshcd ~/.sshssh-keygen -t rsa -P &quot;&quot;cat id_rsa.pub &gt;&gt; authorized_keysscp authorized_keys linux122:/root/.ssh/scp authorized_keys linux123:/root/.ssh/ 本地域名配置 123xxx linux121xxx linux122xxx linux123 准备安装的目录 12mkdir -p &#x2F;opt&#x2F;janhen&#x2F; 12345678910111213141516171819202122232425262728# 防火墙关闭systemctl status firewalldvi /etc/selinux/config# ================================================# 软件安装# ================================================mkdir -p /opt/janhen/cd /opt/janhenwget https://archive.apache.org/dist/hadoop/common/hadoop-2.9.2/hadoop-2.9.2.tar.gztar -zxvf hadoop-2.9.2.tar.gz# 环境变量配置vim /etc/profileexport JAVA_HOME=/usr/local/java/jdk1.8.0_251export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATHexport HADOOP_HOME=/opt/janhen/hadoop-2.9.2export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinsource /etc/profileln -s /usr/local/java/jdk1.8.0_251/bin/java /usr/bin/javajava -versionhadoop version 配置1234567891011121314151617# ================================================# 配置# ================================================# 配置# JDK路径明确配置给HDFSvi $HADOOP_HOME/etc/hadoop/hadoop-env.shvi $HADOOP_HOME/etc/hadoop/core-site.xmlvi $HADOOP_HOME/etc/hadoop/hdfs-site.xmlvi $HADOOP_HOME/etc/hadoop/mapred-env.shvi $HADOOP_HOME/etc/hadoop/mapred-site.xmlvi $HADOOP_HOME/etc/hadoop/yarn-env.shvi $HADOOP_HOME/etc/hadoop/yarn-site.xmlvi $HADOOP_HOME/etc/hadoop/slaves# 更改权限chown -R root:root $HADOOP_HOME 验证1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# ================================================# 手动启动# ================================================hadoop namenode -formathadoop_tmp_dir=/home/hadoop/data/tmpmetadata_file=fsimage_0000000000000000000cd $hadoop_tmp_dir/dfs/name/current# 启动NameNodehadoop-daemon.sh stop namenodehadoop-daemon.sh start namenode# 启动DataNodehadoop-daemon.sh start datanode# 启动 yarnyarn-daemon.sh start resourcemanager# 启动 yarn 从即诶单yarn-daemon.sh start nodemanagerjps | grep NameNodejps | grep DataNodejps | grep ResourceManagerjps | grep NodeManager# ================================================# 集群启动# ================================================start-dfs.shstart-yarn.sh# ================================================# 测试集群# ================================================hdfs dfs -mkdir -p /test/inputcat &gt; /root/test.txt &lt;&lt;EOFhello hdfs ...EOF#上传linxu文件到Hdfshdfs dfs -put /root/test.txt /test/inputhdfs dfs -get /test/input/test.txt# --------测试词频统计执行--------cat &gt; /root/wc.txt &lt;&lt;EOFhadoop mapreduce yarnhdfs hadoop mapreducemapreduce yarn sparkflink hqlEOFhdfs dfs -ls /hdfs dfs -du /hdfs dfs -mkdir /wcinputhdfs dfs -put /root/wc.txt /wcinputhadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar wordcount /wcinput /wcoutputhdfs dfs -cat /wcoutput/part-r-00000# --------日志收集--------# 启动历史服务器mr-jobhistory-daemon.sh start historyserverjps | grep JobHistoryServercurl http://linux121:19888/jobhistory# --------日志配置--------yarn-daemon.sh stop resourcemanageryarn-daemon.sh stop nodemanagermr-jobhistory-daemon.sh stop historyserveryarn-daemon.sh start resourcemanageryarn-daemon.sh stop resourcemanageryarn-daemon.sh start nodemanagermr-jobhistory-daemon.sh start historyserver19888,9000,50090,50010,6379,9092,8088,8080# 验证地址jobhistorylinux121:19888 RefsHadoop 安装包地址","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://example.com/tags/Hadoop/"}]},{"title":"旋转数组类问题","date":"2020-11-09T17:03:57.000Z","path":"2020/11/10/旋转数组类问题/","text":"数组旋转 k 位 189. Rotate Array 123456Input: [1,2,3,4,5,6,7] and k = 3Output: [5,6,7,1,2,3,4]Explanation:rotate 1 steps to the right: [7,1,2,3,4,5,6]rotate 2 steps to the right: [6,7,1,2,3,4,5]rotate 3 steps to the right: [5,6,7,1,2,3,4] 123456789public void rotate(int[] nums, int k) &#123; if (nums.length == 1) return ; int n = nums.length; k = k % n; // prevent unnecessary rotate reverse(nums, 0, n-k-1); reverse(nums, n-k, n-1); reverse(nums, 0, n-1);&#125;","tags":[{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"Leetcode","slug":"Leetcode","permalink":"http://example.com/tags/Leetcode/"}]},{"title":"二分查找及变种","date":"2020-11-09T16:56:19.000Z","path":"2020/11/10/二分查找/","text":"二分查找（1） 算法 （2） 复杂度 O(logN) （3）性质 适用于处理 ceil、floor 等操作； 配合索引相当于是实现了跳表结构； 1、 普通二分查找 12345678910111213int binarySearch(int[] arr, int target) &#123; int lo = 0, hi = arr.length - 1; while (lo &lt;= hi) &#123; int mid = lo + (hi - lo) / 2; if (arr[mid] == target) return mid; if (arr[mid] &lt; target) lo = mid + 1; else hi = mid - 1; &#125; return -1;&#125; 2、 带有重复元素的二分查找-最先&amp;最后 （1） 查找含有重复元素的数组集合中元素第一次出现的位置 在相等的情况下，进行判断决定是否进行缩小范围或找到对应的值； 12345678910111213int binarySearchFirst(int[] nums, int lo, int hi, int aim) &#123; while (lo &lt;= hi) &#123; int mid = lo + (hi - lo) / 2; if (nums[mid] == aim) &#123; if (mid == 0 || nums[mid-1] != nums[mid]) return mid; else hi = mid - 1; &#125; else if (nums[mid] &lt; aim) &#123; lo = mid + 1; &#125; else hi = mid - 1; &#125; return -1;&#125; （2） 查找含有重复元素的数组集合中元素最后一次出现的位置 1234567891011121314int binarySearchLast(int[] nums, int key, int lo, int hi) &#123; while (lo &lt;= hi) &#123; int mid = (hi - lo) / 2 + lo; if (nums[mid] == key) &#123; if (mid == nums.length - 1 || nums[mid] != nums[mid + 1]) return mid; else lo = mid + 1; &#125; else if (nums[mid] &lt; key) &#123; lo = mid + 1; &#125; else &#123; hi = mid - 1; &#125; &#125; return -1;&#125; 3、 二分查找-大于&amp;小于 （1） 查找小于等于给定元素的最小元素在数组中的位置 123456789101112public int binarySearchFloor(int[] nums, int key, int lo, int hi) &#123; while (lo &lt;= hi) &#123; int mid = lo + (hi - lo) / 2; if (nums[mid] &lt;= key) &#123; if (mid == nums.length-1 || nums[mid+1] &gt; key) return mid; else lo = mid + 1; &#125; else &#123; hi = mid - 1; &#125; &#125; return -1;&#125; （2） 查找大于等于给定元素的最小元素在数组中的位置 123456789101112public int binarySearchCeil(int[] nums, int key, int lo, int hi) &#123; while (lo &lt;= hi) &#123; int mid = lo + (hi - lo) / 2; if (nums[mid] &gt;= key) &#123; if (mid == 0 || nums[mid-1] &lt; key) return mid; else hi = mid - 1; &#125; else &#123; lo = mid + 1; &#125; &#125; return -1;&#125; 4、 带偏移的二分查找 用于旋转数组的查找，偏移后数据有序； 1234567891011121314int binarySearchOffset(int[] nums, int key, int offset) &#123; int lo = 0, hi = nums.length - 1; while (lo &lt;= hi) &#123; int mid = lo + (hi - lo) / 2; int realMid = (mid + offset) % nums.length; if (nums[realMid] == key) &#123; return mid; &#125; else if (nums[mid] &lt; key) &#123; lo = mid + 1; &#125; else hi = mid - 1; &#125; return -1;&#125;","tags":[{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"基础排序及变种","date":"2020-11-09T16:51:39.000Z","path":"2020/11/10/基础排序/","text":"排序 基本的排序算法，以及变种 算法 稳定性 时间复杂度 空间复杂度 备注 选择排序 × N2 1 冒泡排序 √ N2 1 插入排序 √ N ~ N2 1 与初始的逆序度相关 希尔排序 × N 的若干倍乘于递增序列的长度 1 每次交换逆序度数量减少大于1 快速排序 × NlogN logN 三向切分快速排序 × N ~ NlogN logN 适用于有大量重复值 归并排序 √ NlogN N 堆排序 × NlogN 1 无法利用局部性原理 基础排序选择排序（1） 算法: 选择数组中最小的元素, 将它与数组的第一个元素交换, 之后开始次小元元素…（2） 复杂度 比较: N²/2, 交换: N最坏: O(n²) 最好: O(n²)， 平均: O(n²) （3） 性质: 运行时间与输入无关； 不稳定； 原地排序； 1234567891011void selectSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) return; for (int i = 0; i &lt; arr.length; i ++) &#123; int minIndex = i; for (int j = i + 1; j &lt; arr.length; j ++) &#123; if (arr[j] &lt; arr[minIndex]) minIndex = i; &#125; swap(arr,i, minIndex); &#125;&#125; 冒泡排序（1） 算法: 从左到右不断交换相邻逆序的元素, 经过一次循环确定最后一个元素到达最右侧存在传入数组已经有序的情况 （2） 复杂度分析: 最坏: O(n²) 最好: O(n), 集合有序, 需要进行一次冒泡 平均: O(n²) （3） 性质： 元素交换的次数为固定值, 原始数据的逆序度 需要三次赋值操作； 稳定； 原地排序； 1、基础冒泡 1234567void bubbleSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) return; for (int i = arr.length - 1; i &gt; 0; i --) // insure N-1~1 position, 0 must in correct position for (int j = 0; j &lt; i; j ++) if (arr[j] &gt; arr[j + 1]) swap(arr, j, j + 1);&#125; 2、 有序性优化 对于已经有序的数据，不进行元素交换。 12345678910111213void bubbleSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) return; boolean hasSorted = false; for (int i = arr.length - 1; i &gt; 0 &amp;&amp; !hasSorted; i --) &#123; // except bad condtion hasSorted = true; for (int j = 0; j &lt; i; j ++) &#123; if (arr[j] &gt; arr[j + 1]) &#123; // when equal not modify original order hasSorted = false; swap(arr, j, j + 1); &#125; &#125; &#125;&#125; 插入排序（1）算法: 将数组分为两部分，将后部分元素逐一与前部分元素比较，如果前部分元素比array[i]小，就将前部分元素往后移动。当没有比array[i]小的元素，即是合理位置，在此位置插入array[i]。 （2） 复杂度分析最坏: O(n²), 数组逆序, 需要 N²/2 比较 N²/2 交换最好: O(n), 正序, 需要 N-1 比较 0 次交换平均: O(n^2) N²/4 比较 N²/4 交换 （3） 性质: 复杂度取决于数组的初始顺序， 移动次数为逆序对的数量； 稳定； 原地排序； 1、基础插入排序 1234567public static void insertSortB(int[] arr) &#123; if (arr == null || arr.length &lt; 2) return; for (int i = 1; i &lt; arr.length; i++) for (int j = i; j &gt; 0 &amp;&amp; arr[j] &lt; arr[j - 1]; j --) swap(arr, j, j - 1);&#125; 2、 赋值优化 1234567891011void insertSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) return; for (int i = 1; i &lt; arr.length; i ++) &#123; int e = arr[i], j; // e current element, j should put position for (j = i; j &gt; 0; j --) &#123; if (e &lt; arr[j-1]) arr[j] = arr[j-1]; &#125; arr[j] = e; &#125;&#125; 3、链表实现插入排序 147. Insertion Sort List(Medium) 1 希尔排序（1） 算法：使用插入排序对间隔 h 的序列进行排序。通过不断减小 h，最后令 h=1，就可以使得整个数组是有序的。 （2） 复杂度分析： 希尔排序的运行时间达不到平方级别，使用递增序列 1, 4, 13, 40, … 的希尔排序所需要的比较次数不会超过 N 的若干倍乘于递增序列的长度。 （3） 性质： 交换不相邻元素，将逆序数量减少大于1； 基于原来的插入排序； 不稳定； 原地排序； 1234567891011121314void shellSort(int[] arr) &#123; int N = arr.length; int h = 1; while (h &lt; N/3) h = 3 * h + 1; while (h &gt; 0) &#123; for (int i = h; i &lt; N; i += h) &#123; for (int j = i; j &gt;= h; j -= h) &#123; if (arr[j] &lt; arr[j-h]) swap(arr, j, j - h); else break; &#125; &#125; h /= 3; &#125;&#125; 快速排序（1） 思想: 分治, 分区 （2） 复杂度:由每次选取的分割点控制最好: 每次分割点都为中间的元素， O(logN)最坏: 每次分割点都为最后元素 O(n²) （3） 性质: 每趟排序就有一个元素排在了最终的位置上，第n趟结束，至少有n个元素已经排在了最终的位置上； 非稳定 原地排序 （4） 归并 VS 快排:归并由下到上, 先处理子问题之后合并，快排由上到下, 先进行分区然后处理子问题； 归并非原地排序，需要辅助空间，快排通过原地分区函数实现原地排序； 归并排序为稳定的排序，保留原来相同值的顺序； （5） 优化: 三数取中法 随机选取法 1、随机取枢纽元 小数据集使用插入排序； 随机选择枢纽元比较； 12345678910111213void quickSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) return; quickSort(arr, 0, arr.length);&#125;void quickSort(int[] arr, int lo, int hi) &#123; if (hi - lo &lt; INSERTION_SORT_THRESHOLD) &#123; insertSort(arr, lo, hi); return; &#125; int j = partition(arr, lo, hi); quickSort(arr, lo, j - 1); quickSort(arr, j +1, hi);&#125; 快速选择； 123456789101112int partition(int[] arr, int lo, int hi) &#123; swap(arr,lo,lo +(int) Math.random() * (hi-lo+1); int pivot = arr[lo]; int i = lo, j = hi + 1; while (true) &#123; while (arr[++ i] &lt; pivot) if (i == hi) break; while (arr[-- j] &gt; pivot) if (j == lo) break; if (i &gt;= j) break; swap(arr, i, j); &#125; swap(arr, j, lo);&#125; 2、三路快排优化 对重复元素较多的情形优化； 函数返回重复元素第一次和最后一次出现位置； 类似荷兰国旗问题的处理； 相关： 75. Sort Colors 1234567891011121314int[] partition(int[] arr, int lo,int hi) &#123; int pivot = arr[lo]; int lt = lo - 1, gt = hi + 1; int i = lo; while (i &lt; gt) &#123; // 各个区间的语义 if (arr[i] == pivot) i ++; else if (arr[i] &lt; pivot) swap(arr, i ++, ++ lt); else swap(arr, i, -- gt); &#125; return new int[]&#123;lt+1, gt-1&#125;;&#125; 3、 三数取中值确定枢纽元优化 枢纽元的选取上进行优化； 选取边界和中间数将三处进行排序，选择中间元素作为枢纽元，并放入 [hi-1] 位置； 之后 [lo], [hi] 可以作为快排内循环的哨兵； 123456789// sort three element AND put hi-1 positionint medianOf3(int[] arr, int lo, int hi) &#123; int mid = lo + (hi - lo) / 2; if (arr[lo] &gt; arr[mid]) swap(arr, lo, mid); if (arr[lo] &gt; arr[hi]) swap(arr, lo, hi); if (arr[mid] &gt; arr[hi]) swap(arr, mid, hi); swap(arr, mid, hi - 1); return arr[hi - 1]; // pivot is mid value, and position is hi-1&#125; 1234567891011int partition(int[] arr, int lo, int hi, int pivot) &#123; // pivot original position hi - 1 int i = lo, j = hi - 1; while (true) &#123; while (arr[++ i] &lt; pivot); // NOTE: [hi-1] as sentinel while (arr[-- j] &gt; pivot); // [lo] as sentinel if (i &gt;= j) break; swap(arr, i, j); &#125; swap(arr, i, hi - 1); // put pivot as correct position return i;&#125; 1234567891011void quickSort(int[] arr, int lo, int hi) &#123; if (hi - lo &lt;= INSERTITION_SORT_THRESHOLD) &#123; insertSort(arr, lo, hi); return; &#125; int median = medianOf3(arr, lo, hi); int i = partition(arr, lo, hi, median); quickSort(arr, lo, i - 1); quickSort(arr,i +1, hi);&#125; 归并排序（1） 算法 （2） 复杂度 大部分为 O(NlogN) T(n) = 2T(n/2) + n = 2(2T(n/4) + n/2) + n = 4T(n/4) + 2n = 4(2T(n/8) + n/4) + 2n = 8T(n/8) + 3n = 8*(2T(n/16) + n/8) + 3n = 16T(n/16) + 4n …… = 2^k * T(n/2^k) + k * n （3） 性质 大数据量情况下出现无法分配空间情况； 稳定的排序； 非原地排序； 1、基础归并排序 ① 对排序的两个子数组 [lo,mid], [mid+1, hi]，在 [mid] &gt;= [mid+1] 数组整体有序情况下跳过合并； ② 分配当前两个数组对应的数组空间作为辅助； 1234567891011void mergeSort(int[] arr, int lo, int hi) &#123; if (hi - lo &lt; INSERTITION_SORT_THRESHOLD) &#123; insertSort(arr, lo, hi); return; &#125; int mid = lo + (hi - lo) / 2; mergeSort(arr, lo, mid); mergeSort(arr, mid + 1, hi); if (arr[mid] &gt; arr[mid + 1]) loopArrQueue merge(arr, lo, mid, hi);&#125; 123456789101112void merge(int[] arr, int lo, int mid, int hi) &#123; int[] aux = new int[hi - lo + 1]; int i = lo, j = mid + 1; for (int k = 0; k &lt; aux.length; k ++) &#123; if (i &gt; mid) aux[k] = arr[j ++]; else if (j &gt; hi) aux[k] = arr[i ++]; else if (arr[i] &lt; arr[j]) aux[k] = arr[i ++]; else aux[k] = arr[j ++]; &#125; for (int k = 0; k &lt; aux.length; k ++) arr[k + lo] = aux[k];&#125; 2、 自底向上的归并排序 考虑处理两种情况： 12P1 __ __ | __ __ | __ __ | _ i + sz &lt; arr.length to controlP2 __ __ | __ __ | __ __ | __ _ min&#123;i + sz + sz - 1, arr.length - 1&#125; to control sz 为两个子数组的区间大小 123456789void mergeSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) return; int N = arr.length; for (int sz = 1; sz &lt; N; sz += sz) &#123; for (int i = 0; i + sz &lt; N; i += sz + sz) &#123; loopArrQueue merge(arr, i, i + sz - 1, Math.min(i + sz + sz - 1, N-1)); &#125; &#125;&#125; 3、使用链表进行归并排序 找出中间节点，分割链表； 对分割的链表分别进行归并排序； 将链表合并； 相关： 148. Sort List(Medium) 12345678910111213public class ListNode implements Cloneable &#123; public int val; public ListNode next; public ListNode(int val) &#123; this.val = val; &#125; public ListNode(int val, ListNode next) &#123; this.val = val; this.next = next; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637public ListNode sortList(ListNode head) &#123; if (head == null || head.next == null) return head; // 1. find mid node and cut two list ListNode preMid = findPreMid(head); ListNode mid = preMid.next; preMid.next = null; // 2. handle two sub problem ListNode l1 = sortList(head); ListNode l2 = sortList(mid); // 3. merge result return merge(l1, l2); &#125; private ListNode findPreMid(ListNode head) &#123; ListNode pre = null, fast = head, slow = head; while (fast != null &amp;&amp; fast.next != null) &#123; pre = slow; slow = slow.next; fast = fast.next.next; &#125; return pre; &#125; private ListNode merge(ListNode l1, ListNode l2) &#123; if (l1 == null) return l2; if (l2 == null) return l1; if (l1.val &lt; l2.val) &#123; l1.next = merge(l1.next, l2); return l1; &#125; else &#123; l2.next = merge(l1, l2.next); return l2; &#125; &#125; 堆排序（1） 算法 （2） 复杂度 O(logN) （3） 性质 无法利用到现代处理器的缓存局部性原理，一般不使用； 不稳定； 原地排序，适用于嵌入式系统中内存小的情况； 1、 基础堆排序 先通过向堆中不断插入元素，向上调整形成堆结构； 之后不断删除堆顶元素实现排序； 1234567891011void heapSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) return; for (int i = 0; i &lt; arr.length; i ++) heapify(arr, i); // heapInsert int N = arr.length; while (N &gt; 0) &#123; // heapify: delete and adjust heap structure swap(arr, -- N, 0); sink(arr, N, 0); &#125;&#125; 123456void heapify(int[] arr, int k) &#123; while (arr[k] &gt; arr[(k - 1) / 2]) &#123; swap(arr, k, (k - 1) / 2); k = (k - 1) / 2; &#125;&#125; 1234567891011void sink(int[] arr, int N, int k) &#123; while (2 * k + 1 &lt; N) &#123; int j = 2 * k + 1; if (j + 1 &lt; N &amp;&amp; arr[j] &lt; arr[j + 1]) j ++; if (arr[k] &gt;= arr[j]) break; swap(arr, k, j); k = j; &#125;&#125; 2、算法优化 与 java.util.ProrityQueue 中实现逻辑相同 ① 通过 sink 向下调整进行优化； ② 下沉操作中使用赋值替代交换，常数级优化； 1234567891011void heapSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) return; int N = arr.length; for (int i = (N - 2) / 2; i &gt;= 0; i --) // build heap sink(arr, i, N); while (N &gt; 0) &#123; // delete max ⇔ put into last position swap(arr, 0, -- N); sink(arr, 0, N); &#125;&#125; 123456789101112void sink(int[] arr, int k, int N) &#123; int val = arr[k]; while (k * 2 + 1 &lt; N) &#123; int j = k * 2 + 1; if (j + 1 &lt; N &amp;&amp; arr[j] &lt; arr[j + 1]) j = j + 1; if (val &gt;= arr[j]) break; arr[k] = arr[j]; k = j; &#125; arr[k] = val;&#125;","tags":[{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"}]}]