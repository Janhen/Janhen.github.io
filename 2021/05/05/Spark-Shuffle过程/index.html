<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Spark-Shuffle 过程和原理 | Janhen</title><meta name="keywords" content="Spark"><meta name="author" content="Janhen"><meta name="copyright" content="Janhen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Shuffle按照一定的规则对数据重新分区的过程就是 Shuffle。需要 Shuffle 是因为某种具有共同特征的数据汇聚到一个计算节点上进行计算。 Shuffle 过程中包含了许多低效的操作，包括磁盘 IO、序列化、网络数据传输等。 Shuffle 分类  Hash Shuffle V1 两个严重问题：生成大量文件，占用文件描述符，同时引入 DiskObjectWriter 带来的 Write">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark-Shuffle 过程和原理">
<meta property="og:url" content="http://example.com/2021/05/05/Spark-Shuffle%E8%BF%87%E7%A8%8B/index.html">
<meta property="og:site_name" content="Janhen">
<meta property="og:description" content="Shuffle按照一定的规则对数据重新分区的过程就是 Shuffle。需要 Shuffle 是因为某种具有共同特征的数据汇聚到一个计算节点上进行计算。 Shuffle 过程中包含了许多低效的操作，包括磁盘 IO、序列化、网络数据传输等。 Shuffle 分类  Hash Shuffle V1 两个严重问题：生成大量文件，占用文件描述符，同时引入 DiskObjectWriter 带来的 Write">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2021-05-05T08:55:56.000Z">
<meta property="article:modified_time" content="2021-05-05T09:18:41.524Z">
<meta property="article:author" content="Janhen">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2021/05/05/Spark-Shuffle%E8%BF%87%E7%A8%8B/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-05-05 17:18:41'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Janhen" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="http://img.janhen.com/20210331220602f7HmbN.pnghttp://img.janhen.com/20210331220602f7HmbN.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">27</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div></div></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Janhen</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark-Shuffle 过程和原理</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-05-05T08:55:56.000Z" title="Created 2021-05-05 16:55:56">2021-05-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-05-05T09:18:41.524Z" title="Updated 2021-05-05 17:18:41">2021-05-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark-Shuffle 过程和原理"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h2><p><strong>按照一定的规则对数据重新分区的过程就是 Shuffle。</strong>需要 Shuffle 是因为某种具有共同特征的数据汇聚到一个计算节点上进行计算。</p>
<p>Shuffle 过程中包含了许多低效的操作，包括磁盘 IO、序列化、网络数据传输等。</p>
<p><strong>Shuffle 分类</strong></p>
<ul>
<li>Hash Shuffle V1<ul>
<li>两个严重问题：生成大量文件，占用文件描述符，同时引入 DiskObjectWriter 带来的 Writer Handler 的缓存也非常消耗内存。如果在 Reduce Task 时需要合并操作的话，会把数据放在一个 HashMap 中进行合并，如果数据量较大，很容易引发 OOM</li>
</ul>
</li>
<li>Hash Shuffle V2<ul>
<li>在 v1 基础上引入 File Consolidation</li>
<li>一个 Executor 上所有的 Map Task 生成的分区文件只有一份，即将所有的 Map Task 相同的分区文件合并，这样每个 Executor 上最多只生成 N 个分区文件。</li>
</ul>
</li>
<li>Sort Shuffle V1<ul>
<li>每个 Task 不会为后续的每个 Task 创建单独的文件，而是将所有对结果写入同一个文件。该文件中的记录首先是按照 Partition Id 排序，每个 Partition 内部再按照 Key 进行排序，Map Task 运行期间会顺序写每个 Partition 的数据，同时生成一个索引文件记录每个 Partition 的大小和偏移量。</li>
<li>在 Reduce 阶段，Reduce Task 拉取数据做 Combine 时不再采用 HashMap，而是采用 <strong>ExternalAppendOnlyMap</strong>，该数据结构在做 Combine 时，如果内存不足，会刷写磁盘，避免(AppendOnlyMap)大数据情况下的 OOM。</li>
<li>Sort Shuffle 解决了 Hash Shuffle 的所有弊端，但是因为需要其 Shuffle 过程需要对记录进行排序，所以在性能上有所损失。</li>
</ul>
</li>
<li>Tungsten-Sort Based Shuffle(Unsafe Shuffle)<ul>
<li>1.5+ 后开始钨丝计划，优化内存和 CPU 的使用，使用了堆外内存进一步提升 Spark 的性能。</li>
<li>将数据记录用二进制的方式存储，直接在序列化的二进制数据上 Sort 而不是在 Java 对象上。减少内存的使用和 GC 的开销，避免 Shuffle 过程中频繁的序列化以及反序列化。</li>
<li>排序过程中，提供 cache-efficient sorter，使用一个 8 bytes 的指针，把排序转化成了一个指针数组的排序，极大的优化了排序性能。</li>
<li>使用要求：<ul>
<li>Shuffle 阶段不能有 aggregate 操作，对于 aggretateByKey 之类的算子无法使用。</li>
<li>分区数不能超过 2^24</li>
<li>序列化支持 relocation，如 kyro 序列化</li>
</ul>
</li>
</ul>
</li>
<li>Sort Shuffle V2<ul>
<li>Spark1.6+，把 Sort Shuffle 和 Tungsten-Sort Based Shuffle 全部统一到 Sort Shuffle 中，如果检测到满足 Tungsten-Sort Based Shuffle 条件会自动采用 Tungsten-Sort Based Shuffle，否则采用 Sort Shuffle。</li>
<li>sort-based shuffle 的缺陷<ul>
<li>mapper 的 Task 数量过大，依旧会产生大量小文件，此时在 shuffle 传递数据的过程中 reducer 端，reduce 会需要同时大量的记录进行反序列化，导致大量的内存消耗和 GC 的巨大负担，造成系统缓慢甚至崩溃。</li>
<li>如果需要在分片内也进行排序，此时需要进行 mapper 端和 reducer 端的两次排序。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="ShuffleWriter-执行流程"><a href="#ShuffleWriter-执行流程" class="headerlink" title="ShuffleWriter 执行流程"></a>ShuffleWriter 执行流程</h3><ol>
<li><strong>数据先写入一个内存数据结构中。</strong>不同的 shuffle 算子，可能选用不同的数据结构<ul>
<li>如果是 reduceByKey 聚合类的算子，选用 Map(ExternalAppendOnlyMap) 数据结构，一边通过 Map 进行聚合，一边写入内存</li>
<li>如果是 join 类的 shuffle 算子，那么选用 Array(CompactBuffer) 数据结构，直接写入内存</li>
</ul>
</li>
<li><strong>检查是否达到内存阈值。</strong>每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会将内存数据结构中的数据溢写到磁盘，并清空内存数据结构</li>
<li><strong>数据排序。</strong>在溢写到磁盘文件之前，会先根据 key 对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的 batch 数量是 10000 条，也就是说，排序好的数据，会以每批 1 万条数据的形式分批写入磁盘文件</li>
<li><strong>数据写入缓冲区</strong>。写入磁盘文件是通过 Java 的 BufferedOutputStream 实现的。BufferedOutputStream 是 Java 的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘 IO 次数，提升性能</li>
<li><strong>重复写多个临时文件</strong>。一个 Task 将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，会产生多个临时文件</li>
<li><strong>临时文件合并</strong>。最后将所有的临时磁盘文件进行合并，这就是 merge 过程。此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中写索引文件。由于一个 Task 就只对应一个磁盘文件，也就意味着该 task 为下游 stage 的 task 准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个 Task 的数据在文件中的 start offset 与 end offset。</li>
</ol>
<h3 id="SortShuffleManager"><a href="#SortShuffleManager" class="headerlink" title="SortShuffleManager"></a>SortShuffleManager</h3><p>当前 ShuffleManager 唯一实现类。在基于排序的 shuffle 中，传入记录根据其目标分区 id 进行排序，然后写入单个映射输出文件。Reducers 获取这个文件的连续区域，以便读取映射输出中属于它们的部分。在 map 输出数据太大而无法装入内存的情况下，可以将输出的排序子集溢出到磁盘，并合并那些磁盘上的文件以生成最终的输出文件。</p>
<ul>
<li>getReader(): 返回读取 shuffle 过程的数据的 reader，当前使用的位置<ul>
<li>当前获取的位置： CoGroupedRDD、ShuffledRDD、SubtractedRDD、ShuffledRowRDD</li>
</ul>
</li>
<li>getWriter(): 根据条件判断选择哪种 ShuffleWriter，可选择 BypassMergeSortShuffleWriter, UnsafeShuffleWriter, SortShuffleWriter</li>
<li>registerShuffle(): 注册 shuffle</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">SortShuffleManager</span>(<span class="params">conf: <span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">ShuffleManager</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * A mapping from shuffle ids to the number of mappers producing output for those shuffles.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> numMapsForShuffle = <span class="keyword">new</span> <span class="type">ConcurrentHashMap</span>[<span class="type">Int</span>, <span class="type">Int</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> shuffleBlockResolver = <span class="keyword">new</span> <span class="type">IndexShuffleBlockResolver</span>(conf)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Obtains a [[ShuffleHandle]] to pass to tasks.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">registerShuffle</span></span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">      shuffleId: <span class="type">Int</span>,</span><br><span class="line">      numMaps: <span class="type">Int</span>,</span><br><span class="line">      dependency: <span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]): <span class="type">ShuffleHandle</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">SortShuffleWriter</span>.shouldBypassMergeSort(conf, dependency)) &#123;</span><br><span class="line">      <span class="comment">// 小于 bypassMerge(spark.shuffle.sort.bypassMergeThreshold) 的阈值分区数，不需要 map 端的聚合，</span></span><br><span class="line">      <span class="comment">// 然后直接写入MnumPartitions 文件，并在最后将它们连接起来。这避免了两次序列化和反序列化来合并溢出的文件，缺点是一次打开多个文件，从而分配给缓冲区更多的内存</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">        shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="type">SortShuffleManager</span>.canUseSerializedShuffle(dependency)) &#123;</span><br><span class="line">      <span class="comment">// 尝试以序列化的形式缓冲映射输出，因为这样更有效率</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">        shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 缓冲区映射以反序列化形式输出</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">BaseShuffleHandle</span>(shuffleId, numMaps, dependency)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 获取一系列 reduce 分区的读取器(startPartition 到 endPartition-1，包括在内)。通过 reduce 任务调用 executor。</span></span><br><span class="line"><span class="comment">   * 当前使用到的位置：</span></span><br><span class="line"><span class="comment">   * - CoGroupedRDD</span></span><br><span class="line"><span class="comment">   * - ShuffledRDD</span></span><br><span class="line"><span class="comment">   * - SubtractedRDD</span></span><br><span class="line"><span class="comment">   * - ShuffledRowRDD</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getReader</span></span>[<span class="type">K</span>, <span class="type">C</span>](</span><br><span class="line">      handle: <span class="type">ShuffleHandle</span>,</span><br><span class="line">      startPartition: <span class="type">Int</span>,</span><br><span class="line">      endPartition: <span class="type">Int</span>,</span><br><span class="line">      context: <span class="type">TaskContext</span>): <span class="type">ShuffleReader</span>[<span class="type">K</span>, <span class="type">C</span>] = &#123;</span><br><span class="line">    <span class="comment">// shuffle 过程的 reader, 当前 ShuffleReader 的唯一实现类</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">BlockStoreShuffleReader</span>(</span><br><span class="line">      handle.asInstanceOf[<span class="type">BaseShuffleHandle</span>[<span class="type">K</span>, _, <span class="type">C</span>]], startPartition, endPartition, context)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 获取给定分区的写入器。通过 map 任务调用 executor</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getWriter</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      handle: <span class="type">ShuffleHandle</span>, <span class="comment">// 根据传入的 ShuffleHandle 获取到</span></span><br><span class="line">      mapId: <span class="type">Int</span>,</span><br><span class="line">      context: <span class="type">TaskContext</span>): <span class="type">ShuffleWriter</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line">    numMapsForShuffle.putIfAbsent(</span><br><span class="line">      handle.shuffleId, handle.asInstanceOf[<span class="type">BaseShuffleHandle</span>[_, _, _]].numMaps)</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">SparkEnv</span>.get</span><br><span class="line">    handle <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="comment">// 序列化的 Shuffle 处理，使用 UnsafeShuffleWriter</span></span><br><span class="line">      <span class="keyword">case</span> unsafeShuffleHandle: <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>] =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">UnsafeShuffleWriter</span>(</span><br><span class="line">          env.blockManager,</span><br><span class="line">          shuffleBlockResolver.asInstanceOf[<span class="type">IndexShuffleBlockResolver</span>],</span><br><span class="line">          context.taskMemoryManager(),</span><br><span class="line">          unsafeShuffleHandle,</span><br><span class="line">          mapId,</span><br><span class="line">          context,</span><br><span class="line">          env.conf)</span><br><span class="line">      <span class="comment">// 跳过 mergeSort 的 shuffle 处理</span></span><br><span class="line">      <span class="keyword">case</span> bypassMergeSortHandle: <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>] =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleWriter</span>(</span><br><span class="line">          env.blockManager,</span><br><span class="line">          shuffleBlockResolver.asInstanceOf[<span class="type">IndexShuffleBlockResolver</span>],</span><br><span class="line">          bypassMergeSortHandle,</span><br><span class="line">          mapId,</span><br><span class="line">          context,</span><br><span class="line">          env.conf)</span><br><span class="line">      <span class="comment">// 通用的 ShuffleWriter 处理</span></span><br><span class="line">      <span class="keyword">case</span> other: <span class="type">BaseShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>, _] =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SortShuffleWriter</span>(shuffleBlockResolver, other, mapId, context)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">object</span> <span class="title">SortShuffleManager</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 在以序列化形式缓冲映射输出时 SortShuffleManager 支持的最大 shuffle 输出分区数。</span></span><br><span class="line"><span class="comment">   * 这是一种极端的防御性编程措施，因为一次 shuffle 极不可能产生超过 1600 万个输出分区。</span></span><br><span class="line"><span class="comment">   * */</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE</span> =</span><br><span class="line">    <span class="type">PackedRecordPointer</span>.<span class="type">MAXIMUM_PARTITION_ID</span> + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 用于确定 shuffle 是否可使用优化的序列化 shuffle path 是否应该退回到对反序列化对象进行操作的 original path</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">canUseSerializedShuffle</span></span>(dependency: <span class="type">ShuffleDependency</span>[_, _, _]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> shufId = dependency.shuffleId</span><br><span class="line">    <span class="keyword">val</span> numPartitions = dependency.partitioner.numPartitions</span><br><span class="line">    <span class="comment">// 需要依赖对应的序列化器支持 relocation</span></span><br><span class="line">    <span class="keyword">if</span> (!dependency.serializer.supportsRelocationOfSerializedObjects) &#123;</span><br><span class="line">      log.debug(<span class="string">s&quot;Can&#x27;t use serialized shuffle for shuffle <span class="subst">$shufId</span> because the serializer, &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;<span class="subst">$&#123;dependency.serializer.getClass.getName&#125;</span>, does not support object relocation&quot;</span>)</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (dependency.mapSideCombine) &#123;</span><br><span class="line">      <span class="comment">// 不可为 MapSideCombine</span></span><br><span class="line">      log.debug(<span class="string">s&quot;Can&#x27;t use serialized shuffle for shuffle <span class="subst">$shufId</span> because we need to do &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;map-side aggregation&quot;</span>)</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (numPartitions &gt; <span class="type">MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE</span>) &#123;    <span class="comment">// 2^24</span></span><br><span class="line">      <span class="comment">// 分区数小于 2^24</span></span><br><span class="line">      log.debug(<span class="string">s&quot;Can&#x27;t use serialized shuffle for shuffle <span class="subst">$shufId</span> because it has more than &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;<span class="subst">$MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE</span> partitions&quot;</span>)</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      log.debug(<span class="string">s&quot;Can use serialized shuffle for shuffle <span class="subst">$shufId</span>&quot;</span>)</span><br><span class="line">      <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="与-Hadoop-Shuffle-的比较"><a href="#与-Hadoop-Shuffle-的比较" class="headerlink" title="与 Hadoop Shuffle 的比较"></a>与 Hadoop Shuffle 的比较</h3><p>二者从功能上看是相似的；从 High Level来看，没有本质区别。</p>
<ul>
<li><strong>何时进行 fetch Map 端的数据</strong>： Hadoop 中有一个 Map 完成，Reduce 便可以去 fetch 数据了，MR Shuffle 不必等到所有 Map 任务完成；而 Spark shuffle 必须等到父 stage 完成，也就是父 stage 的 map 操作全部完成才能去 fetch 数据。这是因为 spark 必须等到父 stage 执行完，才能执行子 stage，主要是为了迎合 stage 规则</li>
<li><strong>何时执行 Reduce 端的聚合：</strong> Hadoop 的 Reduce 要等到 fetch 完全部数据，才将数据传入 reduce 函数进行聚合，Spark 是一边 fetch 一边聚合。</li>
<li><strong>分区有序的要求：</strong> Hadoop 的 Shuffle 是 sort-base 的，那么不管是 Map 的输出，还是 Reduce 的输出，都是 partition 内有序的，而 spark 不要求 partition 内有限</li>
</ul>
<h3 id="Shuffle-Writer"><a href="#Shuffle-Writer" class="headerlink" title="Shuffle Writer"></a>Shuffle Writer</h3><p><strong>BypassMergeSortShuffleWriter</strong></p>
<blockquote>
<p>与 Hash Shuffle 的实现基本相同，区别在于 map task 输出汇总一个文件，同时还会产生一个 index file</p>
</blockquote>
<p>特点：</p>
<ul>
<li>类似于Hash Shuffle，多了文件的合并</li>
<li>对于大量 reduce 分区的 shuffle，是低效的，因为同时为所有分区打开单独的序列化器和文件流</li>
</ul>
<p>Writer 流程：</p>
<ul>
<li>每个 Map Task 为每个下游 reduce task 创建一个临时磁盘文件，并将数据按 key 进行 hash 然后根据 hash 值写入内存缓冲，缓冲写满之后再溢写到磁盘文件；</li>
<li>最后将所有临时磁盘文件都合并成一个磁盘文件，并创建索引文件；</li>
<li>在 shuffle 过程中会创建很多磁盘文件，最后多了一个磁盘文件合并的过程。Shuffle Read 的性能会更好；</li>
</ul>
<p>Bypass 方式与普通的 Sort Shuffle 方式的不同点：</p>
<ul>
<li>磁盘写机制不同</li>
<li>根据 key 求 hash，减少了数据排序操作，提高了性能</li>
</ul>
<p>选择条件：</p>
<ul>
<li>分区数 &lt;=200(spark.shuffle.sort.bypassMergeThreshold)</li>
<li>非聚合操作</li>
<li>没有指定排序</li>
<li>不是 mapSideCombine</li>
</ul>
<p><strong>UnsafeShuffleWriter</strong></p>
<p>序列化的排序方式： 在序列化排序模式中，传入的记录一旦传递给 shuffle 写入器就会被序列化，并在排序期间以序列化的形式进行缓冲。</p>
<p>序列化排序方式的优化：</p>
<ul>
<li>它的排序操作是序列化的二进制数据，而不是 Java 对象，这减少了内存消耗和 GC 开销。此优化要求记录序列化器具有某些属性，以允许在不反序列化的情况下重新排序序列化的记录。</li>
<li>它使用专用的高效缓存排序器([[ShuffleExternalSorter]])对压缩记录指针和分区id数组进行排序。通过在排序数组中每条记录只使用 8 个字节的空间，这可以将更多的数组放入缓存中</li>
<li>spill 合并过程对属于同一分区的序列化记录块进行操作，合并过程中不需要反序列化记录。</li>
<li>当 spill 压缩编解码器支持连接压缩数据时，spill merge 只是将序列化的 spill 分区和压缩的 spill 分区连接起来，以产生最终的输出分区。这允许使用高效的数据复制方法，如 NIO 的 transferTo，并避免了在合并期间分配解压缩缓冲区或复制缓冲区的需要。</li>
</ul>
<p>选择条件：</p>
<ul>
<li>shuffle 序列化器支持 object relocation, 目前 KryoSerializer 或 SparkSQL 自定义的一些序列化方式支持</li>
<li>分区数 &lt; 2^24(16777216)</li>
<li>shuffle 依赖项不指定聚合或输出顺序，即 mapSideCombine 为 false。</li>
<li>设置堆外内存大小</li>
</ul>
<p><strong>SortShuffleWriter</strong></p>
<p>对于没有选择 BypassMergeSortShuffleWriter、UnsafeShuffleWriter 的，默认选择 SortShuffleWriter。</p>
<p>执行流程：</p>
<ul>
<li>数据先写入内存数据结构。聚合类操作写入 Map，非聚合类算子写入 Array</li>
<li>检查是否达到内存阈值。非实时检查，不定时采样，不准确</li>
<li>数据排序</li>
<li>数据写入缓冲区(32k)</li>
<li>重复写多个临时文件</li>
<li>最后临时文件合并为数据文件</li>
<li>写索引文件</li>
<li>将文件位置、计算状态等封装到 MapStatus 中，汇报给 Driver</li>
</ul>
<h3 id="MapOutputTracker"><a href="#MapOutputTracker" class="headerlink" title="MapOutputTracker"></a>MapOutputTracker</h3><blockquote>
<p>Shuffle 过程中的中间数据的元信息，由 MapOutputTracker 负责管理。</p>
</blockquote>
<p>Shuffle Writer 会将中间数据保存到 Block 里面，然后将数据的位置发送给 MapOutputTracker。Shuffle Reader 通过向 MapOutputTracker 获取中间数据的位置之后，才能读取到数据。</p>
<ul>
<li>MapOutputTracker： 跟踪 map 阶段输出的位置，在 executor 和 driver 端都存在。<ul>
<li>trackerEndpoint： 一个 RpcEndpointRef</li>
</ul>
</li>
<li>MapOutputTrackerMaster： 存在于 driver 端，DAGScheduler 使用该类注册 map 输出状态和查找统计信息执行位置感知减少任务调度。<ul>
<li>负责管理所有 shuffleMapTask 的输出数据，每个 shuffleMapTask 执行完后会把执行结果（MapStatus）注册到  MapOutputTrackerMaster</li>
<li>MapOutputTrackerMaster 会处理 executor 发送的 GetMapOutputStatuses 请求，并返回 serializedMapStatus 给 executor 端</li>
</ul>
</li>
<li>MapOutputTrackerMasterEndpoint：存在于 driver 端</li>
<li>MapOutputTrackerWorker： 存在于 executor 端<ul>
<li>负责为 reduce 任务提供 shuffleMapTask 的输出数据信息（MapStatus）</li>
<li>如果 MapOutputTrackerWorker 在本地没有找到请求的 shuffle 的 mapStatus，则会向 MapOutputTrackerMasterEndpoint 发送 GetMapOutputStatuses 请求获取对应的 mapStatus</li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">MapOutputTrackerMasterEndpoint</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    override val rpcEnv: <span class="type">RpcEnv</span>, tracker: <span class="type">MapOutputTrackerMaster</span>, conf: <span class="type">SparkConf</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">RpcEndpoint</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 处理 GetMapOutputStatuses、StopMapOutputTracker 的偏函数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receiveAndReply</span></span>(context: <span class="type">RpcCallContext</span>): <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">GetMapOutputStatuses</span>(shuffleId: <span class="type">Int</span>) =&gt;</span><br><span class="line">      <span class="keyword">val</span> hostPort = context.senderAddress.hostPort</span><br><span class="line">      logInfo(<span class="string">&quot;Asked to send map output locations for shuffle &quot;</span> + shuffleId + <span class="string">&quot; to &quot;</span> + hostPort)</span><br><span class="line">      <span class="comment">// MapOutputTrackerMaster 发送 GetMapOutputMessage</span></span><br><span class="line">      <span class="keyword">val</span> mapOutputStatuses = tracker.post(<span class="keyword">new</span> <span class="type">GetMapOutputMessage</span>(shuffleId, context))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">StopMapOutputTracker</span> =&gt;</span><br><span class="line">      logInfo(<span class="string">&quot;MapOutputTrackerMasterEndpoint stopped!&quot;</span>)</span><br><span class="line">      context.reply(<span class="literal">true</span>)</span><br><span class="line">      stop()</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">MapOutputTrackerMaster</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    conf: <span class="type">SparkConf</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    broadcastManager: <span class="type">BroadcastManager</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    isLocal: <span class="type">Boolean</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">MapOutputTracker</span>(<span class="params">conf</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> minSizeForBroadcast =</span><br><span class="line">    conf.getSizeAsBytes(<span class="string">&quot;spark.shuffle.mapOutput.minSizeForBroadcast&quot;</span>, <span class="string">&quot;512k&quot;</span>).toInt</span><br><span class="line">  <span class="comment">// spark.rpc.message.maxSize 默认为 128M</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> maxRpcMessageSize = <span class="type">RpcUtils</span>.maxMessageSizeBytes(conf)</span><br><span class="line">  <span class="comment">// requests for map output statuses</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> mapOutputRequests = <span class="keyword">new</span> <span class="type">LinkedBlockingQueue</span>[<span class="type">GetMapOutputMessage</span>]</span><br><span class="line">  <span class="comment">// 用于处理映射输出状态请求的线程池。</span></span><br><span class="line">  <span class="comment">// 这是一个单独的线程池，以确保我们不会阻塞普通的调度程序线程。</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> threadpool: <span class="type">ThreadPoolExecutor</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> numThreads = conf.getInt(<span class="string">&quot;spark.shuffle.mapOutput.dispatcher.numThreads&quot;</span>, <span class="number">8</span>)</span><br><span class="line">    <span class="keyword">val</span> pool = <span class="type">ThreadUtils</span>.newDaemonFixedThreadPool(numThreads, <span class="string">&quot;map-output-dispatcher&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until numThreads) &#123;</span><br><span class="line">      pool.execute(<span class="keyword">new</span> <span class="type">MessageLoop</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    pool</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 发送消息</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">post</span></span>(message: <span class="type">GetMapOutputMessage</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 放到 LinkedBlockingQueue 阻塞队列中</span></span><br><span class="line">    mapOutputRequests.offer(message)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 用于发送消息的 message loop， threadpool 中执行的任务</span></span><br><span class="line">  <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">MessageLoop</span> <span class="keyword">extends</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 消费阻塞队列中的 GetMapOutputMessage 消息</span></span><br><span class="line">            <span class="keyword">val</span> data = mapOutputRequests.take()</span><br><span class="line">             <span class="keyword">if</span> (data == <span class="type">PoisonPill</span>) &#123;</span><br><span class="line">              <span class="comment">// Put PoisonPill back so that other MessageLoops can see it.</span></span><br><span class="line">              mapOutputRequests.offer(<span class="type">PoisonPill</span>)</span><br><span class="line">              <span class="keyword">return</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">val</span> context = data.context</span><br><span class="line">            <span class="keyword">val</span> shuffleId = data.shuffleId</span><br><span class="line">            <span class="keyword">val</span> hostPort = context.senderAddress.hostPort</span><br><span class="line">            logDebug(<span class="string">&quot;Handling request to send map output locations for shuffle &quot;</span> + shuffleId +</span><br><span class="line">              <span class="string">&quot; to &quot;</span> + hostPort)</span><br><span class="line">            <span class="keyword">val</span> shuffleStatus = shuffleStatuses.get(shuffleId).head</span><br><span class="line">            context.reply(</span><br><span class="line">              shuffleStatus.serializedMapStatus(broadcastManager, isLocal, minSizeForBroadcast))</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(e.getMessage, e)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> =&gt; <span class="comment">// exit</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Shuffle-Reader"><a href="#Shuffle-Reader" class="headerlink" title="Shuffle Reader"></a>Shuffle Reader</h3><ul>
<li>Map Task 执行完毕后会将文件位置、计算状态等信息封装到 MapStatus 中，通过  MapOutPutTrackerWorker 对象将其发送给 Driver 进程的 MapOutPutTrackerMaster</li>
<li>Reduce Task 开始执行之前会先让本进程中的 MapOutputTrackerWorker 向 Driver 进程中的 MapOutputTrackerMaster 发动请求，获取磁盘文件位置等信息</li>
<li>当所有的 Map Task 执行完毕后，Driver 进程中的 MapOutputTrackerMaster 获得了所有的 Shuffle 文件的信息。此时 MapOutPutTrackerMaster 会告诉 MapOutPutTrackerWorker 磁盘小文件的位置信息</li>
<li>完成之前的操作之后，由 BlockTransforService 去 Executor 所在的节点拉数据，默认会启动五个子线程。每次拉取的数据量不能超过 48M</li>
<li>spark.reducer.maxSizeInFlight: shuffle reader 时候，一次 fetch 不能过多，不能超过的数据量，默认为 48M, 空间是由这 5 个 fetch 线程共享的</li>
<li>BlockStoreShuffleReader: 当前 ShuffleReader 的唯一实现类<ul>
<li>read(): 读取 reduce 任务的组合键值</li>
</ul>
</li>
<li>ShuffleBlockFetcherIterator: 获取多个块的迭代器。对于本地块，它从本地块管理器获取。对于远程块，它使用提供的 BlockTransferService 获取。<ul>
<li>创建 (BlockID, InputStream) 元组，对远程获取进行了限制，不会超过 maxBytesInFlight，避免使用太多的内存</li>
</ul>
</li>
</ul>
<h3 id="Shuffle-的优化"><a href="#Shuffle-的优化" class="headerlink" title="Shuffle 的优化"></a>Shuffle 的优化</h3><p><strong>开发过程中的优化：</strong></p>
<ul>
<li>减少 Shuffle 过程中的数据量： 使用高性能算子，如使用 filter + coalesce 过滤不需要的值并减少分区数，使用 reduceByKey 替代 groupByKey 有 mapSideCombine 减少聚合的数据量，</li>
<li>避免 Shuffle： 更改代码避免 shuffle，使用 map 端的 join 避免， 如使用 colasce 替代 repartititon, repartitionAndSortxxx 将会产生两个 shuffle 的合并为一个</li>
</ul>
<p><strong>参数优化：</strong></p>
<ul>
<li>调节 map 端缓冲区大小<ul>
<li>spark.shuffle.file.buffer 默认值为32K，shuffle write 阶段 buffer 缓冲大小。将数据写到磁盘文件之前，会先写入 buffer 缓冲区，缓冲写满后才溢写到磁盘</li>
</ul>
</li>
<li>调节 reduce 端拉取数据缓冲区大小<ul>
<li>spark.reducer.maxSizeInFlight 默认值为48M。设置shuffle read阶段buffer缓冲区大小，这个buffer缓冲决定了每次能够拉取多少数据</li>
<li>在内存资源充足的情况下，可适当增加参数的大小（如96m），减少拉取数据的次数及网络传输次数，进而提升性能</li>
<li>合理设置参数，性能会有 1%~5% 的提升</li>
</ul>
</li>
<li>调节 reduce 端拉取数据重试次数及等待间隔，Shuffle read 阶段拉取数据时，如果因为网络异常导致拉取失败，会自动进行重试。<ul>
<li>Shuffle read阶段拉取数据时，如果因为网络异常导致拉取失败，会自动进行重试</li>
<li>spark.shuffle.io.maxRetries，默认值3。最大重试次数</li>
<li>spark.shuffle.io.retryWait，默认值5s。每次重试拉取数据的等待间隔</li>
<li>一般调高最大重试次数，不调整时间间隔</li>
</ul>
</li>
<li>调节 Sort Shuffle 排序操作阈值<ul>
<li>如果shuffle reduce task的数量小于阈值，则shuffle write过程中不会进行排序操作，而是直接按未经优化的Hash Shuffle方式写数据，最后将每个task产生的</li>
<li>所有临时磁盘文件都合并成一个文件，并创建单独的索引文</li>
<li>spark.shuffle.sort.bypassMergeThreshold，默认值为200 当使用SortShuffleManager时，如果的确不需要排序操作，建议将这个参数调大</li>
</ul>
</li>
<li>调节 Shuffle 内存大小，分配多一些的比例给执行内存用于 Shuffle</li>
</ul>
<h3 id="配置参数"><a href="#配置参数" class="headerlink" title="配置参数"></a>配置参数</h3><p>Shuffle 中的重要参数：</p>
<ul>
<li><p><code>spark.local.dir</code>: Shuffle 缓存目录</p>
</li>
<li><p><code>spark.shuffle.file.buffer</code>： shuffle write 阶段 buffer 缓冲大小，将数据写到磁盘文件之前，会先写入 buffer 缓冲中，待缓冲写满之后，才会溢写到磁盘。默认值为 32K。 </p>
</li>
<li><p><code>spark.reducer.maxSizeInFlight</code>： shuffle read 阶段 buffer 缓冲区大小。默认值为 48M。</p>
</li>
<li><p><code>spark.shuffle.io.maxRetries</code>： Shuffle read 阶段拉取数据失败时的最大重试次数。默认值 3。</p>
</li>
<li><p><code>spark.shuffle.io.retryWait</code>： Shuffle read 阶段拉取数据失败重试时的等待时间。默认值 5s。</p>
</li>
<li><p><code>spark.shuffle.sort.bypassMergeThreshold</code>： 使用 bypassMergeSortShuffleWriter 机制，RDD 分区数的限制阈值。默认值为 200。</p>
</li>
<li><p><code>spark.memory.fraction &amp; spark.memory.storageFraction</code>： 调整 Shuffle 相关内存所占的比例</p>
</li>
<li><p><code>spark.memory.fraction</code>： 缺省值 0.6。存储内存和执行内存占（heap 内存 - 300M）的百分比</p>
</li>
<li><p><code>spark.memory.storageFraction</code>： 缺省值 0.5 存储内存与 （存储内存与执行内存之和）的百分比</p>
</li>
<li><p><code>spark.shuffle.manager</code>： 通过反射方式生成的 SortShuffleManager 的实例。默认为 SortShuffleManager。</p>
</li>
<li><p>Spark 1.5 以后，有三个可选项：hash、sort 和 tungsten-sort。</p>
</li>
<li><p><code>spark.shuffle.consolidateFiles</code>：</p>
</li>
<li><p><code>spark.shuffle.mapOutput.minSizeForBroadcast</code>：默认值 512K</p>
</li>
<li><p><code>spark.shuffle.mapOutput.dispatcher.numThreads</code>: 默认值为 8，map 端输出派发线程池中的线程数</p>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Janhen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2021/05/05/Spark-Shuffle%E8%BF%87%E7%A8%8B/">http://example.com/2021/05/05/Spark-Shuffle%E8%BF%87%E7%A8%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/05/07/Spark-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Spark-数据倾斜</div></div></a></div><div class="next-post pull-right"><a href="/2021/05/05/Spark-SQL%E4%B8%ADJoin%E7%9A%84%E5%AE%9E%E7%8E%B0/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Spark-Join的原理</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/05/05/Spark-SQL中Join的实现/" title="Spark-Join的原理"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-05</div><div class="title">Spark-Join的原理</div></div></a></div><div><a href="/2021/04/09/Spark-内存管理与持久化/" title="Spark-内存管理与持久化"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-09</div><div class="title">Spark-内存管理与持久化</div></div></a></div><div><a href="/2021/05/07/Spark-数据倾斜/" title="Spark-数据倾斜"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-07</div><div class="title">Spark-数据倾斜</div></div></a></div><div><a href="/2021/04/16/Spark实现KNN近邻算法/" title="Spark实现KNN近邻算法"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-16</div><div class="title">Spark实现KNN近邻算法</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="http://img.janhen.com/20210331220602f7HmbN.pnghttp://img.janhen.com/20210331220602f7HmbN.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Janhen</div><div class="author-info__description">大数据、后端、运维分享</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">27</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Janhen"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Shuffle"><span class="toc-text">Shuffle</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ShuffleWriter-%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-text">ShuffleWriter 执行流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SortShuffleManager"><span class="toc-text">SortShuffleManager</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E-Hadoop-Shuffle-%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-text">与 Hadoop Shuffle 的比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Shuffle-Writer"><span class="toc-text">Shuffle Writer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MapOutputTracker"><span class="toc-text">MapOutputTracker</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Shuffle-Reader"><span class="toc-text">Shuffle Reader</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Shuffle-%E7%9A%84%E4%BC%98%E5%8C%96"><span class="toc-text">Shuffle 的优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0"><span class="toc-text">配置参数</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/05/07/Spark-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/" title="Spark-数据倾斜"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark-数据倾斜"/></a><div class="content"><a class="title" href="/2021/05/07/Spark-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/" title="Spark-数据倾斜">Spark-数据倾斜</a><time datetime="2021-05-07T07:45:41.000Z" title="Created 2021-05-07 15:45:41">2021-05-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/05/Spark-Shuffle%E8%BF%87%E7%A8%8B/" title="Spark-Shuffle 过程和原理"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark-Shuffle 过程和原理"/></a><div class="content"><a class="title" href="/2021/05/05/Spark-Shuffle%E8%BF%87%E7%A8%8B/" title="Spark-Shuffle 过程和原理">Spark-Shuffle 过程和原理</a><time datetime="2021-05-05T08:55:56.000Z" title="Created 2021-05-05 16:55:56">2021-05-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/05/Spark-SQL%E4%B8%ADJoin%E7%9A%84%E5%AE%9E%E7%8E%B0/" title="Spark-Join的原理"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark-Join的原理"/></a><div class="content"><a class="title" href="/2021/05/05/Spark-SQL%E4%B8%ADJoin%E7%9A%84%E5%AE%9E%E7%8E%B0/" title="Spark-Join的原理">Spark-Join的原理</a><time datetime="2021-05-05T08:29:59.000Z" title="Created 2021-05-05 16:29:59">2021-05-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/04/19/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1/" title="数据结构设计"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构设计"/></a><div class="content"><a class="title" href="/2021/04/19/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1/" title="数据结构设计">数据结构设计</a><time datetime="2021-04-19T14:53:47.000Z" title="Created 2021-04-19 22:53:47">2021-04-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/04/16/Spark%E5%AE%9E%E7%8E%B0KNN%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/" title="Spark实现KNN近邻算法"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark实现KNN近邻算法"/></a><div class="content"><a class="title" href="/2021/04/16/Spark%E5%AE%9E%E7%8E%B0KNN%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/" title="Spark实现KNN近邻算法">Spark实现KNN近邻算法</a><time datetime="2021-04-16T02:32:24.000Z" title="Created 2021-04-16 10:32:24">2021-04-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Janhen</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>