<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>SparkCore-RDD-编程 | Janhen</title><meta name="keywords" content="Spark"><meta name="author" content="Janhen"><meta name="copyright" content="Janhen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="RDD 概述 分布式弹性数据集，是Spark 中的核心概念，是一个有容错机制的特殊数据集，可分布在集群中的节点上，以函数式操作集合的方式进行各种操作，所有算子都基于 RDD 执行。  RDD 执行的过程中会形成 DAG 图，然后形成 Lineage 保证容错性。 物理角度看，RDD 存储的是 block 和 node 之间的映射。 不支持增量迭代计算，Flink 支持。 基本组成RDD 的组成：">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkCore-RDD-编程">
<meta property="og:url" content="http://example.com/2021/04/09/SparkCore-RDD-%E7%BC%96%E7%A8%8B/index.html">
<meta property="og:site_name" content="Janhen">
<meta property="og:description" content="RDD 概述 分布式弹性数据集，是Spark 中的核心概念，是一个有容错机制的特殊数据集，可分布在集群中的节点上，以函数式操作集合的方式进行各种操作，所有算子都基于 RDD 执行。  RDD 执行的过程中会形成 DAG 图，然后形成 Lineage 保证容错性。 物理角度看，RDD 存储的是 block 和 node 之间的映射。 不支持增量迭代计算，Flink 支持。 基本组成RDD 的组成：">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2021-04-09T08:50:21.000Z">
<meta property="article:modified_time" content="2021-04-21T09:12:02.281Z">
<meta property="article:author" content="Janhen">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2021/04/09/SparkCore-RDD-%E7%BC%96%E7%A8%8B/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-04-21 17:12:02'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Janhen" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="http://img.janhen.com/20210331220602f7HmbN.pnghttp://img.janhen.com/20210331220602f7HmbN.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">47</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">21</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div></div></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Janhen</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">SparkCore-RDD-编程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-04-09T08:50:21.000Z" title="Created 2021-04-09 16:50:21">2021-04-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-04-21T09:12:02.281Z" title="Updated 2021-04-21 17:12:02">2021-04-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="SparkCore-RDD-编程"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="RDD-概述"><a href="#RDD-概述" class="headerlink" title="RDD 概述"></a>RDD 概述</h2><blockquote>
<p>分布式弹性数据集，<strong>是Spark 中的核心概念，是一个有容错机制的特殊数据集</strong>，可分布在集群中的节点上，以函数式操作集合的方式进行各种操作，所有算子都基于 RDD 执行。</p>
</blockquote>
<p>RDD 执行的过程中会形成 DAG 图，然后形成 Lineage 保证容错性。</p>
<p>物理角度看，RDD 存储的是 block 和 node 之间的映射。</p>
<p>不支持增量迭代计算，Flink 支持。</p>
<h3 id="基本组成"><a href="#基本组成" class="headerlink" title="基本组成"></a>基本组成</h3><p>RDD 的组成：</p>
<ul>
<li>一个分区列表，每个分区是 RDD 中的部分数据</li>
<li>一个名为 <code>compute</code> 的计算函数，用于计算 RDD 各分区的值</li>
<li>一个依赖列表，存储依赖的其他 RDD，记录数据的血缘关系，用于数据的恢复</li>
<li>可选的分区器，用于键值类型的 RDD, 如某个 RDD 按照散列分区</li>
<li>可选的计算各分区时的优先位置列表，如从 HDFS 中的文件生成 RDD 时，RDD 分区的位置优先选择数据所在的节点，避免数据移动带来的开销</li>
</ul>
<p>RDD 的特点：</p>
<ul>
<li>弹性：默认情况下存储在内存中，内存不足时，可存储在磁盘上</li>
<li>分区： 逻辑上分区，每个分区数据抽象存在的</li>
<li>依赖： RDD 算子进行转换，转换得到的 RDD 保存了从其他 RDDs 衍生所需要的新，RDDs 之间维护着这种血缘关系(lineage)。</li>
<li>只读： 若需要改变 RDD 中的数据，需要在现有的 RDD 上创建新的 RDD, 转换 RDD 可通过丰富的算子进行操作</li>
<li>缓存： 可控制缓存级别进行数据的缓存，将在应用程序中使用的同一个 RDD 缓存起来，在后续使用该 RDD 做其他操作时，直接从缓存中取而不需要根据血缘关系计算</li>
<li>checkpoint： 可将数据持久化存储，截断之前的血缘关系</li>
</ul>
<p>RDD 的数据结构源码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">RDD</span>[<span class="type">T</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    @transient private var _sc: <span class="type">SparkContext</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    @transient private var deps: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]]</span></span></span><br><span class="line"><span class="class"><span class="params">  </span>) <span class="keyword">extends</span> <span class="title">Serializable</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">conf</span> </span>= sc.conf</span><br><span class="line">  <span class="comment">// 分区列表</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]</span><br><span class="line">  <span class="comment">// 依赖列表</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps</span><br><span class="line">  <span class="comment">// 分区器</span></span><br><span class="line">  <span class="meta">@transient</span> <span class="keyword">val</span> partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span></span><br><span class="line">  <span class="comment">// 计算各分区时的优先位置列表</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span></span><br><span class="line">  <span class="comment">// 计算函数</span></span><br><span class="line">  <span class="meta">@DeveloperApi</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>RDD 的隐式转换源码</p>
<ul>
<li>PairRDDFunctions： 对于 RDD[K, V] 时，可自动转换，使用键值操作</li>
<li>AsyncRDDActions</li>
<li>OrderedRDDFunctions</li>
<li>SequenceFileRDDFunctions</li>
<li>DoubleRDDFunctions</li>
<li>…</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDD</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> <span class="type">CHECKPOINT_ALL_MARKED_ANCESTORS</span> =</span><br><span class="line">    <span class="string">&quot;spark.checkpoint.checkpointAllMarkedAncestors&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">rddToPairRDDFunctions</span></span>[<span class="type">K</span>, <span class="type">V</span>](rdd: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)])</span><br><span class="line">    (<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>], vt: <span class="type">ClassTag</span>[<span class="type">V</span>], ord: <span class="type">Ordering</span>[<span class="type">K</span>] = <span class="literal">null</span>): <span class="type">PairRDDFunctions</span>[<span class="type">K</span>, <span class="type">V</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">rddToAsyncRDDActions</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">AsyncRDDActions</span>[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">rddToSequenceFileRDDFunctions</span></span>[<span class="type">K</span>, <span class="type">V</span>](rdd: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)])</span><br><span class="line">      (<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>], vt: <span class="type">ClassTag</span>[<span class="type">V</span>],</span><br><span class="line">                keyWritableFactory: <span class="type">WritableFactory</span>[<span class="type">K</span>],</span><br><span class="line">                valueWritableFactory: <span class="type">WritableFactory</span>[<span class="type">V</span>])</span><br><span class="line">    : <span class="type">SequenceFileRDDFunctions</span>[<span class="type">K</span>, <span class="type">V</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">rddToOrderedRDDFunctions</span></span>[<span class="type">K</span> : <span class="type">Ordering</span> : <span class="type">ClassTag</span>, <span class="type">V</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)])</span><br><span class="line">    : <span class="type">OrderedRDDFunctions</span>[<span class="type">K</span>, <span class="type">V</span>, (<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">doubleRDDToDoubleRDDFunctions</span></span>(rdd: <span class="type">RDD</span>[<span class="type">Double</span>]): <span class="type">DoubleRDDFunctions</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">numericRDDToDoubleRDDFunctions</span></span>[<span class="type">T</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>])(<span class="keyword">implicit</span> num: <span class="type">Numeric</span>[<span class="type">T</span>])</span><br><span class="line">    : <span class="type">DoubleRDDFunctions</span></span><br></pre></td></tr></table></figure>

<h3 id="RDD-创建方式"><a href="#RDD-创建方式" class="headerlink" title="RDD 创建方式"></a>RDD 创建方式</h3><ul>
<li>从 Hadoop 文件系统创建，或与 Hadoop 兼容的持久化存储系统，如 Hive、HDFS、HBase 中创建</li>
<li>通过本地文件系统创建 RDD</li>
<li>基于数据流，如 Socket 创建 RDD</li>
<li>通过集合创建 RDD</li>
<li>基于数据库创建 RDD</li>
<li>基于 NoSql 创建 RDD, 如 HBase</li>
<li>从父 RDD 转换得到新的 RDD</li>
</ul>
<h3 id="RDD-序列化"><a href="#RDD-序列化" class="headerlink" title="RDD 序列化"></a>RDD 序列化</h3><p>对于普通类不具备序列化能力时，使用会直接报错， Task not serializable。</p>
<p>一般使用 case object 进行类型的定义。</p>
<p>可在运行的时候指定 RDD 的序列化方式，对于 Dataset, DataFrame 则默认对序列化进行优化，优先选择使用 Kyro 的序列化方式。</p>
<h2 id="RDD-算子"><a href="#RDD-算子" class="headerlink" title="RDD 算子"></a>RDD 算子</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> distData = sc.parallelize(data)</span><br><span class="line">sc.parallelize(data, <span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> distFile = sc.textFile(<span class="string">&quot;data.txt&quot;</span>)</span><br><span class="line">distFile.map(s =&gt; s.length).reduce((a, b) =&gt; a + b)</span><br><span class="line"><span class="comment">// 支持在目录，压缩文件和通配符上运行</span></span><br><span class="line"><span class="comment">// textFile(&quot;/my/directory/*.gz&quot;)</span></span><br><span class="line"><span class="comment">// textFile(&quot;/my/directory&quot;)</span></span><br><span class="line"><span class="comment">// textFile(&quot;/my/directory/*.txt&quot;)</span></span><br></pre></td></tr></table></figure>

<p>传递函数给 Spark</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyFunctions</span> </span>&#123;  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">func1</span></span>(s: <span class="type">String</span>): <span class="type">String</span> = &#123; ... &#125;</span><br><span class="line">&#125;</span><br><span class="line">myRdd.map(<span class="type">MyFunctions</span>.func1)</span><br></pre></td></tr></table></figure>

<h3 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h3><blockquote>
<p>转换操作，不会触发执行</p>
</blockquote>
<ul>
<li><p><code>filter</code>: 过滤，尽早过滤，过滤掉空值/默认值/异常值，减少网络传输</p>
</li>
<li><p><code>map</code>: 对RDD每个元素转换，文件中的每一行数据返回一个数组对象。</p>
</li>
<li><p><code>flatMap</code>： 对 RDD 每个元素转换，然后进行扁平化</p>
</li>
<li><p><code>mapPartitions</code>: 针对初始化链接之类的操作，使用 mapPartitions，如数据库链接，减少链接创建的次数，提高性能</p>
</li>
<li><p><code>mapPartitionsWithIndex</code>：</p>
</li>
<li><p><code>glom()</code>：将每一个分区形成一个数组，形成新的RDD类型 RDD[Array[T]]</p>
</li>
<li><pre><code>sample(withReplacement, fraction, seed)
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  ：采样算子。以指定的随机种子(seed)随机抽样出数量为 fraction 的数据</span><br><span class="line"></span><br><span class="line">  - withReplacement 表示是抽出的数据是否放回</span><br><span class="line">  - true 为有放回的抽样，false为无放回的抽样</span><br><span class="line"></span><br><span class="line">- &#96;coalesce(numPartitions)&#96;：缩减分区数，无shuffle, 优先于 repartition 使用</span><br><span class="line"></span><br><span class="line">- &#96;repartition(numPartitions)&#96;：增加或减少分区数，有 shuffle, 可以控制输出数据产生的文件个数(saveAsTextFile)</span><br><span class="line"></span><br><span class="line">- &#96;zip(otherRDD)&#96;：将两个 RDD 组合成 key-value 形式的 RDD，默认两个 RDD 的 partition 数量以及元素数量都相同</span><br><span class="line"></span><br><span class="line">- &#96;cartesian(otherRDD)&#96;：求笛卡尔积，为窄依赖。得到的 RDD 分区数为两个 RDD 分区数之积</span><br><span class="line"></span><br><span class="line">- &#96;union&#96;： 是窄依赖。得到的 RDD 分区数为：两个 RDD 分区数之和</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;scala</span><br><span class="line">def mapPartitions[U: ClassTag](      f: Iterator[T] &#x3D;&gt; Iterator[U],      preservesPartitioning: Boolean &#x3D; false): RDD[U]</span><br><span class="line">private[spark] def mapPartitionsInternal[U: ClassTag](      f: Iterator[T] &#x3D;&gt; Iterator[U],      preservesPartitioning: Boolean &#x3D; false): RDD[U]</span><br><span class="line">def sortBy[K](      f: (T) &#x3D;&gt; K,      ascending: Boolean &#x3D; true,      numPartitions: Int &#x3D; this.partitions.length)      (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]</span><br><span class="line">def coalesce(numPartitions: Int, shuffle: Boolean &#x3D; false,               </span><br><span class="line">partitionCoalescer: Option[PartitionCoalescer] &#x3D; Option.empty)              (implicit ord: Ordering[T] &#x3D; null)      : RDD[T]</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<h3 id="宽-窄依赖"><a href="#宽-窄依赖" class="headerlink" title="宽/窄依赖"></a>宽/窄依赖</h3><p>宽依赖的算子（shuffle）：groupBy、distinct、repartition、sortBy、intersection、subtract</p>
<p>宽依赖的算子：</p>
<ul>
<li>join: join, leftOutJoin, rightOutJoin</li>
<li>xxByKey: groupByKey, reduceByKey, foldByKey, arrxxByKey</li>
<li>repartition、distinct、intersection、subtract</li>
</ul>
<h3 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h3><blockquote>
<p>会触发执行的算子</p>
</blockquote>
<p>Action 算子的整体执行流程</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">collect() =&gt; sc.runJob() =&gt; ... =&gt; dagScheduler.runJob() =&gt; 触发了<span class="type">Job</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>collect()</code> / <code>collectAsMap()</code>： 收集数据</li>
<li><code>foreach(func)</code>：逐个进行遍历</li>
<li><code>foreachPartition(func)</code>：按照分区进行逐个遍历，可使用该算子进行自定义输出，按照分区进行批量操作节省资源，如将结果保存到 MySQL 中</li>
<li><code>saveAsTextFile(path)</code> ： 数据存储</li>
<li><code>stats</code> / <code>count</code> / <code>mean</code> / <code>stdev</code> / <code>max</code> / <code>min</code>: 简单统计的算子</li>
<li><code>reduce(func) / fold(func) / aggregate(func)</code>: 聚合</li>
<li><code>take(n)</code>：</li>
<li><code>first()</code>：</li>
<li><code>top(n)</code>: 从大到小的降序</li>
<li><code>takeSample(withReplacement, num, [seed])</code>: 数据采样，是否放回采样</li>
<li><code>countByKey</code>: Map[Key, Int]</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">Array</span>[<span class="type">T</span>]<span class="function"><span class="keyword">def</span> <span class="title">aggregate</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">T</span>) =&gt; <span class="type">U</span>, combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">U</span> <span class="comment">// 为 def fold(zeroValue: T)(op: (T, T) =&gt; T): T</span></span><br><span class="line"><span class="comment">// 可保存到...</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> </span><br><span class="line"><span class="comment">// 迭代对应的分区</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreachPartition</span></span>(f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Unit</span>): <span class="type">Unit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsTextFile</span></span>(path: <span class="type">String</span>): <span class="type">Unit</span> </span><br></pre></td></tr></table></figure>

<ul>
<li>aggregate 使用流程： 定义初始值 → 局部汇总 → 全局汇总</li>
</ul>
<h3 id="K-V-算子"><a href="#K-V-算子" class="headerlink" title="K-V 算子"></a>K-V 算子</h3><blockquote>
<p>带有键值的 RDD 算子，PairRDD 使用普遍</p>
</blockquote>
<p>PairRDD</p>
<ul>
<li>Map 类操作<ul>
<li><code>mapValues</code> / <code>flatMapValues</code> / <code>keys</code> / <code>values</code></li>
</ul>
</li>
<li>聚合操作<ul>
<li><code>groupByKey</code> / <code>reduceByKey</code> / <code>foldByKey</code> / <code>aggregateByKey</code></li>
</ul>
</li>
<li><code>subtractByKey</code>：类似于 subtract，删掉 RDD 中键与 other RDD 中的键相同的元素</li>
<li><code>combineByKey（OLD）</code> / <code>combineByKeyWithClassTag （NEW）</code> =&gt; 底层实现</li>
<li>排序操作<ul>
<li><code>sortByKey</code>： sortByKey 函数作用于 PairRDD，对 Key 进行排序。在  org.apache.spark.rdd.OrderedRDDFunctions 中</li>
</ul>
</li>
<li>join操作<ul>
<li><code>cogroup</code>：</li>
<li><code>join</code>：</li>
<li><code>leftOuterJoin</code> / <code>rightOuterJoin</code> / <code>fullOuterJoin</code>：</li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始值</span></span><br><span class="line"><span class="comment">// 分区内的聚合函数</span></span><br><span class="line"><span class="comment">// 分区间的聚合函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregateByKey</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>, partitioner: <span class="type">Partitioner</span>)(seqOp: (<span class="type">U</span>, <span class="type">V</span>) =&gt; <span class="type">U</span>,      combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure>

<p>操作算子的选择：</p>
<ul>
<li>groupByKey 一般情况下效率低，有 Shuffle，少用，使用 reduceByKey, aggregateByKey 进行替代</li>
<li>groupByKey 与 reduceByKey 的区别<ul>
<li>groupByKey Shuffle 过程中传输的数据量大，效率低</li>
<li>reduceByKey 有 map 端的 combiner， shuffle 过程，数据量小，效率高</li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 分区内的合并与分区间的合并，可以采用不同的方式；这种方式是低效的</span></span><br><span class="line">rdd.aggregateByKey(scala.collection.mutable.<span class="type">ArrayBuffer</span>[<span class="type">Int</span>]())((x, y) =&gt; &#123;x.append(y); x&#125;,(a, b) =&gt; &#123;a++b&#125;)</span><br><span class="line">	.mapValues(v =&gt; v.sum.toDouble/v.size)</span><br><span class="line">	.collect</span><br></pre></td></tr></table></figure>

<p><strong>其他操作</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)], partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))] = self.withScope &#123;  <span class="keyword">this</span>.cogroup(other, partitioner).flatMapValues( pair =&gt;    <span class="keyword">for</span> (v &lt;- pair._1.iterator; w &lt;- pair._2.iterator) <span class="keyword">yield</span> (v, w)  )&#125;</span><br></pre></td></tr></table></figure>

<p><strong>Action 操作</strong></p>
<ul>
<li>collectAsMap / countByKey / lookup(key)</li>
</ul>
<p><strong>序列化</strong></p>
<p>使用 case class 样例类，集成 / 特质序列化类</p>
<h3 id="算子源码"><a href="#算子源码" class="headerlink" title="算子源码"></a>算子源码</h3><p><strong>Map 类算子：</strong></p>
<p>map, flatMap, filter, mapPartitions, mapPartitionsWithIndex  ⇒ <strong>MapPartitionsRDD</strong></p>
<p><strong>聚合类算子:</strong></p>
<p>groupByKey, reduceByKey, foldByKey, aggregateByKey ⇒ CombineByKeyWithClassTag ⇒ (初始值, 分区内合并, 分区间数据合并) ⇒ <strong>ShuffledRDD</strong></p>
<p><strong>join 类算子:</strong></p>
<p>cogroup ⇒ <strong>CoGroupedRDD</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))] = self.withScope &#123;</span><br><span class="line">  join(other, defaultPartitioner(self, other))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>Action 类算子</strong></p>
<p>collect 将结果收集为一个 Array, 生产环境中慎用。</p>
<ul>
<li>触发 job</li>
<li>合并多个 Array</li>
<li>将 rdd 的原始放到数组中返回</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> results = sc.runJob(<span class="keyword">this</span>, (iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; iter.toArray)</span><br><span class="line">  <span class="type">Array</span>.concat(results: _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>reduce： </p>
</li>
<li><p>fold:</p>
</li>
<li><p>sum：求和</p>
</li>
<li><p>aggregate: 聚合</p>
</li>
</ul>
<p><strong>foreach / foreachPartition</strong></p>
<ul>
<li>foreach 函数作用在每个 RDD 上</li>
<li>foreachPartition 作用在</li>
</ul>
<p><strong>makeRDD, paralelize</strong></p>
<p>属于 Tranformation, SparkContext.makeRDD</p>
<p>没有使用分区器</p>
<p>// 测试案例打印  [1..10] 对半顺序分了…</p>
<p>调用 ParallelCollectionRDD.slice 进行分区, 为 Scala 代码，可单独进行测试。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    parallelize(seq, numSlices)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>其他相应的 RDD</p>
<ul>
<li>WholeTextFileRDD</li>
<li>HadoopRDD</li>
</ul>
<h3 id="输入与输出"><a href="#输入与输出" class="headerlink" title="输入与输出"></a>输入与输出</h3><p>文本文件读取</p>
<ul>
<li>textFile(path), 读取单个文件，支持通配符读取</li>
<li>wholeTextFiles: 可读取多个文件</li>
</ul>
<p>对象文件： 读取对象序列化后保存的文件，采用 Java 的序列化机制。</p>
<h2 id="RDD-容错"><a href="#RDD-容错" class="headerlink" title="RDD 容错"></a>RDD 容错</h2><p><strong>RDD Lineage：</strong></p>
<p>记录 RDD 的元数据信息和转换行为，当部分分区数据丢失，根据依赖信息重新计算和恢复丢失的数据分区。用于解决数据容错，划分 Stage。</p>
<p>可根据 RDD 的依赖关系重新计算出来，用于数据的恢复。一般对于依赖链过长的，需要进行 checkpoint、持久化避免重新计算耗费大量时间。</p>
<h3 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h3><blockquote>
<p>截断 RDD 的依赖关系，将数据根据对应的存储级别保存起来。</p>
</blockquote>
<p>试用场景：</p>
<ul>
<li>DAG 中的 Lineage 过长</li>
<li>宽依赖上做 Checkpoint 获得的收益更大</li>
</ul>
<p><strong>checkpoint 也是 lazy 的</strong></p>
<h2 id="RDD-的分区"><a href="#RDD-的分区" class="headerlink" title="RDD 的分区"></a>RDD 的分区</h2><blockquote>
<p>RDD 逻辑上是分区的，每个分区的数据是抽象存在的。</p>
</blockquote>
<p>Spark 中分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 过程属于哪个分区和 Reduce 的个数。</p>
<p>Partition 默认与接入组件的分区一致，在 RDD 中，最少有两个分区。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">defaultMinPartitions</span></span>: <span class="type">Int</span> = math.min(defaultParallelism, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h3 id="HashPartitioner"><a href="#HashPartitioner" class="headerlink" title="HashPartitioner"></a>HashPartitioner</h3><p>默认的分区器，分区的结果无序，将 key 的 hash 除以分区个数。</p>
<h3 id="RangePartitioner"><a href="#RangePartitioner" class="headerlink" title="RangePartitioner"></a>RangePartitioner</h3><p>RangePartitioner 分区尽量保证每个分区中的数据量的均匀，并且分区于分区之间是有序的，即一个分区中的元素肯定都是比另一个分区内的元素小或者大； 但分区内的元素不能保证顺序的，将一定范围内的数据映射到某一个分区内，原理是水塘抽样。</p>
<p>实现中，分界的算法重要，算法对应的函数为 rangeBounds。</p>
<p>需要 key 类型为可排序的。</p>
<h3 id="分区对应关系"><a href="#分区对应关系" class="headerlink" title="分区对应关系"></a>分区对应关系</h3><p><strong>与 HDFS 的分区关系</strong></p>
<p>Spark 自己根据底层 HDFS 的 block 数量来设置 task 的数量，默认是一个 HDFS block 对应一个 task。</p>
<p><strong>与 Kafka 分区的关系</strong></p>
<p>默认与 Kafka 的分区对应起来?</p>
<p><strong>与 HBase 分区的关系</strong></p>
<p>默认情况下，HBase 有多少个 region，Spark 读取时就会有多少个 partition</p>
<p>…</p>
<h2 id="Shuffle-操作"><a href="#Shuffle-操作" class="headerlink" title="Shuffle 操作"></a>Shuffle 操作</h2><p>Shuffle 操作开销大，经过 Shuffle 的操作都为宽依赖的算子。</p>
<h3 id="三种-shuffle-机制"><a href="#三种-shuffle-机制" class="headerlink" title="三种 shuffle 机制"></a>三种 shuffle 机制</h3><p><a target="_blank" rel="noopener" href="https://www.notion.so/Spark-Shuffle-DataSkew-928d7b1ebf404a97b083503956a77ac0">Spark 中 Shuffle 与 DataSkew  </a></p>
<h2 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h2><blockquote>
<p>算子，persist、cache、unpersist，都为 Tranfaction, 为标记持久化。</p>
</blockquote>
<p>具体见 <a target="_blank" rel="noopener" href="https://www.notion.so/Spark-b3a8bef4bf1d4b7594402b9123245c70">Spark 内存管理与持久化</a></p>
<p>标记持久化： 出现 persist() 语句的地方，并不会马上计算生成 RDD 并把它持久化，而是要等到遇到第一个行动操作触发真正计算以后，才会把计算结果进行持久化。</p>
<p>持久化场景： 多个动作需要用到某个 RDD，而它的计算代价又很高，那么就应该把这个 RDD 缓存起来</p>
<p>存储级别</p>
<ul>
<li>MEMORY_ONLY：RDD 默认缓存级别</li>
<li>MEMORY_AND_DISK：Dateset 默认缓存级别</li>
<li>MEMORY_ONLY_SER： 序列化存储，比将对象反序列化的空间利用率更高，读取时比较占用 CPU。DStream 默认缓存级别。</li>
<li>MEMORY_AND_DISK_SER：</li>
<li>MEMORY_ONLY_2 / MEMORY_AND_DISK_2： 将每个分区都复制到集群的两个节点</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;..&#125;</span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>, allowOverride: <span class="type">Boolean</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;&#125;</span><br></pre></td></tr></table></figure>

<h2 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h2><h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><blockquote>
<p>广播变量，避免 shuffle,常见的优化手段。</p>
</blockquote>
<p>各个 Task 中变量的更新不会返回给 Driver。</p>
<p>不需要每个 Task 有一份变量副本，变为每个节点的 Executor 有一份副本，减少变量产生的副本，使用广播算法 BT 协议分发广播变量降低通信成本。</p>
<p>典型应用案例为 Map Side Join。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> broadcastVar = sc.broadcast(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))broadcastVar.value</span><br></pre></td></tr></table></figure>

<h3 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h3><p>Spark 提供的一种分布式的变量机制，对变量进行累加操作。</p>
<p>典型应用场景为记录某些事件/信息的数量。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;AccumulatorOpScala&quot;</span>)</span><br><span class="line">	.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment">// 定义累加变量</span></span><br><span class="line"><span class="keyword">val</span> sumAccumulator: <span class="type">LongAccumulator</span> = sc.longAccumulator</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用累加变量</span></span><br><span class="line">rdd.foreach(num =&gt; sumAccumulator.add(num))</span><br><span class="line"></span><br><span class="line"><span class="comment">// N: 只能在 Driver 进程中获取累加变量的结果</span></span><br><span class="line">println(sumAccumulator.value)</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure>

<h2 id="RDD-编程优化"><a href="#RDD-编程优化" class="headerlink" title="RDD 编程优化"></a>RDD 编程优化</h2><p>对于复杂的业务需要使用到 RDD 编程，目前 Spark SQL 优化较多，能使用 Spark SQL 的情况下解决问题尽可能使用。</p>
<p><strong>给 RDD 重新设置 partition 的数量</strong></p>
<p>通过 repartition 增大分区数，生成新的 rdd 提高任务的并行度。</p>
<p><strong>RDD 复用</strong></p>
<p><strong>RDD 缓存/持久化</strong></p>
<p>调用 cache 或者 persist方法，根据实际的应用场景合理的设置缓存级别，减少数据的重新计算。</p>
<p><strong>filter 无效数据</strong></p>
<p>过滤掉业务不相关的数据，减少网络传输开销。</p>
<p><strong>使用高性能算子</strong></p>
<p>在可实现相同业务的情况下，避免 shuffle / 减少 shuffle 过程的数据量，如 groupByKey → reduceByKey / aggregateByKey。</p>
<p>对于批量的连接操作，如 MySQL 的连接、HBase 的连接，map → mapPartition，foreach → foreachPartition，避免无效的连接创建。</p>
<p><strong>设置合理的并行度</strong></p>
<p><strong>广播大变量</strong></p>
<p>map 端的 join, 比较典型的优化</p>
<h2 id="RDD-编程案例"><a href="#RDD-编程案例" class="headerlink" title="RDD 编程案例"></a>RDD 编程案例</h2><h3 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// RDD 方式进行词频统计</span></span><br><span class="line"><span class="comment">// - 统计时忽略单词的大小写</span></span><br><span class="line"><span class="comment">// - 剔除标点符号，如 ().,;:?</span></span><br><span class="line"><span class="comment">// - 剔除停位词，如 in,on,to...</span></span><br><span class="line"><span class="comment">// - 结果保存到 MySQL</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SuperWordCount</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> stopWords: <span class="type">Array</span>[<span class="type">String</span>] = <span class="string">&quot;in on to from by a an the is are were was i we you your he his some any of as can it each&quot;</span>.split(<span class="string">&quot;\\s+&quot;</span>)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> punctuation = <span class="string">&quot;[\\)\\.,:;&#x27;!\\?]&quot;</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> url = <span class="type">Constant</span>.<span class="type">MYSQL_URL</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> username = <span class="type">Constant</span>.<span class="type">MYSQL_USERNAME</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> password = <span class="type">Constant</span>.<span class="type">MYSQL_PASSWD</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 0. check and init</span></span><br><span class="line">        <span class="keyword">if</span> (args == <span class="literal">null</span> || args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            println(<span class="string">&quot;must have two param&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getCanonicalName.init).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        sc.setLogLevel(<span class="string">&quot;WARN&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. get rdd</span></span><br><span class="line">        <span class="keyword">val</span> inputPath: <span class="type">String</span> = args(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">val</span> outPath: <span class="type">String</span> = args(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">val</span> lineRdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(inputPath)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. rdd operator</span></span><br><span class="line">        <span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = lineRdd</span><br><span class="line">          .flatMap(_.split(<span class="string">&quot;\\s+&quot;</span>))</span><br><span class="line">          .map(_.toLowerCase)</span><br><span class="line">          .map(_.replaceAll(punctuation, <span class="string">&quot;&quot;</span>))</span><br><span class="line">          .filter(word =&gt; !stopWords.contains(word) &amp;&amp; word.trim.nonEmpty)</span><br><span class="line">          .map((_, <span class="number">1</span>))</span><br><span class="line">          .reduceByKey(_ + _)</span><br><span class="line">          .sortBy(_._2, ascending = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. result handle, output into &#123;console, file, MySQL&#125;</span></span><br><span class="line">        resultRDD.cache()</span><br><span class="line">        resultRDD.foreach(println)</span><br><span class="line">        resultRDD.saveAsTextFile(outPath)</span><br><span class="line">        resultRDD.foreachPartition &#123; iter =&gt; saveAsMySQL(iter) &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// X. resource recovery</span></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">saveAsMySQL</span></span>(iter: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">Int</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> stmt: <span class="type">PreparedStatement</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">val</span> sql = <span class="string">&quot;INSERT IGNORE INTO wordcount VALUES (?, ?)&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            conn = <span class="type">DriverManager</span>.getConnection(url, username, password)</span><br><span class="line">            stmt = conn.prepareStatement(sql)</span><br><span class="line">            iter.foreach &#123; <span class="keyword">case</span> (k, v) =&gt;</span><br><span class="line">                stmt.setString(<span class="number">1</span>, k)</span><br><span class="line">                stmt.setInt(<span class="number">2</span>, v)</span><br><span class="line">                stmt.executeUpdate()</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (stmt != <span class="literal">null</span>) stmt.close()</span><br><span class="line">            <span class="keyword">if</span> (conn != <span class="literal">null</span>) conn.close()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="TopN"><a href="#TopN" class="headerlink" title="TopN"></a>TopN</h3><p>求组内的 TopN</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Janhen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2021/04/09/SparkCore-RDD-%E7%BC%96%E7%A8%8B/">http://example.com/2021/04/09/SparkCore-RDD-%E7%BC%96%E7%A8%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/04/09/Kafka-%E5%9F%BA%E7%A1%80/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Kafka-基础</div></div></a></div><div class="next-post pull-right"><a href="/2021/04/09/Spark-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Spark-内存管理与持久化</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/05/05/Spark-SQL中Join的实现/" title="Spark-Join的原理"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-05</div><div class="title">Spark-Join的原理</div></div></a></div><div><a href="/2021/04/09/Spark-SparkContext与任务调度/" title="Spark-Master与Worker和SparkContext"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-09</div><div class="title">Spark-Master与Worker和SparkContext</div></div></a></div><div><a href="/2021/04/09/Spark-内存管理与持久化/" title="Spark-内存管理与持久化"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-09</div><div class="title">Spark-内存管理与持久化</div></div></a></div><div><a href="/2021/05/07/Spark-数据倾斜/" title="Spark-数据倾斜"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-07</div><div class="title">Spark-数据倾斜</div></div></a></div><div><a href="/2021/04/09/Spark-管理/" title="Spark-管理"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-09</div><div class="title">Spark-管理</div></div></a></div><div><a href="/2021/04/16/Spark实现KNN近邻算法/" title="Spark实现KNN近邻算法"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-16</div><div class="title">Spark实现KNN近邻算法</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="http://img.janhen.com/20210331220602f7HmbN.pnghttp://img.janhen.com/20210331220602f7HmbN.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Janhen</div><div class="author-info__description">大数据、后端、运维分享</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">47</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">21</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Janhen"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD-%E6%A6%82%E8%BF%B0"><span class="toc-text">RDD 概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90"><span class="toc-text">基本组成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD-%E5%88%9B%E5%BB%BA%E6%96%B9%E5%BC%8F"><span class="toc-text">RDD 创建方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD-%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-text">RDD 序列化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD-%E7%AE%97%E5%AD%90"><span class="toc-text">RDD 算子</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformation"><span class="toc-text">Transformation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%BD-%E7%AA%84%E4%BE%9D%E8%B5%96"><span class="toc-text">宽&#x2F;窄依赖</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Action"><span class="toc-text">Action</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-V-%E7%AE%97%E5%AD%90"><span class="toc-text">K-V 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E5%AD%90%E6%BA%90%E7%A0%81"><span class="toc-text">算子源码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E4%B8%8E%E8%BE%93%E5%87%BA"><span class="toc-text">输入与输出</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD-%E5%AE%B9%E9%94%99"><span class="toc-text">RDD 容错</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#checkpoint"><span class="toc-text">checkpoint</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD-%E7%9A%84%E5%88%86%E5%8C%BA"><span class="toc-text">RDD 的分区</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#HashPartitioner"><span class="toc-text">HashPartitioner</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RangePartitioner"><span class="toc-text">RangePartitioner</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E5%AF%B9%E5%BA%94%E5%85%B3%E7%B3%BB"><span class="toc-text">分区对应关系</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Shuffle-%E6%93%8D%E4%BD%9C"><span class="toc-text">Shuffle 操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E7%A7%8D-shuffle-%E6%9C%BA%E5%88%B6"><span class="toc-text">三种 shuffle 机制</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD-%E6%8C%81%E4%B9%85%E5%8C%96"><span class="toc-text">RDD 持久化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F"><span class="toc-text">共享变量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="toc-text">广播变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="toc-text">累加器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD-%E7%BC%96%E7%A8%8B%E4%BC%98%E5%8C%96"><span class="toc-text">RDD 编程优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD-%E7%BC%96%E7%A8%8B%E6%A1%88%E4%BE%8B"><span class="toc-text">RDD 编程案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#WordCount"><span class="toc-text">WordCount</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TopN"><span class="toc-text">TopN</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/05/07/Spark-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/" title="Spark-数据倾斜"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark-数据倾斜"/></a><div class="content"><a class="title" href="/2021/05/07/Spark-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/" title="Spark-数据倾斜">Spark-数据倾斜</a><time datetime="2021-05-07T07:45:41.000Z" title="Created 2021-05-07 15:45:41">2021-05-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/05/Spark-Shuffle%E8%BF%87%E7%A8%8B/" title="Spark-Shuffle 过程和原理"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark-Shuffle 过程和原理"/></a><div class="content"><a class="title" href="/2021/05/05/Spark-Shuffle%E8%BF%87%E7%A8%8B/" title="Spark-Shuffle 过程和原理">Spark-Shuffle 过程和原理</a><time datetime="2021-05-05T08:55:56.000Z" title="Created 2021-05-05 16:55:56">2021-05-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/05/Spark-SQL%E4%B8%ADJoin%E7%9A%84%E5%AE%9E%E7%8E%B0/" title="Spark-Join的原理"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark-Join的原理"/></a><div class="content"><a class="title" href="/2021/05/05/Spark-SQL%E4%B8%ADJoin%E7%9A%84%E5%AE%9E%E7%8E%B0/" title="Spark-Join的原理">Spark-Join的原理</a><time datetime="2021-05-05T08:29:59.000Z" title="Created 2021-05-05 16:29:59">2021-05-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/04/22/Hadoop-MapReduce-%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/" title="Hadoop-MapReduce-计算引擎"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hadoop-MapReduce-计算引擎"/></a><div class="content"><a class="title" href="/2021/04/22/Hadoop-MapReduce-%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/" title="Hadoop-MapReduce-计算引擎">Hadoop-MapReduce-计算引擎</a><time datetime="2021-04-22T05:22:45.000Z" title="Created 2021-04-22 13:22:45">2021-04-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/04/22/Hadoop-HDFS-%E5%AD%98%E5%82%A8/" title="Hadoop-HDFS-存储"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hadoop-HDFS-存储"/></a><div class="content"><a class="title" href="/2021/04/22/Hadoop-HDFS-%E5%AD%98%E5%82%A8/" title="Hadoop-HDFS-存储">Hadoop-HDFS-存储</a><time datetime="2021-04-22T05:22:29.000Z" title="Created 2021-04-22 13:22:29">2021-04-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Janhen</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>